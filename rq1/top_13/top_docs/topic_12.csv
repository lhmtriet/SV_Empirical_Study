index,answers,creation_date,len,postid,question,ratio,raw,source,tags,title,uniq,words,probability
33694,it is not that simple  indeed md5 is considered as weak today and it is possible to  find a collision  another input giving the same hash under a minute  regardless that you have to compute the inputs just guessing won t work md5 produces 128 bit output so with random guessing will find collision with probability 1/2^128   almost impossible,2019-03-20 17:10:34.337 UTC,115,55266546,i know that there is some case 2 different strings can have same md5 hash but when i try with php it still become 2 different hash so is there any string that have same md5 hash in php if i use  ,0.017391304347826087,2,so,cryptography|exploit|md5|php,same md5 hash value but different input using hash md5,2,exploit|weakness,0.695672869682312
10187,from wikipedia   http://en.wikipedia.org/wiki/cryptographic_hash_function        properties        most cryptographic hash functions are designed to take a string of any  length as input and produce a fixed-length hash value a cryptographic  hash function must be able to withstand all known types of  cryptanalytic attack as a minimum it must have the following  properties       preimage resistance  given a hash  h  it should be difficult to find any message  m  such that  h = hashm  this concept is related to that of one-way  function functions that lack this property are vulnerable to preimage  attacks       second-preimage resistance  given an input  m  1  it should be difficult to find another input  m  2  — where  m  1  !=  m  2  — such that  hash   m  1   =  hash   m  2   this property is  sometimes referred to as weak collision resistance and functions that  lack this property are vulnerable to second-preimage attacks       collision resistance  it should be difficult to find two different messages  m  1  and  m  2  such that    hash   m  1   =  hash   m  2   such a pair is called a cryptographic hash  collision this property is sometimes referred to as strong collision  resistance it requires a hash value at least twice as long as that  required for preimage-resistance otherwise collisions may be found by  a birthday attack,2012-01-14 06:05:08.43 UTC,237,8860512,for hash function what s the difference for collision protection and preimage protection,0.0379746835443038,9,so,cryptography|hash|security,what s the difference between collision resistance and preimage  resistance,4,attacks|weakness|protection|vulnerability,0.6933221817016602
26980,brute force is the worst attack nothing can be brute force proof..  right now ~80-90 bits is considered cryptographically safe from a brute force attack standpoint so you only need 10 bytes if a collision resistant hash function is perfect but they aren t so you just do more bits..  the proof that nothing can be brute force proof is in the  pigeon hole principle   since hash function   allows arbitrary sized input   and outputs constant output   when the size of input exceeds the output size:   there are necessarily some outputs that can be produced by more than one input  you can visualize that with a square divided into 9 sub squares   these are your 9 holes we are a brute force attacker we have unlimited chances to attack.. we have unlimited pigeons.. but we at most need 10 to find a collision..  after 4 pidgeons and a good collision resistant hashing algorithm   after 9 pidgeons    so our 10th pigeon will necessarily be a collision because all of the holes are full  but it really isn t even that good because of another numerical property called the  birthday paradox  where given a number of independent selections you will find a duplicate much much faster than it takes to fill all of your holes  all cryptographic systems are vulnerable to brute force  another term for this is a trivial attack  a simple explanation for hashing  is that all hashing algorithms we use accept an infinitely sized input and have a fixed sized output  this is an unavoidable collision  and for something like sha256 it takes 2^256 operations to find one naturally md5 has a shortcut making it 2^39th operations to find a collision   one thing you can do to make your passwords stronger is to hide your salt   a password hash cannot be broken until its salt is retrieved   john the ripper  can be given a dictionary  a salt and a password to recover password hashes of any type  in this case sha256 and md5 will break in about the same amount of time  if the attacker doesn t have the salt he will have to make  significantly more  guesses  if your salt is the same size as sha256 32 bytes it will take   guesses to break one password  this property of salts is the basis of  cwe-760   as codeka said no hashing algorithm is 100% secure against brute force attacks however even with hardware-assisted password cracking using the gpu to try passwords the time it takes to crack a sufficiently long password is astronomical if you have a password of 8ish characters you could be vulnerable to a brute force attack but if you add a few more characters the time it takes to crack increases radically  of course this doesn t mean you re safe from rainbow attacks the solution to that is to add a salt to your password and use a hashing algorithm that isn t vulnerable to preimage attacks  if you use a salted password of 12-14 characters preferably hashed with an sha2 algo or equivalent you ve got a pretty secure password  read more here  http://www.codinghorror.com/blog/2007/10/hardware-assisted-brute-force-attacks-still-for-dummies.html   consider the output of the hash algorithms  a md5 128 bit or sha-1 160 bit is certainly easier to brute-force than a sha-2 224 384 or even 512 bit  of course there can be other flaws like in  md5  or a bit less in  sha-1  which weaken the algorithm a lot more  the only protection against brute force is the fact that it takes an inordinately long time to perform a brute force  brute force works by simply going through every possible input string and trying it one at a time there s no way to protect against simply trying every possible combination  if you know that the input space is small enough for a brute force attack to be feasible then there are two options for protecting against brute-force attacks   artificially enlarging the input space this isn t really feasible -  password salting  looks like that at first glance but it really only prevents attackers from amortizing the cost of a brute force attack across multiple targets  artificially slowing down the hashing through  key strengthening  or using a hash algorithm that is inherently slow to compute - presumably it s only a small extra cost to have the hash take a relatively long time say a tenth of a second in production but a brute-force attacker incurs this cost billions of times   so that s the answer  the slower a hash algorithm is to compute the less susceptible it is against brute-forcing the input space   original answer follows  any additional bit in the output format makes the algorithm twice as strong against a straightforward brute force attack  but consider that if you had a trillion computers that could each try a trillion hashes per second it would still take you over 100 trillion years to brute-force a 128 bit hash and you ll realize that a straightforward brute-force attack on the output is simply not worth wasting any throughts on  of course if the  input  of the hash has less than 128bits of entropy then you can brute-force the input - this is why it s often feasible to brute-force password cracking nobody can actually remember a password with 128 bits of entropy,2010-05-05 08:09:12.13 UTC,936,2771487,well from the discussion of hashing methods weaknesses i ve got that the only ol  good brute-force is efficient to break   so the question is is there a hashing algorithm which is more rigid against brute-force than others? in case of hashing passwords,0.029914529914529916,28,so,brute-force|hash|security,is there a bruteforce-proof hashing algorithm,6,cwe|flaws|attacks|weakness|protection|vulnerability,0.6840500235557556
53561,,2018-12-05 05:56:56.633 UTC,68,53626008,"i have a list of sha-256 and sha1 hashes.to add these hashes to the system i have to convert these hashes to md5  if there is a scanned result of malware file on virustotal.com,i can know md5 hash through result  but i have thousands of hashes.it s impossible to convert one by one  how to convert hashes ",0.058823529411764705,4,so,hash|malware|md5,convert sha-256 or sha-1 to md5 through virustotal.com,2,virus|malware,0.6443780660629272
2194,strong passwords always matter because of two concepts that make hacking passwords easier dictionary attacks and rainbow attacks often used together for maximum effectiveness for a dictionary attack one simply takes every word in the dictionary and usually two three and four words etc and tries those combinations first given the rather small size of dictionaries having a password that matches a word in the dictionary makes finding the password trivial this is why you shouldn t use common names words or phrases as passwords  rainbow attacks give attackers even more firepower by pre-computing values say from a dictionary attack or even millions of possible random-character passwords since they are pre-computed the attacker only needs to scan their table for your hash and if there s a match they know your password immediately while this is similar in nature to a brute-force attack the deciding factor here is that the attacker comes at your data already armed with millions or even billions of hashes which can be scanned in just a couple of seconds for a match even if it took the attackers one million seconds to guess the password they already spent that time even before they found your file preparation for the actual attack can drastically reduce the time needed to find the password even if they don t find it they ve already eliminated all the passwords they ve pre-computed as possible passwords so the work isn t lost  so while there is still a huge space of safe passwords far more than there are unsafe passwords some care must be taken to avoid passwords that most likely would appear in a dictionary attack or a rainbow attack choosing a combination of letters numbers and symbols but not in common patterns is important for example h4ck3rz might seem somewhat safe but since this is a common leet-speek term you should avoid using it as a password or a part thereof also any passwords less than eight letters long are trivially easy to break and should also be avoided  the question implied is does a password of 32 characters with numbers letters uppercase symbols etc make sense?   i d say it s an overkill when being used with bcrypt in a key derivation function  but at those length there is a choice to use passphrases the latter may be composed of existing words like   cater low ebony align scan    spaces added for readability  the words can be chosen from a rather small dictionary like a 7776 word diceware-like dictionary the individual words must be chosen at random! likewise for individual characters of a password.  at  http://en.wikipedia.org/wiki/password_strength   one can see that a 5-word diceware phrase is as strong as a 11 character password consisting of a..z a..z 0..9 if you want to do the math  a 11 character password 11^62possibilities = 5.23e19 combinations  a five word phrase form a 7776 word dictionary 7776^5=2.8e19 combinations  won t do the math a phrase likecater=low=eboony=align=scan  1.3e22 combinationsincluding separators + just one random inserted character  what does the strength of your password have to do with the fact that a bcrypt hash happens to be 60 characters long?  if your password is trivial to guess then it s trivial to guess no matter how long the resulting hash may be if you re talking about a single printable ascii character then the attacker needs at most 95 attempts to find out the password no hash algorithm will prevent this  bcrypt does not magically make every password immune to brute-force attacks it only makes those attacks  harder  weak passwords do not benefit from this but if you have a decent password then bcrypt can make a brute-force attack infeasible by demanding more resources than the attacker has  the strength of a password is exactly how much it is unknown to the attacker it  always  matters that strength equates to the number of tries on average that the attacker will have to perform in order to guess it  what bcrypt does is that it makes each try more expensive with its configurable number of iteration set sufficiently high bcrypt can make it so that the attacker cannot try more than one password per second on his machine this is much better for you than a  billion  tries per second with a less suitable hash function it makes cracking your password a billion times harder however if your password consists of only one character the attacker will still find it in less than a minute  so while bcrypt makes weak passwords more tolerable it cannot save your skin  absolutely   output length of bcrypt is irrelevant for this it is normal that hash functions have a fixed output size in fact if the output size depended on the input size then it would be a weakness since it would leak information on the password,2014-06-16 23:40:53,854,61190,even if i choose 1 character for my password or 32 characters with numbers letters uppercase symbols etc the bcrypted password will still be 60 characters length password so does the password strength matter when using bcrypt,0.03044496487119438,26,sse,bcrypt|passwords,does the strength of password matter in bcrypt,4,leak|attacks|weakness|weak password,0.6426823735237122
21880,the cost should depend on your hardware   you should test your cost settings and aim for the   interval of course if you are working with highly sensitive information the time could be   or even more  you must set the number of iterations at the maximum value which is still tolerable depending on the hardware you use and the patience of the users higher is better  the whole point of the iteration count is to make the password processing slow -- that is to make it slow for the attacker who tries potential passwords the slower the better unfortunately raising the iteration count makes it slow for you too..  as a rule of thumb consider that an attacker will break passwords by trying on average about 10 millions 10 7  of potential passwords if you set the iteration count so that password hashing takes 1 second for you and you consider that the attacker can muster ten times more computing power than you then it will take him 10 7 *1/10 seconds i.e about 12 days if you set the iteration count so that password hashing takes only 0.01 second on your pc then the attacker is done in three hours  the cost you should use depends on how fast your hardware and implementation is  generally speaking a cost of 8 or 10 is fine -- there isn t any noticable delay it s still a huge level of protection and far better than any home-grown solution using shas and salts once you upgrade your hardware you could increase the cost to 16 i would say that 16 is a little high at this time and will probably result in noticeable and annoying delays but if 16 works for you by all means go for it,2011-10-01 20:16:15.953 UTC,342,7622698,i ve read some articles saying you should set the cost to be at least 16 2 16  yet others say 8 or so is fine  is there any official standard for how high the cost should be set to,0.017543859649122806,6,so,bcrypt|cryptography,bcrypt - how many iterations/cost,3,attacks|protection|sensitive information,0.6241496801376343
5593,yes you can  bcrypt and other similarly slow password hashing functions aren t designed to make this impossible but only to make it prohibitively expensive   weak passwords are still susceptible to being discovered but the amount of work an attacker needs to do goes up substantially and compared to a fast hashing function much weaker passwords become infeasible to crack due to the extra work required for each guess,2017-03-01 19:59:44,112,152725,let s suppose i know a website is using bcrypt with salts and the default round of 10 can i do a dictionary attack by hashing all the words using bcrypt with the same parameters,0.044642857142857144,5,sse,bcrypt|cryptography|hash,dictionary attack knowing the cipher,3,attacks|weakness|weak password,0.6090107560157776
524,there is no problem  if  you have a good password created by diceware or bip39 with good entropy then a polynomial-time adversary even with the knowledge of the hash and salt cannot do anything if your password is already   we already assumed that can be broken part of it leaked or not what about the salt is not available to the attacker good luck for them    the bitcoin bip-39 dictionary with 2048 words can create ≈2263-entropy by tossing the coin 256 times to choose the words randomly    the diceware passphrases has an entropy around   a five-word  has 64.6 bits  six words have 77.5 bits  seven words 90.4 bits  eight words 103 bits  nine words 116 bits  ten words 129 bits     we consider that the polynomial-time attackers can access the salt and the hash value and the security of your passwords must be satisfied with this case the motivation for this approach is easy the attacker can always reach/access the database and download the hashes and salts simply   there is no security with obscurity we consider the security of the passwords even the hash and salt are totally leaked therefore a partial recovery must not create a difference   passwords actually are not simply hashed they are processed with good password hashing functions like pbkdf2 scrypt bcrypt and argon2 which is the winner of the password hashing competition in july 2015  if you are not using a good password hashing mechanism and if you don t have a password with good entropy then you have already insecure  tl;dr the answer depends on the hash algorithm what part of the hash is revealed and the strength of the password  assuming that the hash algorithm is known  the knowledge of a specific part of the hash might make brute force attacks easier  the attacker might now run most brute force tests offline and also in parallel and is thus not restricted by rate limiting or even account locking after failed attempts which is hopefully in place to deter massive brute forcing  how much easier the attack gets depends on the hash and on the strength of the password though if the hash algorithm is slow as recommended then brute forcing will be slow too if the password is weak enough such offline brute forcing will still likely be successful too in contrast if the site implements a wait time after each failed login attempt or maybe even locks the account then online brute force attacks will often fail even for weaker passwords  how much easier it gets depends also a lot on the type of hash and which parts are revealed if the attacker does not know the salt which was used in hashing and which is commonly stored as part of the hash then brute forcing is nearly impossible - provided that the salt is chosen from a large enough random pool  there is quite no case when it s not easier for the adversary the only case is if the password is salted with large random and the salt is not displayed    the main concern is to know how much easier it is for the adversary    in the worst case he can perform offline dictionary/bruteforce attack and due to the dispersion of the hashing function he will have few false positives for example if the website display 6 hex characters you can assume that a bruteforce attack will statistically match a hash with the same heading 6 characters every 16^6 attempt so one false positive every 16 millions attempt it s a big advantage  as previously mentioned if the hash algorithm is bcrypt scrypt and so one the speed of an offline attack will not be very interesting comparing to the online attack but !!! with abilities to perform an offline attack you can avoid account lockout due to passwords attempt threshold so there is some cases where displaying part of the hash will permit to bypass an account lockout functionnality even if the offline attack speed is not huge 10 attemps per sec is still better than 10 attemps and then blocked it is also a way to stay quiet relatively to the siem and other intrusion detection mechanisms    note that having a huge randomly generated password will not be enough to ignore the risk if the password is not salted and the hash algorithm is weak a collide could occur with a shorter password attempt,2020-01-04 21:16:54,843,223688,this question was prompted by a recent visit to a certain site that provides apparently for gdpr reasons a table with all of your data including  part  of your hashed password i understand this poses no problem in this case as you would have to be logged in to see this table but what if this data was made public?  to rephrase more clearly does revealing  part  of your password hash including the hash  length  make password cracking via bruteforce or any other method any simpler or more efficient than before,0.02609727164887307,22,sse,hash|password-cracking|passwords,does revealing part of your hash give an attacker advantage when attacking your password,4,leak|bypass|attacks|weakness,0.6029097437858582
18145,"first of all md5 and sha1 are not encryption functions  they are message digest functions  also most hashes are broken in real world using dictionary attacks like  john the ripper  and  rainbow crack   john the ripper is best suited for salted passwords where the attacker knows the salt value  rainbow crack is good for passwords with small unknown salts and straight hashes like     rainbow crack takes a long time to build the tables  but after that passwords break in a matter of seconds  it depends on how fast your disk drives are   when you hash a password multiple times you actually increase the chance of hash collisions so best practice is to hash only once  it also has nothing to do with how easy it will be to perform a brute-force attack such an attack will systematically try every possible password within a given range thus if your password is foobar and the attack tests the password foobar it wont matter how or how many times you hashed the password because the brute-force attack successfully guessed it  therefore if you wish to guard yourself against a brute-force attack you could limit how often a user can attempt authorization or require passwords to be above a certain length  on a side note rainbow tables and similar methods are used by hackers that have already gained access to your database and are meant to decrypt the stored password in order make such an attack more difficult you should use static and dynamic salts  you are talking about 2 distinct although related problems  first is the likely-hood of a collision and the second is the ability to run the algorithm on tons of values to find the original value which created the hash    collisions   if you run sha1md5text you first get the hash of md5 then pass that to sha1   lets assume the sha1 function has a 128-bit output and the md5 also has 128-bit output   your chance of collision in the md5 function is 1/2^128  then your chance of collision in the sha1 is 1/2^128  if either collides then the function overall collides and hence the result is   or     brute forcing   running sha1md5text will only double the time it takes to find the original string  this is nothing in terms of security  for instance if you have 128-bits of output space for each algorithm and it takes 1 hour to brute force then it will take 2 hours to run the same brute force twice to get the original string  this would be the same as increasing the output space to 129-bits  however if you want to really make brute forcing impossible what you have to do is double the output-size which can be compared to the key size in encryption     hashing a hash is sort of encryption though obfuscation which isn t really a best practice you re right in that it could theoretically reduce the possibility of a collision but it probably wont eliminate the possibility whats more a hashing function isn t really an encrypting function google hashing vs encrypting for several hundred explanations  a collision attack the type that s known against md5 for example does no real good to be effective with regard to a password you need a preimage attack i.e the ability to find some input that will hash to a known hash code though there are preimage attacks known against md5 they re not currently practical  collision attacks are useful for entirely different purposes one example that s been carried out is creating two x.509 certificates for two different identities that collide submit one to be signed by a certificate authority and then you can use the other to claim that you re somebody else entirely since the hash will collide with the first when/if a user tries to verify the certificate it will show up as having been verified  first not encryption creating message digest using the hash functions  your question     but can t you just encrypt  hash  your  password using say md5 and then,  say sha-1 or any other doesn t  matter.   if the hash function does not provide any of these properties it does not matter how many times you hashed also the attacker can hash n times to get the collisions         for any given code h it is computationally infeasible to find  such x that hx=h this property is   called one way or preimage resistant      for any given block x ,it is computationally infeasible to find y≠x  with hy=hx.this property is  referred second preimage resistant or  weak collision resistant      it is computationally infeasible to find any pear x,y such that  hx=hy this is called strong  collision resistant       so as the rook mentioned the passwords are stored by adding different salt values for each users the dictionary gets longer and also computational overhead and time gets longer for the attacker if she exploits the password file  let s say attacker has the hashed values of the passwords and starts reading from the dictionary file and compares with the hashed values if matches then pasword is cracked if salt is used then read from the dictionary and add some salt value then try to find a match.however this should be done for each user so the complexity that salt adds is from wikipedia     assume a user’s encrypted secret key  is stolen and he is known to use one  of 200,000 english words as his  password the system uses a  32-bit  salt  the salted key is now the  original password appended to this  random 32-bit salt because of this  salt the attacker’s pre-calculated  hashes are of no value he must  calculate the hash of each word with  each of 2^32 4,294,967,296 possible  salts appended until a match is found.  the total number of possible inputs  can be obtained by multiplying the  number of words in the dictionary with  the number of possible salts",2010-06-09 21:13:45.023 UTC,1059,3009988,i just spent some time reading  https://stackoverflow.com/questions/2768248/is-md5-really-that-bad  i highly recommend!  in it it talks about hash collisions maybe i m missing something here but can t you just encrypt your password using say md5 and then say sha-1 or any other doesn t matter. wouldn t this increase the processing power required to brute-force the hash and reduce the possibility of collision,0.0169971671388102,18,so,cryptography|hash|md5|security|sha1,what s the big deal with brute force on hashes like md5,3,attacks|exploit|weakness,0.5980901122093201
1608,a salt can make brute forcing much longer  assuming the attacker only has hashes he will have to find out the hash algorithm and the salt  you can sometimes tell the hash algo used by looking at the hash or at least narrow it down  next is finding the salt  usually a hacker probably has many hashes  you can assume a user has the password called password or the hacker made their own account so they know the password and start appending a salt until you get a match  it s best to have unique salt per password to make this process more cumbersome although they may focus all resources on an admin s hash  salt does not protect against brute force attacks because the salt is usually stored adjacent to the hash in clear text so it doesn t add additional unknown entropy  salt is a mitigation against  rainbow table attacks   protection against brute force is provided by the hashing algorithm itself  the hash algorithm should be  slow  so that a brute force attack will take an infeasible amount of time  in addition cryptgraphic pepper added to the cleartext + salt can make the brute force attack more difficult because it renders a dictionary attack impossible  on the other hand if the hacker has access to the db table with the hashes and the salts he may also have access to the pepper    if the sap system uses pepper the length of the password doesn t matter because the pepper will be longer than the password anyway  if it doesn t then shorter passwords are indeed more vulnerable as the brute force attack will check for shorter passwords first  more info on salt vs pepper  here  and  here,2013-11-20 19:05:51,361,45808,sap netweaver has its passwords hashed and also salted but i read that a tool such as john the ripper can bruteforce them  so how does security varies when passwords are hashed in particular in the sap system?  what if i have a password such is this.. a12345 ?  is it easy to bruteforce it even though it is  lets say salted,0.027700831024930747,10,sse,hash,how secure are hashed passwords that are salted too,3,attacks|protection|vulnerability,0.5950549840927124
38,so will it be difficult to find the password from either brute force  attack or rainbow tables?   short answer no if the password is weak then it ll probably be quite easy to break it a brute-force attack trying all character combinations of various lengths will work for short passwords and a dictionary lookup will work for longer but common passwords and words  but that s not the point of salting your hashes the point is that if you don t re-use salts an attacker would have to brute-force each password one by one and using multiple iterations makes it more costly to brute-force them so it s really a matter of gaining time to warn your users and let them change their passwords when you are breached  note that a rainbow table attack isn t possible with salts a rainbow table is essentially a pre-hashed table of many many possible passwords that makes if very fast to find matches so with a salt that won t work again an attacker would have to create the rainbow table for every salt in your database which defeats the purpose of these tables  tl;dr your users should still use strong passwords that are difficult/impossible to brute-force,2020-02-04 14:01:11,271,60059046,i found django uses the format    to store the hashed password  assuming some one has the access to database  from the above format one knows the salt algorithm and iterations so will it be difficult to find the password from either brute force attack or rainbow tables,0.02952029520295203,8,so,django,django how will django protect from knowing the passwords by using hashes if someone have access to the database,3,attacks|weakness|protection,0.5949119925498962
23751,a message digest also known as a hash is the output of a digest or  cryptographic hash function  which is a one-way fixed-size-output compression function having the property that small changes to the input message result in large unpredictable changes in the output digest how to use such a function depends greatly on what you are trying to do with it  the term  salt  refers to a small random input to the hash function that is used to alter the state of the function prior to adding additional input that may be predictable in some way this is a security mechanism that was developed for protecting passwords when using a message digest as a password verifier function if a salt is not used then any users having the same password will have the same password hash stored in the user database enormous efficient hash reversal tables  rainbow tables  exist for the most common message digest functions used in this manner md5 ntlm password hash etc. and an attacker who obtains the database has only to perform a table lookup to obtain the plaintext password of every user using a salt prevents the generation of these tables since each byte of salt results in a  256x size increase of the lookup table   it s very important to note that  simple message digest with salt is an insufficient protection  for password storage and verification this is a complex problem and solutions already exist that can properly resist attack  pbkdf2   bcrypt   scrypt  and others  similarly verifying the proper transmission of a message requires the use of a more advanced cryptographic construction called a hashed message authentication code hmac this is built on a message digest function but uses a cryptographic key to guarantee not only the integrity of the protected message but also the authenticity of the message digest itself  your conclusion regarding its use with messagedigest is correct the salt nonce among other things is used to protect against the use of rainbow tables to crack password hashes     a rainbow table is a precomputed table for reversing cryptographic hash functions usually for cracking password hashes tables are usually used in recovering a plaintext password up to a certain length consisting of a limited set of characters it is a practical example of a space/time trade-off using less computer processing time and more storage than a brute-force attack which calculates a hash on every attempt but more processing time and less storage than a simple lookup table with one entry per hash use of a key derivation function that employs a salt makes this attack infeasible   see  wikipedia   use a salt to avoid brute force attacks against all passwords at once with rainbow tables  by adding a salt and storing it with the password you slow down rainbow table attacks by making the attacker calculate the hash for each stored password rather than comparing to all of them at once,2015-03-09 12:40:24.357 UTC,543,28942244,i have using message digest i have seen it being used sometimes with      and some times without salt   now when can should i be using the digest with   and should not i ?brief research i concluded that whenever the password element is involved the salt is used,0.02394106813996317,13,so,java|message-digest,when to use salt with message digest,3,attacks|protection|plaintext password,0.579235851764679
22310,"the cost factor is meant to stop offline brute force attacks i.e after someone grabs hashes from the database bcrypt is already too slow for practical  online  brute force attacks exception if your password is one of the  10,000 most common  it is trivial to guess what you want instead of increasing the cost factor is to focus on  rate limiting your login attempts",2016-04-20 19:38:47.607 UTC,131,36753577,is there any point using sleep to stop brute force attacks if passwords are stored by bcrypt with   and     login form uses standard     i ve noticed that increasing the cost to 16 will significantly increase the login time upon submit does it work the same way as sleep and if so can it be exploited to dos the server,0.03816793893129771,5,so,login|php|security,login form sleep with bcrypt,3,attacks|exploit|denial of service,0.573222815990448
67317,"short version  i think there is no danger doing short-cut comparison of salted hashes of passwords if the salt is hidden to the attacker   long version   using timing attacks in this case will in no way tell an attacker more than what he would know if he had the actual stored hash and salt .. and brypt s security parameter iteration count exponent should be chosen such that even knowing the hash and salt would not give the attacker a significant enough advantage to derive the password since he would have to brute-force the password from this  on the other hand if the attacker does know neither the used salt nor the stored hash i would guess that a timing of the comparison of calculated and stored hash will not give any information at all since every even single bit change of the password input will result in a completely different hash.this applies to every pseudo-random function not only bcrypt we actually need only the  avalanche effect  here. see below for a more formal elaboration  other than this normally the hashing process itself is included in the measuring which takes much longer than the final comparison at the end and thus will dominate the total measured time i don t know how much this hashing time will variate though    mathematical version      now my question is whether this can be transformed into a  better than brute force attack? is there a possibility for  an incremental attack i.e is it possible to construct a  password with n+1 correct bits from a password with n correct  bits other than using brute-force guessing?   so we have a function   which is the number of leading bitsmatching between   and   let   be the total number of bits of h s output  assume we have an algorithm a which can do your attack i.e an algorithm which,for a hash   unknown to a and a given message   with   finds a message   with   in time better than exponential in  .simple brute-forcing the hash is exponential in  . we can assume that this algorithm has o1 oracle access to   for arbitrary    then we can from this construct an algorithm b which will do a  preimage attack  on a given hash   and salt   i.e finds a message   with   it works as follows   the oracle for a is simply done by calculating   andcomparing it with   counting the correct bits until the first non-match  we choose an arbitrary message   calculate    for each i > 0:  we pass   to a and get back   with    if   stop and return       the output of b is a preimage for    as each   is larger than the previous one this will take at most   passes through the loop with each pass taking at most   time i.e in total   thus we got a sub-exponential preimage-finding algorithm  this shows that  assuming   is preimage-resistant there can t be an algorithm which efficiently produces longer partial matches from shorter partial matches  given the match length as an oracle  i assume one can refine my timing approximation a bit better.    notes regarding the comments   as the comment thread is getting quite long i ll try to resume the important points here    with weak passwords an offline dictionary attack on password hasheswill become feasible which would be our algorithm  b  this is whywe want to choose the bcrypt work factor high enough   still i seeno way to get from a fast  b  offline preimage attack with given saltto a fast  a  online partial preimage finder from hash prefix length oraclealgorithm only the other way around  i expect that even for functions which are preimage-broken therewould be no easy way to get our online attack as long as theystill fit the  avalanche criterion     real-life hash functions are not really random which is causedby their iterative design but this does not really affect theirsuitability for password hashing which works on inputs shorterthan the block size    the important property here is preimage resistance not collision resistance. every collision resistant function is second-preimage resistant andsecond-preimage resistant functions are preimage resistant  - thoughwe usually expect a higher resistance against second preimage attacksthan given by this reduction from collision resistance due to the genericbirthday attack to find collisions. this means this proof appliesto md5 which is broken collision-wise as long as there is no preimageattack    the proof above does not rely on the hash function being a random oracle,it just uses an oracle access to   which abstracts the timingmeasure for the",2011-11-25 00:05:12,987,9192,timing attacks can have a devastating impact in scenarios where the secret is involved often in cases where byte-wise array comparison is used   now there are those that advertise using constant time array comparison in any situation where data is compared that was derived from a secret not only when comparing the secret itself the rationale is better safe than sorry.although this is fine i m still wondering if it is really necessary all the time  a particular case where i would tend to think that constant time array comparison is probably unnecessary is comparing stored password hashes with those provided by the user let s assume the password was properly stored using for example bcrypt if the application uses short-circuiting array comparison an attacker could gain information about the number of correct bits of arbitrarily chosen passwords by evaluating the time it takes for a response  now my question is whether this can be transformed into a better than brute force attack? is there a possibility for an incremental attack i.e is it possible to construct a password with n+1 correct bits from a password with n correct bits other than using brute-force guessing?  i would be interested in the following   existence of theoretical results that prove that such an attack is in-feasible for one-way compression functions that do  not  assume the hash function to be pseudo-random  negative/positive results for real-world hash functions such as md5 sha-1 etc,0.02330293819655522,23,sse,brute-force|cryptography|hash|timing-attack,timing attacks on password hashes,3,attacks|timing attack|weak password,0.5727206468582153
24593,also you should avoid md5 in favor of a currently-strong hash algorithm such as sha1 sha-256 sha-512  doesn t make a difference your stored data is still based exclusively on the password so there s no additional protection  instead of hashing two times you should use the username as salt for the function   you should also consider using a different function as md5 is considered broken  md5   the weakness of hashed passwords is the attacker s knowledge of your hash function if they know your hash function but not your salt the salt has protected your passwords if they know both the hash function and your salt your data is at risk  as this applies to your question - using dynamic salt generally makes it more difficult to figure out your salt this increases security but won t help if someone knows your algorithm  increasing your complexity in this way does make your system harder to crack nothing is uncrackable given enough resources however  the point of salting is to prevent the use of huge precalculated tables with this method it is possible to calculate the hash of any password without the need to access your database you should store a random value and hash the password and that value together  this is key strengthening  http://en.wikipedia.org/wiki/key_strengthening  a nice technique that nonetheless does not substitute for actual salt  it will not protect you against a rainbow table written with this double-hash function,2009-07-13 16:03:03.67 UTC,314,1120381,say a user registers for your site you hash the password they have chosen then use that hash as a salt and rehash their password with that salt  example   then store endhash in your database would this effective agaisnt rainbow table attacks if my database was comprimized? or am i missing something that would make it easy to break,0.01910828025477707,6,so,database|hash|security,using a hash of what you are hashing as a salt,3,attacks|weakness|protection,0.5689374804496765
6738,how are plaintext and hashes compared?  during the brute force attack words from the dictionary are hashed with the correct hash algorithm and salt and then compared to the hash in the database dump so the attacker needs to know not only the hash value itself but the algorithm and the salt  how does the attacker know the salt?  the salt is generally stored in the database right next to the hash in fact it needs to be how else could the web server hash the incoming passwords when checking them? so if you have a database dump you have the salts  how does the attacker know what algorithm to use?  this can be done in a number of ways   you can often tell by the format or the length of the hash for instance a bcrypt hash usually begins with the marker    try a few very common passwords using the most common algorithms and sooner or later you will get a match since usually only one algorithm is used for all hashes you only need to find one match and then you can go on cracking the other passwords with that algorithm  even simpler if you know just one password maybe you created your own account before the breach you can try different algorithms on that one thanks  goose .  if you have access to the code you ll find the answer there thanks  marcelm .  or if you know what software is being used you might find it in the documentation thanks  eckes ,2018-02-26 12:02:09,405,180535,i am curious about password cracking methods like dictionary and brute force attacks nowadays passwords are stored as hashes and not plaintext on the server then how can the plaintext passwords in the dictionary be compared with the hashes in the leaked database? since the hashes can be of different types like bcrypt sha-512 and so on how can the cracking tools know how to create the hashes and compare it?   for example take a look at the dictionary attack below the leaked passwords are just hashes and the dictionary has simple english words then how can they be compared? how does the attacker or the cracking tools know what hash algorithm it should use? even the salt is there but how does the attacker know what the salt is,0.03209876543209877,13,sse,brute-force|dictionary|hash|password-cracking|salt,how does the attacker know what algorithm and salt to use in a dictionary attack,3,leak|attacks|plaintext password,0.5611431002616882
472,salt doesn t protect you against a lone attacker who is only after one password an attacker who just wants to break one password will calculate   instead of   if the password scheme is      salt helps if the attacker wants to break many passwords this is usually the case sometimes the attacker is attacking a site and wants to break into an account on that site any account without a salt most of the attacker s work can be used for all accounts so she can test each of her attempts against all accounts at once with a  correctly-chosen salt  i.e if no two accounts have the same salt the attacker has to start over for each hashed password  furthermore in a sense all password cracking attempts are attempting to crack all accounts passwords at once that s because hashes can be precomputed all it takes is for someone to generate a table of hashes — or more efficiently a  rainbow table  — and that initial work can be distributed to multiple attackers who can use it on any account database that uses the same password hashing algorithm again salt makes these precomputations useless  a brute-force password attack can be summarized like this   make any precomputation that the attacker deems useful such as building a rainbow table which is an efficient way to represent a table mapping hashes to common passwords  for every one of the  n  accounts that the attacker is interested in breaking in and for every one of the  p  password guesses that the attacker includes in her dictionary test whether     in a naive approach the second step requires  n  ×  p  hash computations to try all guesses against all accounts however if the first step already calculated all the possible hashes then the second step requires no hash computation at all just testing whether each   is in the precomputed database so the attack requires just  n  table lookups this can even be sped up but we ve already gone from  n  x  p  slow computations¹ down to  n  table lookups  if each password has a different salt then in order to be helpful the precomputation would have to include an entry for every possible salt value  if the salt is large enough the precomputation is infeasible  if the precomputation doesn t take the salt into account it won t be useful to speed up the second step because any cryptographic hash function “mixes” its input knowing the hash of   doesn t help compute the hash of   even to attack a single account the attacker needs to perform  p  hash computations to try all guesses already an improvement on the single table lookup that would be sufficient if the attacker has a precomputed dictionary  ¹   a password should not be stored as a hash such as sha-1 but using a slower hash function such as  bcrypt or scrypt or pbkdf2       if your hashes are unsalted i can generate a whole dictionaries worth of hashes and store them in a database special kind called a rainbow table it s more efficient for large numbers of hashes  then all i need to do is look them up basically one giant memory vs cpu time optimization.also if i see two users with the same hash i know they are using the same password  a salt makes every users password hash unique and if you do it right it ll likely be unique even if they use that same password on some other site not uncommon and so i would actually have to take word apply salt and hash it repeat for next entry in dictionary  unsalted hashes are vulnerable to lookup attacks there are freely available databases containing millions of password hashes mainly md5 and sha1 which can be used to look up the plaintext of a hash these may be based on standard relational databases or on specialized database formats like rainbow tables  so here s an example   unsalted   salted with rxb6ae prefix   a quick lookup on the first hash will reveal the plaintext to be passw0rd however the latter isn t very likely to be found in the case where the salt isn t known by the attacker it makes dictionary and bruteforce attacks difficult  if you re looking to secure passwords in a database you should use something like bcrypt it uses a large number of iterations of a hash algorithm to make each computation slow as such bruteforce attacks dictionary attacks and the building of rainbow tables are highly infeasible  i m going to explain each level of security for database stored password maybe this will help clarify the importance of salt   level 1  password stored in clear in the database  this is just  pure stupidity  but sometimes big company get their database compromised and oh surprise all password are stored in clear what a shame!   level 2  password stored using an hashing algorithm  this is a step toward more security if an attacker get the hashed password he can t still login using this credential to the service he will first have to unhash the password  using a  rainbow table  or by  brute forcing  it .that means it require for the attacker a bit more work but it can still be possible to retrieve the original password   level 3  password stored using an hashing algorithm with a salt  this starts to be interesting as we saw on  level 2  an attacker that has access to the hashed password can either brute force it or use a rainbow table adding a salt to the original password make the  rainbow table totally useless  because they don t take into consideration the salt.it still possible to get the original string using a rainbow table but it would mean that in the rainbow table the password+salt exists since a salt is generally very long the odd having a hashed value of a string40+ is almost impossible the only solution left is brute force   level 4  password stored using an hashing algorithm with a salt and a peper  this is very interesting it s the reason why i m posting my answer since the actual ones are already interesting  i recommend you to use an hashing algorithm like bcrypt scrypt or pbkdf2 since they aren t beek broken so far and are very slow to hack increasing the time to break one and then a complete database of passwords  the difference between salt and pepper is their location the salt is generally stored in the database but the pepper is located in the code this can seems odd but the original idea is that a database can be compromised but not the code leaving the attacker a salt and a list of useless hashed password moreover it add even more complexity to the hashed password making  rainbow tables completely useless  if we suppose they were still a bit useful with a salt!  in the case that only the database is compromised even the  brute force is then useless  since the attacker is not looking for one value the original password but for two values the original password and the pepper,2012-04-21 20:26:04,1273,14025,storing the hash of users  passwords e.g in a database is insecure since human passwords are vulnerable to dictionary attacks everyone suggests that this is mitigated via the use of salts but the salt is considered non-sensitive and does not need to be protected    in the event that the attacker has the salt how has his dictionary attack become more difficult than before? does not having access to the salt effectively remove its usefulness,0.02670856245090338,34,sse,attacks|hash|passwords|salt,why is using salt more secure,3,attacks|protection|vulnerability,0.5598987340927124
24606,"nothing stops anyone for building a rainbow table for doubly hashed passwords  i use a comparible method to hash passwords for users that login a salt random value is generated in the session and is sent to the client the user enters their password which is then hashed with the salt and sent back this makes sure that the value sent from the server is different each time making it harder to break in using a man in the middle attack  both iterating the hash and using a salt increase the security of password hashing but they protect against completely different attacks  iterating the hash increases the work required for brute-force attacks but you shouldn t use a naive iteration as you suggest but an algorithm designed for it such as  pbkdf2   a salt protects against pre-calculated tables so it should be different for every website and user  if you don t use a salt then an attacker can build a single rainbow table can be used to attack every password in your database hashing multiple times does not protect you without a salt because rainbow tables work by chaining hashes together in exactly the way you describe    if you add a random salt for each user then the attacker cannot re-use the same table to crack two passwords so their work becomes much harder as an added benefit two users with the same password will hash to different values if a salt is used  your idea of iterating the hash is still good but you need the salt too if you do this   then you make the attacker s work 1000 times harder with a negligible effect on legitimate users note that attackers can test millions of candidate passwords each second on a single low-end computer - hash functions are designed to be fast this 1000 iteration loop can change a feasible attack into one that will take 100 years or more when computers speed up in 18 months time just change the number of iterations to 2000  the salt hashing algorithm and iteration count do not need to be secret and can be stored in your database alongside the computed hash you can choose a fixed iteration count and hash algorithm but the salt  must  be randomly generated for each user  to keep things simple let s imagine everyone uses digits as their passwords  if everyone uses 8 digits as their password that s 100,000,000 possibilities if you re trying to break the system you need to hash all those possibilities if you have a hash of hash of hash you still just need to hash those 100,000,000 possibilities - just in a slightly more complicated way  now let s pretend we have a 4 digit salt as well now instead of 100,000,000 possibilities there are 1,000,000,000,000.. we ve given a potential attacker 10,000 times the work to do instead of just 3 times as much work to do  basically think of a salt as a way of artificially making everyone s password longer and thus extending the space that a dictionary attack has to work on  edit just to be clear given that the salt is provided in plain-text as well you would still only have 100,000,000 possibilities to try to attack  any one hash  however it means that after trying those possibilities for one password the attacker wouldn t have any useful information for attacking another password without a salt an attacker could create a dictionary of 100,000,000 possibilities and then know  all  the passwords in a database given only their hashes in other words salts help to prevent  bulk  attacks they also mean that you can t pregenerate the dictionary in order to attack a single password effectively you have to know the salt beforehand without a salt you could compute the hash of every possible password before you get access to the hashes themselves  you can build a rainbow table based on a dictionary for hashhashpwd in just twice the time as for hashpwd even less because performance is mainly about disc writes and it wouldn t even be larger using salt greatly expands the size needed for the table up to the amount where it becomes impractical  also even more important users often have the same password without an individual salt per user if you ve broken one users password you ve broken all other users that have the same password  the point of the salt is to make dictionary attacks moot now no matter how much you rehash a hash the same input is always going to yield the same output hash and therefore one can build a dictionary for that so while multiple hashing may make it more difficult for brute-force attacks it doesn t do anything for dictionary attacks  the salt is a site- or user-specific value.that means that in order to retrieve the passwords an attacker must have both access to the database and know the salt  in addition the attacker could additionally generate a table once and then use it against multiple sites however with salts attackers must generate one table per site or even once per usermaking the attacks slower  site-specific salts add very little to the security of a website as said in comments having a combination of site-specific and user-specific salts can significantly improve security over just a site-specific salt   a few years ago i asked here on stackoverflow a question about password storage which might be helpful to you see  secure hash and salt for php passwords",2010-12-19 13:37:01.323 UTC,1012,4483160,since the introduction of  rainbow tables  and using only hashed passwords e.x md5 to stored passwords in database is  not the best secured way   when people talk about salted hashes the always use it in this way   or even    i don t know why to use salt and add extra entry for each password to store the salt?why don t we just use   or even  ?  is it more secure to put salt? or just the sense of being more complex,0.02766798418972332,28,so,hash|password-hash|salt|security,hashhash vs salted hash,3,attacks|protection|man in the middle,0.5565167665481567
14816,no -- salt remains effective even if known to the attacker  the idea of salt is that it makes a dictionary attack on a large number of users more difficult without salt the attacker hashes all the words in a dictionary and sees which match with your users  hashed paswords with salt he has to hash each word in the dictionary many times over once for each possible hash value to be certain of having one that fits each user  this multiplication by several thousand or possibly several million depending on how large a salt you use increases the time to hash all the values and the storage need to store the results -- the point that you hope it s impractical  i should add however that in many most? cases a very large salt doesn t really add a lot of security the problem is that if you use say a 24 bit salt ~16 million possible values but have only say a few hundred users the attacker can collect the salt values you re actually using ahead of time then do his dictionary attack for only those values instead of the full ~16 million potential values in short your 24-bit salt adds only a tiny bit of difficulty beyond what a ~8 bit salt would have provided  otoh for a large server google facebook etc. the story is entirely different -- a large salt becomes quite beneficial  salting passwords protects passwords against attacks where the attacker has a list of hashed passwords there are some common hashing algorithms that hackers have tables for that allow them to look up a hash and retrieve the password for this to work the hacker has to have broken into the password storage and stolen the hashes  if the passwords are salted then the attacker must re-generate their hash tables using the hashing algorithm and the salt depending on the hashing algorithm this can take some time to speed things up hackers also use lists of the most common passwords and dictionary words the idea of the salt is to slow an attacker down  the best approach to use a different salt for each password make it long and random and it s ok to store the salt next to each password this really slows an attacker down because they would have to run their hash table generation for each individual password for every combination of common passwords and dictionary words this would make it implausible for an attacker to deduce strong passwords  i had read a good article on this which i can t find now but googling  password salt  gives some good results have a look at  this article   salting is useful even if intruder knows the salt  if passwords are not salted it makes possible to use widely available precomputed  rainbow tables  to quickly attack your passwords  if your password table was salted it makes it very difficult to precompute rainbow tables - it is impractical to create rainbow table for every possible salt  if you use random salt that is different for every password entry and put it in plaintext right next to it it makes very difficult for intruder to attack your passwords short of brute force attack  i would like to point out that the scheme you described with the hard-coded salt is actually  not a salt  instead it works like a key or a pepper salt and pepper solve different problems  a  salt  should be generated randomly for every password and can be stored together with the hashed password in the database it can be stored plain text and fullfills it s purpose even when known to the attacker  a  pepper  is a secret key that will be used for all passwords it will not be stored in the database instead it should be deposited in a safe place if the pepper is known to the attacker it becomes useless  i tried to explain the differences in a small  tutorial  maybe you want to have a look there     makes sense seems like more effort than worth unless its a site of significant worth or importance for an attacker   all sites small or large important or not should take password hashing as high importance   as long as each hash has its own large random salt then yes it does become mostly impracticable if each hash uses an static salt you can use rainbow tables to weed out the users hashs who used password1 for example  using an good hashing algorithm is also important as well using md5 or sha1 is nearly like using plaintext with the mutli gpu setups these days use scrypt if not then bcrypt or if you have to use pbkdf2 then you need the rounds to be very high,2013-02-20 04:27:00.477 UTC,935,14972290,hearing about all the recent hacks at big tech firms it made me wonder their use of password storage  i know salting + hashing is accepted as being generally secure but ever example i ve seen of salting has the salt key hard-coded into the password script which is generally stored on the same server  so is it a logical solution to hash the user s password initially pass that hash to a salting server or some function stored off-site then pass back the salted hash?  the way i i m looking at it is if an intruder gains access to the server or database containing the stored passwords they won t immediately have access to the salt key,0.0213903743315508,20,so,hash|passwords|salt,is salting a password pointless if someone gains access to the salt key? off server salting,3,attacks|hard coded|protection,0.5470029711723328
7880,sha1 is vulnerable to both  collision attacks  and  length extension attacks   if you have a   and   that are multiples of sha1 s 512-bit 64 byte block size probably also work if the same length modulo 64 bytes would not work if you could find a collision with different sized final padding block that you generated in such a way to be a collision   then by construction you will have    using the pdf collisions from the shattered paper   e.g note   and   have different contents evidenced by different md5 hash but were constructed to have an identical sha1 hash  concatenating the same file   to the end of both files keeps them both as sha1 collisions  i should clarify that the shattered attack to generate a sha1 collision needs to modify both   and    if   can t be tampered with and wasn t specifically chosen with a known collision you will need a  preimage attack  not a collision attack  collision attacks ask to find any match where   allowing you to alter both   and   as you search brute-force collision attacks should take osqrt2 n  work because of the birthday paradox -- sha1 s 160-bit hash should take 2 80  ~ 10 24  work for a brute-force collision attack  pre-image attacks require finding a message   such that   for some fixed hash   brute-force pre-image attacks take o2 n  work which in sha1 s case would be 2 160  ~ 10 48  work that is 10 24  or a million million million million times harder than a collision attack,2019-04-11 16:23:49,437,207257,let s say i have an input   of enoughly long length i ll got for a 128 bytes long message but it might be 256 or whatever  i know the hash   where   is byte concatenation  since the   is long enough and knowing how   works i can tell that the   will be split into chunks each chunk will be hashed leading to an internal state machine reused to hash next chunk and only the internal state before  hashing the first chunk involving the   actually matters to the result hash  so in theory this means i could tamper the   into another thing   and have the same hash result   so long i can keep the internal state unchanged  is there a way to do so? is there a way to built a   so that   when i know   and fully control   including its length? if there is how to do so? it s not a length extension attack but it feels like the same kind of process could be used but i can t find it,0.034324942791762014,15,sse,hash|sha,can one alter sha-1 input data without changing the hash internal state,3,attacks|shatter|vulnerability,0.5429952144622803
67027,if i try to create collisions for md5 i can make one every 14 seconds on average on my pc using a single core core2 2.4 ghz this exploits the weaknesses in the internal structure of md5 if i only try random data and wait for collisions to appear well i will wait for quite some time the first collision is expected after about 2 64  hashed messages give me a thousand pc and i should achieve a collision in about 20 years of full-time computation  for currently unbroken cryptographic hash functions there is no known internal weakness that s what unbroken means so trying random messages is the best known method to create collisions chances to get a collision this way are vanishingly small until you hash at least 2 n/2  messages for a hash function with a n-bit output this means that with any proper hash function with an output of 256 bits or more the collision rate is in practical conditions zero you will not get any and that s the end of the story   wikipedia  has some pointers on the subject see also chapter 9 of the  handbook of applied cryptography  page 369,2011-04-18 09:26:22,254,3144,is there any collision rate measure for popular hashing algorithms md5 crc32 sha-*?  if that depends only from output size it s quite trivial to measure but i suppose that depends also of distribution and algorithm s internals and it demands some kind of formal proof i think,0.015748031496062992,4,sse,attacks|cryptography|hash,collision rate for different hash algorithms,3,attacks|exploit|weakness,0.5397630333900452
15101,now this doesn t seem like a programming question so i ll just give you some info on salting and encryption  the purpose of salting is to aid in one-way functions like hashing which is used widely in cryptography often in use of passwords because of its difficulty to guess and time it takes for other attacks like brute-force attacks to crack them  if you want to securely store passwords the best way is definitely encryption look up encryption on wikipedia for more info on that  if the  attacker  has the password hash and salt used by your site/app they will simply brute force salt + password  however using a salt offers more protection against rainbow tables precalculated hash tables so they re still worth using  for single passwords it doesn t make that much of a difference brute-forcing an unsalted password is just as hard as brute-forcing a salted password you just try out keys until you get a hit  the difference is when there are a lot of passwords for example in a leaked database the basic idea is that part of the necessary computations can be re-used when cracking many passwords this is done by constructing a rainbow table doing that is computationally expensive but once done it allows the attacker to crack a lot of passwords relatively fast cracking   passwords with a rainbow table is a lot faster than brute-forcing those   passwords individually  if every password is hashed with an individual salt you can t re-use information in the same way you could still construct rainbow tables but they would only be usable for exactly one password in the database which renders them useless so in order to crack   passwords you really have to brute-force all   passwords individually which is usually not practical for the attacker  for unsalted passwords and popular hash algorithms you can simply download pre-calculated rainbow tables from the internet so an attacker wouldn t even have to calculate them by himself he can just download a table and lookup the password for a particular hash a salt prevents that  unsalted hashes also have the drawback that the password hash for two users with the same password is identical so if an attacker finds multiple users with the same password hash he only has to crack that password once  salt makes the encryption stronger however dictionary attacks don t try to decrypt the password hash so salt or no salt it doesn t matter they will just try out many passwords until one works  salts prevent instant cracking from a dictionary via rainbow tables the article and follow-up make the point that the cpu/storage tradeoff is now such that rainbow tables don t make sense and so salts don t help you  and of course they never helped with brute-force attacks  this belongs on security.stackexchange.com  the problem is one of compute capacity in combination with the speed of the hashing algorithm  basically he s pitching bcrypt which is slow    if a hacker has both the hash and salt used as well as knows the algorithm used to hash the password then it s simply a matter of time to crack it  if using a very fast algorithm then that time is pretty short  if using an extremely slow algorithm then the time is obviously much longer to find a hit  which brings us to the primary reason why we hash/salt things in the first place to buy time  time that can be used in order to change all of the passwords listed and time to contact all of the users to let them know in case they need to change their passwords on other systems  the reason we use salt is to force the hacker to build a rainbow table  per salt value   this way one table can t be used to crack all of your passwords  the only reasons to do this are to buy time and hopefully dissuade the common hackers from investing further resources in cracking all of them    hashed passwords regardless of mechanism used are not secure in the sense that most people take that word  secure doesn t mean can never be cracked  rather it means this is going to be expensive in term of time/effort to crack  for most hackers they want low hanging fruit such as clear text only  for some they ll go to whatever extreme is required such as building massive rainbow tables per salt value to get them all    and of course underpinning this is whether any super user accounts are easily identified in your user table  for most systems just cracking the sys admin type of account is good enough and therefore the fact of using a different salt value per user is immaterial  the smart ones will just bother with that one account   it is not entirely accurate as with most things it depends on your assumption  main assumption are   attacker has salt  calculation of hashes on the fly are done pretty quick as with salt he will need to recalculate all and wont be able to use predefined lists  same salt for each user   two comments    regular hash algorithms can be iterated there is no need to use a non-standard algorithm just because you want to increase the work factor    using a salt is to be recommended even if you use a slow hash method it might not necessarily increase the work load of the best attack but it will stop trivial attacks in case a user chooses a password identical to that of another user another account or to an old password    for illustration purposes say you are using 2 character string for salts which can be a random element from the set      the formula you use is   thereafter you ll save the hash and salt in your database something like   we assume that in a compromised system an attacker has access to your code - so he knows how you calculated your hashes the attacker will also have access to your database so he has all the password hashes and the salts  given this information in order to do to crack your password which has a hash  dai480hgld0  he ll have to do the following   note that if you d have  not used any salt  at all the algorithm would have been    from the above two code samples its obvious that adding a salt to the password increases the number of attempts in the brute force attack in our case since there are 100 possible salts you ve made the attacker try each word with 100 salts  so to conclude    salts are good they make your passwords tough to crack even if your users enter weak passwords the salt makes sure that the resultant hashes are not googlable for eg its easy to google a hash  3cc31cd246149aec68079241e71e98f6  which is actually a password that is fairly complex and will meet almost all password policies still cracking it requires not a single line of code !    salts are not panacea they just increase the time it takes for a cracker to brute force your passwords however if your salt address space is fairly big then you are pretty good for eg if you have 32 characters alphanumeric string as a salt - brute force will really take very long     slow algorithms like bcrypt help you in this regard just because they are well..  slow  for a brute force attack it will take unrealistically long to break hashes that are slow to compute,2012-02-09 23:21:26.307 UTC,1295,9220509,from this site  http://codahale.com/how-to-safely-store-a-password/      it’s important to note that salts are useless for preventing dictionary attacks or brute force attacks   if salt is useless to prevent dictionary attack why using salt,0.018532818532818532,24,so,hash|saltedhash|security,why salt did not help when using dictionary attack,4,leak|attacks|protection|weak password,0.539319634437561
22172,"psuedocode for the slowest method   this certainly isn t the most secure or at all but it s slow..  i m just pointing out that making it slower is not the answer...  actually the best hash function is the one that generates no colisions and is not suspectible to rainbow-table attacks  that means add a salt preferably a different salt for every user and think of using a sha2 hash function or maybe ripe-md i have not looked at that much  one implementation of sha-256 is here i love how they call it one-way encryption  http://www.freevbcode.com/showcode.asp?id=2565   have not tested it though but there are certainly sha2 implementations for classic asp  a lot of people seem to be beating on the question-asker because he s looking for a slow hash function actually all other aspects being equal a slower hash function is more secure than a fast one this is because a slower hash function results in  slower generation of rainbow tables  and  slower brute forcing or dictionary attacks on the password   from thomas ptacek at  http://www.securityfocus.com/blogs/262  as referenced in this  coding horror article      the problem is that  md5 is fast so  are its modern competitors like sha1  and sha256  speed is a design goal of  a modern secure hash because hashes  are a building block of almost every  cryptosystem and usually get  demand-executed on a per-packet or  per-message basis       speed is exactly what you don’t want  in a password hash function       modern password schemes are attacked  with incremental password crackers      incremental crackers don’t  precalculate all possible cracked  passwords they consider each password  hash individually and they feed their  dictionary through the password hash  function the same way your php login  page would rainbow table crackers  like ophcrack use space to attack  passwords incremental crackers like  john the ripper crack and lc5 work  with time statistics and compute      the password attack game is scored in  time taken to crack password x with  rainbow tables that time depends on  how big your table needs to be and how  fast you can search it with  incremental crackers the time depends  on how fast you can make the password  hash function run      the better you can optimize your  password hash function the faster  your password hash function gets the  weaker your scheme is md5 and sha1,  even conventional block ciphers like  des are designed to be fast md5,  sha1 and des are weak password  hashes on modern cpus raw crypto  building blocks like des and md5 can  be bitsliced vectorized and  parallelized to make password searches  lightning fast game-over fpga  implementations cost only hundreds of  dollars   some  comments on the php md5 documentation  also discuss preference for slowness  to answer your question it looks like bcrypt is the way to go however i have not been able to find any implementations for asp classic if that s true i would stick with a regular hash function like sha512  i ll ignore the slow part and instead go for the good part  i suggest you use sha-512 with a salt to defeat dictionary and rainbow table attacks i don t believe there are any known vulnerabilities for sha-512  if you are trying to defeat brute force attacks you are better off enforcing some failed attempts window/count rather than relying on the speed of the hashing or hash comparison mechanism to make the attack take longer to succeed  lock out the account after a certain number of failed attempts within the failure window and only let new attempts be made after a significant amount of time has elapsed  this could leave you open to a dos attack against a well-known administrative account but you could exempt certain accounts from the lockout policy or have an alternate way -- using a security question/answer -- to logon to a locked out account before the reset period has elapsed  [edit] to help defeat rainbow attacks -- where the attacker has retrieved your hashed passwords and finds suitable matches that hash to the same values -- consider both using a random salt unique to each user s hashed password and a fixed salt that is part of the algorithm not the data  for example   this should invalidate the rainbow tables since even knowing the user s salt and the hash algorithm the values in the attacker s rainbow tables won t map onto your hashed passwords because of the addition of the fixed salt in the algorithm  with asp classic you d have to do this in a library instead of on the page to make sure that the user couldn t see your fixed salt  i personally prefer the  whirlpool algorithm  for all of my hashing needs  it produces a 512 bit output and thus has equal space requirements to sha-512  regretfully i cannot speak authoritatively as to whether one is more secure than the other but there do not appear to be any flagrant weaknesses in this the third version of whirlpool  the reference implementations are in the public domain which is good because i rarely find this implemented by default in various tools and languages  if anyone knows of a good reason why sha should be favored over whirlpool please let me know.    capicom documentation   algorithm is any of the following",2008-10-31 14:19:16.277 UTC,987,253673,what is the slowest therefore best hash algorithm for passwords in asp classic?  edit for those unaware when hashing passwords slower hashes are preferred to faster to help slow rainbow table style attacks   edit2 and yes of course speed isn t the only valid concern for hash selection my question assumes that  all other things being equal   the slowest hash method is preferred  when hashing a password though collision/reverse engineering is of course a concern too i m prioritizing speed in this question since it is arguably the most critical factor to consider when comparing popular hash algorithms for use on passwords  thanks,0.0182370820668693,18,so,asp-classic|hash|security,recommended hash for passwords in asp classic,5,attacks|weakness|weak password|denial of service|known vulnerabilities,0.5346770882606506
21086,in a word yes  salting a password adds a level of complexity to the string and confuses humans and makes dictionary attacks less likely to succeed  brute force can still crack this password however hence the need for a randomly generated salt   salts are typically generated via byte-arrays which is then fed into a function to combine the two strings into one at intervals see my answer  here   you are right in a way but .. the most significant protection from salt is that if the hashes ever do get released into the wild then reverse hash lookups are much much harder  hash a word and then put the hash result into your favourite search engine to see what i mean  yes there is a point in salting a password  the point is that each password has its own salt so that an attacker can t make use of dictionaries and rainbow tables to brute force all passwords at once  the salt doesn t make it harder to crack a single password¹ but it removes the benefit from attempting to crack multiple passwords at once an attacker has to brute force one password at a time   ¹ at least not enough to be a good reason to use it using better passwords works much better  the hashes may be leaked without the salt common scenario database gets dumped but a salt i present in php source that does not leak,2015-05-08 07:40:14.613 UTC,306,30118426,is there really a point in salting a password?  if a program does all the processing of a salt server side then does it really make it any more difficult for brute force or other attack the code is only going to apply the salt to whatever is entered by a user  do i have this all wrong,0.02287581699346405,7,so,hash|passwords|salt,adding salt to a password,3,leak|attacks|protection,0.5315715074539185
14445,when discussing the recent linkedin leak somebody brought up this  link about bcrypt  i think i agree.. we should be using functions that increase the calculation time exponentially according to a factor that s the only way we can beat people trying to use clusters or gpus to do their hashing calculations   generate random salt for each password  avoid md5 and even sha-1  use a slow hashing algorithm sha-256 seems to be a good choice for now  password storage is one of those rare occasions where there is some benefit to having your own overall algorithm  consider an attacker with a rainbow table if your password storage algorithm varies from the one used to generate their rainbow table enough to change the generated values that rainbow table is of no use  the attacker would need to know your algorithm then generate a new table  if you choose a slow hashing algorithm generating a new table is very expensive   by overall algorithm i mean the complete definition of how you transform the plaintext password into the stored value  e.g    if you change that to use angle brackets instead of square brackets you ve changed the rainbow table necessary to exploit your stored passwords  note that i m not advocating writing your own hash algorithm you should still choose a cryptographically secure hash algorithm  see  this article  by the author of md5  he makes the two main points i repeated above 1 if you use a fast hashing algorithm you re missing the point and 2 reuse of overall algorithms allows re-use of rainbow tables  my understanding is that repeated hashing for computational cost &amp a good random salt should defeat all but seriously determined cryptographic attackers  hashing passwords in the database and over the network avoids plaintext being recoverable and usable elsewhere by a snooper or attacker who does get in  basically this is more or less the scheme used by the wordpress authentication   this use of a random salt and looping hash defeats any rainbow tables or single-level  hash collision   hash weakness  attack only brute-forcing the complete keyspace each key through 1000 iterations of the hash function is believed to defeat it ,2012-06-07 00:03:17.227 UTC,452,10923985,during a discussion with a couple of other people i read the argument that   sha512salt + username + password is bad   sha512username + password is worse and   sha512password is plain idiotic   while i partly agree what s really the best security? is there anything safer than using an user unique salt along with a slow hashing method such as sha512? what s the real way to go? argue on!   please edit the title if you find it bad,0.01991150442477876,9,so,hash|salt|security|sha,proper way to use salt along with hashing,5,leak|attacks|exploit|weakness|plaintext password,0.5313693881034851
8360,"storing the salt together with the hash is fine the salt is essentially public information many systems store it in a format like       but isn t this insecure?   no because the point of a salt is not to be secret the point for the salt is to be random and unique it should increase the work factor for the attacker not prevent an attack completely let me demonstrate this  scenario 1  your database contains 1,000,000 users all just having their hashes stored in plain sha-1 an attacker compromises it their possibilities are now   perform a lookup-attack meaning that the attacker just looks up the hash in a database of pre-computed hashes  perform an attack with a rainbow-table which is a variant of the above-mentioned attack  hash a well-known list of passwords themselves e.g rockyou.txt exactly once  brute-force weak passwords or password in a common scheme like an uppercase character followed by 8 lowercase characters and one digit   scenario 2  your database contains 1,000,000 users all just having their hashes stored in argon2 with a unique random salt an attacker compromises it their possibilities are now   hash a well-known list of passwords themselves e.g rockyou.txt 1,000,000 times  brute-force weak passwords or password in a common scheme like an uppercase character followed by 8 lowercase characters and one digit - 1,000,000 times   it s not possible to do a lookup-attack because nobody would store the hashes to millions of password with millions of possible salt combinations it s also not possible to do a rainbow-table attack because building the rainbow table for every user with their salt would take longer than to just computer each hash  furthermore just by adding a salt you changed the amount of work required from hash once check everything to hash every time in our scenario with 1,000,000 users you made the task of the attacker 1,000,000 times more difficult",2019-10-29 09:28:53,456,220371,i want to hash a set of sensitive data using a hash+salt i ve done a little research and now i know that   the salt should be random  the salt should not be reused  using hashing without salt makes it possible to use rainbow tables and dictionary attacks using the top 10000 passwords hashingthem and finding matches   i ve seen alot of libraries that store the salt alongside the data in some cases as a prefix for the hash i.e salt.hash  i m confused as to where you should leave the salt storing the salt with the hash itself seems illogical what if the attacker reads the salt from the field hashes his dictionary with the salt and then finds a match it seems as just one easy extra step,0.03508771929824561,16,sse,hash|passwords|salt,password hashing with salt where do you leave the salt,3,attacks|weak password|sensitive data,0.5311626195907593
26944,your assumption is correct account lockout is more about mitigating educated guessing or brute force over the internet it also helps prevent someone from dosing your server if your hashing algorithm  is  very expensive   the weakness of md5 comes from when you want to break the hash locally usually after having stolen the database where the hashes are kept or getting a dump of the hashes from the db when it comes to attacking over the internet the bottleneck comes from the network not necessarily the algorithm we still don t want people to be able to try as people tend to use very weak passwords and a dictionary attack could break someone s password in a reasonable amount of time.. even over the internet   lockout is probably not a bad idea but as gray mentioned network speed will make a bigger difference unless and until an attacker gets offline access to the hashes  the salt is a big help as long as the salts are stored separately from the hashes  if they are not then they will be stolen if and when the hashes are stolen so a dictionary attack can be used appending the salt to each dictionary try  a thing against which lockout does nothing is an attack in which the attacker does not care whose account he gets into so long as he gets into one  then the attacker can pick the password to try and cycle through user names never stressing the server enough to trigger the lockout  nevertheless lockouts can be helpful  or a delay of say 5 seconds between tries  for someone who needs to guess 5 times how he spelled his password it is an inconvenience  for an attacker those 5 seconds times all the attack attempts he would have tried will be a major hassle  again only as long as they cannot access your hashed passwords and your salts offline,2015-07-23 15:56:38.3 UTC,407,31592439,i understand that an md5 salt+password hash is vulnerable to brute force attacks due to how fast the md5 hash can be generated does locking users out after multiple failed login attempts do anything to address this vulnerability?  my assumption is that the lockout feature is irrelevant in terms of the md5 speed vulnerability  moreover i assume that one must first attain the password hash before attempting to break it using brute force,0.03931203931203931,16,so,cryptography|hash|password-protection|passwords|security,md5 password hash with user lockout on failed login attempts,5,attacks|weakness|protection|vulnerability|weak password,0.530632734298706
20376,is a one-way string hash not an encryption mechanism to use an sha-512 hash you have to use the   function bcrypt requires a php extension for storing passwords why do you want to make them reversible rather than just hashing them? that s less secure -- if someone gets your key and db they have all the passwords but a table of sha512 hashes is pretty useless  actually the answer has to be  no  it doesn t make the hash significant stronger in a cryptographically sense as you probably know bcrypt although the function to use is named   is a hash function itself not an encryption function  in bcrypt you pass a cost factor which defines how many iterations will be done normally hundreds of them that  slows down  calculation of the hash what makes brute force attacks impracticable using sha-512 before will only add one iteration more  what you said about the  salt  is correct but of course if you have to build a rainbow table for each password you will simply brute force until you have found a match no need to build the whole rainbow table  if the attacker has control over  database and code  an additional sha-512 will help nothing at all only a single iteration more if he has only the  database without code  sql-injection then he will recognize the bcrypt hash he can now brute force with bcrypt but because of the sha-512 there aren t any weak passwords it s like the sha-512 hash would be the password to crack so a dictionary is of no use this is security by obscurity but will be effective as long as the code is not known you can get the same effect easier by adding a fix hard coded salt key before using bcrypt with the unique salt  bcrypt already uses salt and what it s doing internally is quite a bit stronger than sha512 adding an iteration of sha512 and/or an extra layer of salt on top of bcrypt will not give you a significantly stronger result if the two functions interact in the wrong way combining them in this way may in fact give you a hash function that is weaker  hashing first won t help a bad password is one that is deemed more probable by an attacker and placed earlier in his list of passwords to try  bcrypt incorporates salt to eliminate pre-computed lookup tables a rainbow table is one example why would an attacker build a rainbow table for a single record? no when attacking a salted password an attacker simply works through his ordered list of most likely passwords repeating the hash algorithm to see if it matches  how far he can work through that list depends on how long the hash algorithm takes to execute bcrypt controls that with the cost factor&mdash;12 in your example which is okay but probably the minimum i d use an extra sha-512 round doesn t add anything to that you are already performing 4096 expensive bcrypt iterations adding 1 cheap sha-512 iteration is negligible  if you choose the first password on the list it will be broken in a fraction of a second if you pick the billionth password the attacker won t break it for a few decades,2012-06-30 17:14:07.567 UTC,674,11276026,i want to use bcrypt for the password encryption in my systems but all the examples are something like this   this looks pretty safe to me but i was wondering in each example nobody hashes the password before using bcrypt  due to the unique salt rainbow tables shouldn t be able to crack all the passwords at once but in case the hacker takes one record and creates a rainbow table with the salt of that particular record he should be able to crack a weak password  so if someone takes a weak password let s say  foo  it would be safer to hash it first with sha-512 before using bcrypt am i right? or is this just looking safer,0.019287833827893175,13,so,bcrypt|php|sha512,using 512-hash before bcrypt,5,attacks|weakness|hard coded|weak password|sql injection,0.5291648507118225
58031,what you could do   this generates a uniformly distributed double precision ieee754 number so every possible value in the whole space is equally possible to get with this code including  s it s quite a lot of  s there actually -   and  s  references   the initial implementation of the   function borrowed from  https://stackoverflow.com/a/21282715/251311    randomsource.getrandomvalues,2017-03-28 20:57:06.127 UTC,101,43079804,i am trying to generate a random number that range from number.min_value to number.max_value but the following algorithm fails due to buffer overflows i guess             is there any way to do it ,0.019801980198019802,2,so,algorithm|buffer-overflow|floating-point|javascript|random,can i generate a random float on the whole space,1,buffer overflow,0.5283259153366089
2599,in theory  if the attacker has not built a rainbow table if a password of some strength can be hacked in 1 unit of time then n passwords of that strength can be hacked in o1 time without salt and on time with salt that s what salt does no advantage on a fixed password shows advantage over multiple passwords  if the attacker has a rainbow table without salt a weak password - in this case weak means already in the rainbow table - is as good as naked with salt it will still need to be cracked in however many seconds that might take  in practice password strength is way more important than any of this sometimes rainbow tables are mere convenience if you have a million passwords that weak they re permeable even salted  you have to consider two attack vectors   online attack  offline attack    limiting login guessing helps against online attacks  let s say it s three times this means that an attacker can test all accounts for the three most common passwords that fit your password policy how about password 12345678 and 12345?   salting helps against offline attacks  same unsalted hashes are the same cleartext password so every hash has to be computed once not once for each salt even with a salt the attacker can still try a dictionary attack and no one except his limit of computing power will stop him because your three strikes rule won t apply here   weak passwords reduce security in both cases   online attack  if you allow common passwords like password or 12345 attackers will be able to break into 5% of your accounts in three guesses  offline attack  here they will also try the most common passwords first of course so if they compute 3 hashes per user they have already broken 5% of the accounts..  if you picked one of the three weak passwords love dog and cow and a password database was lost without salts i can try these three passwords and find immediately  everybody  in the whole database using these three weak password with salting i have to try love+your salt dog+your salt cow+your salt to crack  your  password if it was ridiculously weak then repeat the same with everybody in the database   if a hacker attacks you specifically then salting doesn t help it only helps by making it impossible to attack the whole database instead requiring an attack against each individual user in the database   no one point of salted hashing is to get good randomness in the hash regardless of the starting material  however this does not free us to use bad passwords  good hashing only protects against one attack vector that where the intruder steals the file with the hashes  so many other attack vectors on passwords exist.. shoulder surfing brute forcing modified brute forcing with intelligent starting guesses and so on.. that good passwords are still important  salted hashing will protect good passwords but won t rescue you from bad ones  salted hashes are designed to protect against attackers being able to attack multiple hashes simultaneously or build rainbow tables of pre-calculated hash values  that is all  they do nothing to improve the underlying strength of the password itself weak or strong     this also means that they re not designed to defend against online attacks so they have no impact on an attackers ability to manipulate your login form where the salt is irrelevant because an attacker isn t computing hashes directly but entering candidate passwords into a form that may be as you said rate limited or protected by a captcha    weak passwords are weak  strong passwords are strong  salts don t affect this equation in any way    salting/hashing is great if your database gets stolen but it has nothing to do with dictionary attacks that might take place through the normal login procedure   as you mentioned limiting the number login attempts and using captcha can make dictionary attacks that take place through the normal login procedure ineffective but salting or not won t have any effect on that type of attack  even with the limited logins attempts and captcha in place allowing super weak passwords is not a good idea and if a malicious user is determined enough or sees enough value they can find ways to exploit that weakness  generally the most important thing is password length but it is true that an easy password could be broken down easier than a random one for example when you are trying to guess a hash using  rainbow tables  if it is a normally used word like cat it is more likely that you can have it in this table than 07ofmy3hoy3l9e1gcnww7nnpd5lq8i9an ;d,2014-10-27 21:14:40,892,71761,when you have a password stored in a database that has been strongly hashed and salted does it really matter if the underlying user password is weak?  if you setup features like limiting login guessing and use captchas to stop automated guessing can you effectively make up for a weak password such as  ?  i guess my question is does using a password like password make the salted hash any weaker than using a longer password such as  ? - are all salted hashes equally as secure or does it depend upon the password being a good one,0.0515695067264574,46,sse,hash|password-policy|passwords,do bad passwords produce bad salted hashes,5,attacks|exploit|weakness|protection|weak password,0.5266767740249634
67009,,2019-02-27 21:50:33,70,204435,i was told that you can dos a server with unlimited password length because of the time that it would take to hash it but isn t the password hashed on the client s side before leaving the user s machine and the server s only work wouldn t be just to store that hash,0.05714285714285714,4,sse,denial-of-service|hash|passwords|web,does limiting the length of a password help prevent dos attacks,2,attacks|denial of service,0.5266767740249634
26455,"salts are not something you have to strive to keep secret their protection is effective even when known  https://crackstation.net/hashing-security.htm      the salt does not need to be secret just by randomizing the hashes lookup tables reverse lookup tables and rainbow tables become ineffective an attacker won t know in advance what the salt will be so they can t pre-compute a lookup table or rainbow table if each user s password is hashed with a different salt the reverse lookup table attack won t work either   since you appear to be using a fixed salt value heed this     a common mistake is to use the same salt in each hash either the salt is hard-coded into the program or is generated randomly once this is ineffective because if two users have the same password they ll still have the same hash an attacker can still use a reverse lookup table attack to run a dictionary attack on every hash at the same time they just have to apply the salt to each password guess before they hash it if the salt is hard-coded into a popular product lookup tables and rainbow tables can be built for that salt to make it easier to crack hashes generated by the product   i d suggest using   without its optional parameters the default function is built to be highly secure and by specifying your own algorithm and options you potentially weaken its function  per the  php documentation       caution  it is strongly recommended that you do not generate your own salt for this function it will create a secure salt automatically for you if you do not specify one    edit  here s why keeping the salt secret in bcrypt is pointless  per  this post  there are 3,025,989,069,143,040 possible combinations in an 8 character password you should generally tune bcrypt s work factor to take 0.1 seconds to hash a password this means calculating all possibilities takes 302,598,906,914,304 seconds that is 9,588  millennia   the salt isn t a secret it s generally stored in the database with the hash and could just as well be stored directly in the hash like   does  the salt creates uniqueness so the hash can t easily be cracked with things like rainbow tables or dictionaries it doesn t really add security other than making the hash more unique so running a dictionary or table against the hash doesn t match because it also includes the salt  if you omit the salt a random salt will be generated by   for each password hashed this is the intended mode of operation and you shouldn t supply your own salts php7 will actually produce a warning telling you that using the salt option is deprecated  the salt passed needs to be at least 22 characters but most underlying algorithms like bcrypt doesn t use the entire salt see   this answer   for more on that",2015-06-07 00:46:18.52 UTC,572,30688957,i am checking out the bcrypt hash-algorithm  my first test with password_hash   both will return  $2y$10$123456789012345678901uialpjxtpf6vbfi5nadlsrsfvem6aq9c    why the heck is the salt stored inside of the hash? this makes no sense at all for me a attacker who gets the hashes from a database can´t do anything with them if he does not know the salt  why do i get the same hash with two different salts? are only the first 22 chars used for the salt passed to the function?   thank you very much,0.017482517482517484,10,so,bcrypt|hash|php,php password_hash / bcrypt,4,attacks|weakness|protection|hard coded,0.5260829329490662
15039,any hash that satisfies the  strict avalanche criterion  that is if any bit is flipped in the input every bit in the output will be flipped with a probability of 50% may be used in this way and that includes every cryptographic hash in common use including sha512 there are security implications to using very short hashes but if they really aren t relevant as you claim you re free to select the fastest hash available probably md5  since short hashes will be particularly vulnerable to the birthday paradox though consider using longer hashes anyway if you re generating so many hashes that 16 bits versus 256 bits is significant you  will  run into duplicates even without malicious attackers,2013-04-04 20:05:24.95 UTC,182,15820699,i was thinking of just using sha256 and then using only the first two bytes of the result is there anything wrong with this approach?  note the concern here is not malicious attacks but to ensure the best possible protection against random bit flips,0.02197802197802198,4,so,c|cryptographic-hash-function|openssl|sha|sha256,what is the best cryptographic hash function that generates 16-bit hashes values in openssl,3,attacks|protection|vulnerability,0.5255541205406189
24456,,2019-01-15 08:02:26.79 UTC,51,54194777,i have implemented argon2 hashing algorithm for password hashing i am worry about my code it may vulnerable to timing attack   if user not found then response time less than success time    passwordencoder.class,0.09803921568627451,5,so,argon2-ffi|java|owasp|timing-attack,is it necessary to worry about timing attacks when comparing sha256 or argon2 hashes,4,owasp|attacks|timing attack|vulnerability,0.5249543786048889
11972,you don t want to use md5 or any simple hash to store passwords in a database you want a  good salt  and  bcrypt .the  phpass  library provide a good portable way to implement a not too weak password storage  those are all different algorithms some of them are cryptographic hashes some are simple checksums such as crc32 and adler32 that are very fast to compute but should never be used for cryptographic purposes  md5 and sha-1 used to be the standard cryptographic hashes but recently weaknesses have been found in both you re probably best off using the newer sha-256 for cryptographic purposes the other new sha variants use fewer or more bits but aren t fundamentally different  you can probably find more detailed information about most of those algorightms on wikipedia  the differences are that of the algorithm used which also determines the size of the output e.g md5 produces 128bit output sha 160 bits  md5 and sha1 have weakness that have been discovered collisions in the hash space though for most purposes md5 is sufficient unless you re working on banking site  you must however use a salt regardless of the hash algorithm used just using the md5 of the password for example leaves you potentially vulnerable to a rainbow attack  edit this is more of a crypto question than php per se,2010-06-28 10:36:41.88 UTC,274,3131651,what is the difference between the hashing methods available in php   i generally use md5 to store passwords in my db.  i searched for it but i couldn t get exact advantages and disadvantages,0.01824817518248175,5,so,cryptography|hash-function|php|security,what is the diff between the hashing methods of php,4,attacks|weakness|weak password|vulnerability,0.5230667591094971
1141,your valu is the count multipli by max valu of a sign bigint is 2 63 -1 = 9223372036854775807 so if your count is greater than 4 then it will exceed the max valu that can be repres in a sign bigint,2020-01-28 23:55:06,70,59958822,i look at some of the other answer for similar question but none of them seem to help me,0.014285714285714285,1,so,integer-overflow|mysql,mysql bigint valu is out of rang,1,integer overflow,0.5225163698196411
67201,this  answer on crypto.se outlines the attack anders points out in the comments even if there is a salt it s still best practice to make it a time-safe comparison simply because salts are assumed to be public information though in practice it seems unlikely it d be a big issue unless you divulge the salt without also divulging the password hash  your assumption about this attack requiring a first pre-image attack on the hash seems logical but it has a major problem the pre-image space that matters is the pre-image space of hashed passwords not of the hash itself i.e the preimage of   where   is a password rather than   where   is any data this is a problem even with a slow hash because that pre-image space can be much smaller such as trying the 2 20  most re-used passwords vs eg 2 128  for the hash itself,2018-05-11 12:05:31,369,185629,i recently came across some password code that hashed the password and then compared it with the saved hash in the naive way one character at a time short-circuiting as soon as a non-match was found we agreed it was a security bug and fixed it  however this code piqued my curiosity suppose an attacker is able to glean from the timing of password checking the number of hash digits that match how could this bug be exploited in practice? i understand security should be deep and you need not find a practical exploit to take a security measure - i m asking purely out of curiosity  for example the attacker might try progressively extending the hash one character at a time but if the hash is n digits long the attacker eventually needs to extend from n-1 digits to n digits assuming a good hash function this seems to require a full pre-image attack assuming the attacker can compute the hash which requires the attacker to have the salt if there is one this attack could largely be performed client-side given n digits in base b a maximum of n*b queries to the server are required but hash functions are chosen to resist pre-image attacks so is this exploit practical,0.04336043360433604,16,sse,attacks|hash|passwords,how can short-circuit hash equality be exploited,2,attacks|exploit,0.5180338621139526
3363,either option would contribut toward mitig time attack rememb though that some of those method will take vari amount of time to complet specif the databas lookup anoth way you could contribut toward mitig time attack is to add a random delaya to success login callsc and anoth random delayb to fail login call if you make the delay a a randommax_jitter_amount and if you can accur calcul the averag time taken by c then your random delay b would be someth like avg_timec+randommax jitter amount thi way everi singl request will return a certain amount of time + a random jitter amount which would defeat statist analysi,2020-06-12 04:49:06,177,233137,consid thi common exampl use to demonstr time attack the usual suggest is to do the same thing on all execut branch for exampl someth like thi but i wonder if the follow strategi is also use against time attack not consid other type of side-channel attack while not wast comput resourc,0.03954802259887006,7,sse,hash|passwords|side-channel|timing-attack|web-application,use delay with a fix total time to defend against time attack,2,attacks|timing attack,0.5180140733718872
67099,your understanding of the reasons why one should use scrypt and bcrypt is correct  yes you could at least in theory produce a new algorithm that requires time and memory and it could incorporate an existing cryptographically secure hash function  you can increase time through iteration and increase memory by requiring large numbers of prior values to be available to compute the next value  but why?  as you point out there are already functions that do this designed by cryptographers subjected to inspection by others and that have withstood the test of time  trying to roll your own crypto is very dangerous  your question tells why there is no assumed shortcut  it is not enough for an algorithm to be time and memory intensive it must also be resistant to shortcut attacks  shortcut attacks can be subtle and clever and avoiding them or even spotting the vulnerabilities is hard especially in your own work which you know is correct  you are far more likely to end up with what someone else has called wacky hash functions that turn out to have easy shortcuts  if you want to design your own cryptographic algorithms first get a ph.d in math with a concentration in crypto  the nsa will hire you when you ve got that ph.d  after that spend ten years working on crypto at nsa and you  might  be qualified to develop new cryptographic algorithms  making a hash function iterative already exists it is called  pbkdf2  bcrypt is still  preferable  because pbkdf2 can be thoroughly optimized on gpu  designing a good password hashing function is a difficult job but yes existing hash function are good building elements so they are likely to be involved at some point indeed look at  scrypt  it starts and ends with a pbkdf2 invocation hence a lot of hashing but the memory hardness comes from what happens between these two hashing phases it is not a matter of a  simple assemblage  of hash functions that would provide memory hardness just like a car is not simply wheels that go where you want the wheels are essential but there is more in a car than wheels  cryptographers are currently busy designing and analysing new password hashing functions that try to be better in some way than pbkdf2 bcrypt and scrypt this is the  password hashing competition  some of the  candidates  reuse existing hash functions e.g catena but like scrypt their nice password hashing properties e.g memory hardness comes from the rest of the design,2014-11-20 04:17:27,560,73243,both bcrypt and scrypt hashing algorithms are designed to increase the resources required during the computation hashing passwords with these algorithms can be beneficial as it makes the task of an offline attacker more difficult    the time required to hash a password can be increased by increasing the number of iterations in bcrypt the attacker then has to execute this iterations in sequential manner as there is no assumed shortcut   the memory required to hash a password can be increased in scrypt the attacker will therefore need a large amount of memory to mount parallel attacks   as i understand both these algorithm attempt to thwart the parallel attacks   my question is can we make sha-x or any exiting secure hash algorithm iterative and memory intensive and achieve the same functionality as that of bcrypt and scrypt,0.01607142857142857,9,sse,attacks|hash|passwords,"bscrypt,scrypt v/s iterative sha-x",2,attacks|vulnerability,0.5162569880485535
564,random generator is not completely random as it used a definite mathematical algorithm the output is deterministic  secure random generator is much secure as it uses system clock as the seed to generate a random number the attacker may not know the time at which the seed was generated the secure random generator also takes random data from our operating system such as the interval between keystrokes  therefore secure random generator must be used to protect confidential data,2020-02-25 02:40:26,112,226393,what are the real-world implications of using java random generator? what are the practical types of attacks possible? how easy is it to exploit this,0.03571428571428571,4,sse,encryption,risk of using random generator instead securerandom generator,3,attacks|exploit|protection,0.5159410834312439
4924,no  because the key acts as a bias adding just enough entropy to defeat the weakness in the same way you can also still use hmacs based on md5 obviously you shouldn t do so  the prefixing hash collision vulnerabilities found in md5 and sha-1 do not undermine the security of an hmac     to put it simply  the security of the hmac is dependent on a secret key and generating a collision isn t a shortcut the attacker cannot generate the intended valid hmac without knowledge of this secret key  - nor any other message input that would lead to the same output e.g a collision  additionally the vulnerability found in md5 and sha-1 are prefixing attacks and an attacker cannot control the prefix of the hash used in an hmac due to an xor operation as shown in the diagram below,2016-09-05 15:56:45,198,135936,for instance sha-1 is known as weak hash function if attacker finds collision in hash function can he generate valid mac for given message m?i mean it is possible to generate valid mac for any message  just using weak hash function.if it possible what is needed to generate valid mac,0.045454545454545456,9,sse,hash|hmac|sha,finding hash collision,3,attacks|weakness|vulnerability,0.5154306292533875
27149,it depends on the implementation of the bcrypt module you re using bcrypt itself is just a  key derivation function  and does not indicate how comparison should be done in theory a   function that compared hashes with a naive string   comparison could leak information about the hash   however assuming you re referring to the most widely used     module for node.js the   function is implemented using a timing safe     function this function always compares all characters in the hash before breaking which prevents it from revealing where/when the comparison failed,2016-02-25 07:31:17.063 UTC,123,35620979,in node.js web development i saw it as a common practice to use   for hashing and comparison of password is   vulnerable to  timing attack ,0.04065040650406504,5,so,bcrypt|javascript|node.js|security,is bcrypt.compare vulnerable to timing attack,3,leak|attacks|vulnerability,0.5148061513900757
21866,you re assuming no  rainbow table  targeting your setup would be available which is not a given imnsho consider it broken the moment it s leaked even with  bcrypt  you cannot be sure about how much work was already done before your hash became public  first you are not talking about a  collision  a collision is when someone finds two  distinct  messages which hash to the same value here you are not fearing someone finding  another  input which hash to the value you publish indeed you fear someone finding  your  input the correct term is  preimage attack  sometimes we say that the attacker is trying to invert the hash function find an input matching a given output  there are two ways to try to find a preimage for a given hash value exploit a weakness of the hash function or guess the input by trying out candidates  there is no known weakness of sha-2 with regards to preimage resistance come to that there is no such known weakness for md5 or even md4 although those two functions are considered to be cryptographically speaking thoroughly broken therefore barring stupendous advances in scientific research on hash function chances are that your hash value will not be found through a hash function cryptographic weakness  trying candidates may be possible or not depending on what the attacker knows of the input this is quite hard to accurately model suppose for instance that the input is a single word containing seven letters there are 26 7  = 8031810176 such words trying out all of them with sha-256 comparing each time with your hash value takes a few minutes on a recent pc with a naive implementation  on a more general basis exploring the set of possible inputs is called a  dictionary attack  because it is often applied on the problem of recovering a user password users are depressingly unimaginative and often choose passwords from a limited set of well words and it seems logical to call dictionary that set of words we also call it brute force or exhaustive search  assuming that the dictionary is sufficiently small for an attacker to realistically try out all its words then not only your hash value will be eventually inverted if there is enough incentive for the attacker but this also opens the way for  cost sharing  the attacker may try to share his computing efforts across several similar attack situations i.e several hash values to invert with the same hash function -- there again a common password-related attack model a basic cost sharing method is to make a  precomputed table  the attacker computes all the hashes for his dictionary  once  then all subsequent hash values can be attacked by simply looking up the hash value in the table a lookup is very fast the attacker sorts his hashes in increasing order  rainbow tables  are a kind of precomputed table in a smart way which allows for a compact representation they make it possible for the attacker to keep a big precomputed table without needing a truckload of hard disks still rainbow or not all the values in the table the one  before  compression in the case of a rainbow table must be computed at least once by an attacker somewhere i.e that someone is able to do a full dictionary attack this has two costs the cpu cost for computing all the hashes and the storage cost for storing the hash values a rainbow table makes the storage cheaper but does not improve things with regards to cpu   salting  defeats precomputed tables including rainbow tables it makes small dictionaries more tolerable that is if we assume that inverting  one  hash value is doable then the salt makes sure that  at least  the attacker will have to pay the full cpu cost of a dictionary attack each time and he won t be able to share his cost across several attacks or with other attackers salting is needed for passwords because it turned out to be impossible in all generality to make generic users choose and remember passwords from a large enough set of possible passwords  it still is much better if your input is from a dictionary large enough to defeat a single brute forcing effort the important thing is the size of the set of values which your input string may take that set must be estimated with regards to what the attacker knows of the attacked data for instance if the attacker is trying to find a user password then he knows that the input string is short users have little patience and consists only in characters that can be input blindly ! on a keyboard and he also knows that the sequence can be memorized which makes things like .%f*.ds/~\d09j@ quite improbable there is no limit per se on the input size we say that rainbow tables are limited to 15 characters or so because users who accept to type more than 15 characters will also choose passwords from too large a set to allow the single brute force effort needed for table construction note that trying  all  sequences of 15 characters is already way too much even all sequences of 15 lowercase letters imply more than 2 70  hash computations and that s not really feasible with today s technology  thomas s answer is already very detailed but i would add this  criteria      what is the benefit of breaking the hash?   drop a penny on the street how long does it take until someone picks it up? now drop a $20 bill and do the same experiment  if the value of what you are trying to secure is  low  it is possible that nobody will try to break the hash at all  if the value and benefit of breaking the hash is  high  it will only survive as long as it takes to buy the necessary computing power from the amazon cloud they now even sell gpus,2011-01-14 02:48:11.76 UTC,1168,4687518,if i were to leave a sha2 family hash out on my website - how long would it be considered safe? how long would i have before i could be sure that someone would find a collision for it and know what was hashed?  i know that the amount of time would be based on the computational power of the one seeking to break it it would also depend on the string length but i m curious just how secure hashes are  since many of us run web-servers we constantly have to be prepared for the day when someone might make it all the way to the database which stores the user hashes so move the server security out of the way and then what do you have?  this is a slightly theoretical area for many of the people i have talked with so i would love to actually have some more information about average expectations for cracking,0.023972602739726026,28,so,hash|security|sha,how long can a hash left out in the open be considered safe,4,leak|attacks|exploit|weakness,0.5122888684272766
7068,from  rfc 2104 § 6  describing the security requirements for an hmac     the security of the message authentication mechanism presented here depends on cryptographic properties of the hash function h the resistance to collision finding limited to the case where the initial value is secret and random and where the output of the function is not explicitly available to the attacker and the message authentication property of the compression function of h when applied to single blocks in hmac these blocks are partially unknown to an attacker as they contain the result of the inner h computation and in particular cannot be fully chosen by the attacker   so it s not that collisions are not important for an hmac it s that a collision attack against a hash function does not  affect  that hash function when used in an hmac construction in particular an hmac demands much weaker security guarantees from the hash function than many other applications it may have it only requires weak collision resistance from the underlying hash  hmac is defined as  hmac k m = hk ⊕ opad || hk ⊕ ipad || m  where  opad  and  ipad  are constants and  h  is an at least weakly collision-resistant hash the key  k  is not known to an attacker to trigger a collision with an hmac an attacker would need to do one of the following    find a collision that is valid for a number of keys    perform a successful key recovery attack    both of these are extremely unlikely to be possible for even the worst of cryptographic hashes assuming the authentication tag is sufficiently large typically at least 128 bits,2018-06-15 23:39:15,317,187866,sha-1 is broken because collisions can be found in substantially fewer hash operations than naive brute-force would suggest hmac-sha1 is fine however because for hmac  “collisions aren’t important.”    why aren’t collisions important for hmacs,0.03470031545741325,11,sse,hmac|sha|vulnerability,why aren’t collisions important with hmac,3,attacks|weakness|vulnerability,0.510526180267334
14912,"the point of the salt is not to make a single password stronger it is about preventing the attacker from scaling up when attacking several passwords with the salt the attacker cannot reuse his efforts into attacking another password he must rehash his dictionary  rainbow tables are nothing magical they are just a special case of a precomputed table which is akin to a simple dictionary attack with slightly distinct space-time modalities building the rainbow table implies more or less going through the complete dictionary precomputed tables are a gain for the attacker if he can use them to attack several passwords if passwords are salted then precomputed tables rainbow or not will not gain him anything  that being said a single password is often weak and can be brute-forced because the average password will fit in the average user brain and as such cannot be very complex to mitigate that risk one should use repeated or iterated hashing a salt does not help here but it does not harm either see  this answer  for details     can t they just use a rainbow table,  or brute force to figure out the salt   how would that work? but it s a non-issue anyway - assume that the attacker  knows  the salt its purpose is not to be secret that s why you store it right next to the hash     and then append the salt to every word  in a dictionary attack?   sure they can do that but they have to do it for that particular user they cannot amortize the effort over all users in the db or use a precomputed table of hash->password mappings  that and only that is the point of a salt  they can do that  the power is that they would therefore need to generate a new rainbow table for each password or iterate through each dictionary entry for each password  so the total compute time for a single password is still the same as for a common salt  but the total compute time for multiple passwords goes up exponentially..  oh and it s typically considered good practice to have two salts  one stored in the database that s unique per password hash and one stored on the filesystem that s unique for the whole site  that way if the database is compromised there s no significant worry as they only have 1/2 the salts used  sure if the filesystem s compromised they could get it all but if the filesystem s compromised they can install password sniffers and other nasties..  i hope that helps..  let s use a simple example we have two databases  alpha  and  beta    alpha  just hashes the password and stores the result    beta  creates a random value for each user and uses it as part of the input to the hash function   now say your adversary has prior knowledge that some of your users are using the password    to find all users in  alpha  whose password is   you only have to calculate the hash of   once here s an example from sql   since it just involves integer equality it s about as efficient as a query can be even if  alpha  had hundreds of thousands of users it would return very quickly  the fact that  beta  s hashes include a row-specific random value in every password hash you cannot write a similarly efficient query for it the closest you could get would be to re-evaluate the intentionally expensive to compute hash function for every row s     the fact that searching for a  known  password is so expensive should indicate how expensive it is to perform a brute force attack even if that attack is guided by dictionaries of common passwords or algorithms that attempt to mix words and numbers together to create passwords the way humans do   you already know that   is a measure to defend against rainbow table attacks so your question is.. how?  rainbow table has become a flowery term for any attack that computes the hashes for common and likely potential passwords ahead of time and stores them in an efficient lookup table once you have that table built which can take several hours you then iterate through every user and see if their password hash is in the lookup table if it is you ll have guessed that user s password  the users within  alpha  are indeed vulnerable to this kind of attack  alpha  will have equivalent hashes for equivalent passwords so a hash table or rainbow table could be used to reverse the hashes but  beta  cleverly sidesteps this vulnerability by making the result of the hash function unique to the user by virtue of the    i hope this helps some reader someday!  well for one they cannot use a precomputed rainbow table to find a collision - an attacker would have to generate their own rainbow table using the salt also assuming every user has a different salt that rainbow table would only work for a single user - making their job that much more difficult",2010-11-11 22:19:24.017 UTC,937,4159827,i know there are tons of blogs articles and questions on so about salting passwords but one thing i haven t been able to find an answer to is this  if i am generating a password hash like this   and i have a table like this   why is it so difficult for an attacker to figure this password out? can t they just use a rainbow table or brute force to figure out the salt and then append the salt to every word in a dictionary attack,0.021344717182497332,20,so,hash|passwords|php|salt|security,another question about salting passwords,3,attacks|weakness|vulnerability,0.5085763335227966
4237,sha-1 is vulnerable to collisions even if no one have been publicly disclosed yet  some cryptographers have been working on the subject the latest years and they estimated that  the cost of finding a collision is decreasing so much  that some attacks will be very soon within realm of possibility  in october 2015 an important milestone was marked with  the first freestart collision example for sha-1  which is a collision for its internal function this is not a full collision but this is still a major improvement on the way on finding one so they recommended to move from sha-1 because they expect the first collisions to be found very soon  they estimated the new  cost  of a full collision from 75k$ and 120k$ which is a 3-4 years improvement over previous estimations  sha-256 and sha-512 are different algorithms and are not affected by these theoretical attacks   update february 2017 the first public collision on sha-1 has been announced!  both  this pdf  and  this one  share the same sha-1 hash this collision was found using  shattered  a new attack on sha-1   you can also read  google security team s article  on their blog  without even considering the design details of the sha-1 algorithm itself it is vulnerable to collision attacks simply from having a too short output  the output of sha-1 is 160 bits by using a birthday attack it is possible to find a collision with only 2⁸⁰ invocations of sha-1 the bitcoin system has proven that computing 2⁸⁰ hash values is feasible with current technology in fact bitcoin computes that many hashes every two weeks  as such any hashing algorithm with an output that is 160 bits or shorter can be ruled out for usages that require collisions to be intractable,2016-03-16 21:32:51,371,117662,i have been hearing a lot lately about how sha-1 is no longer going to be used because of how weak and insecure it is but if it is not vulnerable to collisions like md5 is then how is it weak and insecure? what is it vulnerable to that sha-256 and sha-512 are not or are less likely or easy to be,0.03773584905660377,14,sse,hash|sha|vulnerability,how is sha-1 insecure if it is not vulnerable to what md5 is,4,attacks|shatter|weakness|vulnerability,0.5062444806098938
1339,for a simple invocation of md5 or sha-1 a realistic rate is between 1 and 10  billions  of passwords per second that s what can be achieved with a good gpu furthermore if the hashing is not salted then the attacker can share the effort between several attack sessions usually using precomputed tables e.g  rainbow tables  an attacker somewhere had to hash gazillions of possible passwords  once  but then the resulting tables can be applied to thousands of hashes with only a small cost  if the hash function is proper as described in  this answer  then it uses a  slowdown parameter  which makes the function as slow as can be wished for slowness impacts both the defender the honest server who uses the hash to authenticate users and the attacker we could imagine for instance that the defender is ready to invest 10ms worth of his own cpu on each hashing instance which means that the defender can handle 100 password hashes per second the attacker can throw more pc at the issue with multiple cores and possibly use specialized hardware gpu fpga... which may give him some boost let s say that the attacker can be 100 times more efficient at the password hashing than the defender leading to brute force performance of 10000 guesses per second  the boost that the attacker can obtain through specialized hardware depends on the function e.g a gpu will be good for attacking pbkd2/sha-256 not as much for attacking bcrypt how much hardware the attacker will be able to throw at the problem also depends on external parameters his budget his motivation... the slowness used by the defender is a trade-off because increased slowness uses server resources and may make it more vulnerable to  denial-of-service attacks,2013-06-28 13:29:13,356,38134,i m trying to gauge password strength assuming that the attacker has a hash of my password can anyone cite some realistic contemporary rates at which someone could perform various hashes? i know most sites use unnecessarily weak hashes like md5 of sha-1 so rates for these would be the most relevant,0.042134831460674156,15,sse,brute-force|hash|passwords,what are realistic rates for brute force hashing,4,attacks|weakness|vulnerability|denial of service,0.5058208107948303
56511,as op wants to print the result in the screen divide the number in 2 parts most-significant-digits and least-significant-digit,2015-04-10 16:04:32.19 UTC,61,29565934,is there a possible way to sum two different   variables when the result is going to be bigger than a   in c,0.01639344262295082,1,so,c|integer-overflow|long-integer|math,sum of two long long int when they give a result bigger thant long long int,1,integer overflow,0.5052286982536316
14384,see         additionally securerandom must produce non-deterministic output therefore any seed material passed to a securerandom object must be unpredictable and all securerandom output sequences must be cryptographically strong as described in  rfc 1750 randomness recommendations for security    if your   is a constant it is predictable     uses the cryptographically strong   only to generate the salt see         random - an instance of securerandom to use   that leads to predictable salts   the salt should be at best unique see  a future-adaptable password scheme      as proposed by morris and thompson [9] however lookup tables can be thwarted with the second input to f which they call a salt if a random salt is chosen whenever users establish new passwords and if the salt space is large enough to ensure a negligible probability of recurrence lookup tables offer an adversary no advantage he may as well compute f at the time of attack if on the other hand the salt space is too small the output bits of f become useful predicates on passwords a fact exploited by the qcrack [12] program described in section 6   if you use a constant seed you will get the same sequence of salts after every restart of your application that leads to  salt collisions   a salt collision and a predictable salt weaken your security see  seven ways to screw up bcrypt       #1 using a non-random salt       [...] if the salt for any two hashes is the same then the attacker can re-use the computation to attack both at the same time for brute force style attacks [...]       #2 using an incorrect random source for salt generation       [...] additionally some of the weak random sources suffer from problems known as seed poisoning where an attacker can effect future generated randomness,2016-10-05 12:17:12.483 UTC,358,39873734,i am just doing a code review of a co-workers task and came across the following lines of code he was implementing a spring security based login system     is it a good idea to initialize this particular   with a constant seed? i don t think so but can t really explain why,0.030726256983240222,11,so,bcrypt|java|secure-random|spring-security,should i initialize the securerandom for my bcryptpasswordencoder with a seed,5,poison|attacks|exploit|weakness|predictable salt,0.5049664974212646
24203,md5 is not suitable for use for with any sort of sensitive information collisions exist with the algorithm and there have been successful attacks against it   at the time of this update 3/2017 sha-2 is now preferred  when ever you embark on securely dealing with information it is recommended that you check the latest guidelines  here is the comparison between md5 and sha1 you can get a clear idea about which one is better        first of all md5 is broken - you can generate a collision so md5 should not be used for any security applications sha1 is not known to be broken and is believed to be secure other than that - yes md5 is faster but has 128-bit output while sha1 has 160-bit output   update  sha1 has been broken a team of researchers at google and cwi have published a collision -  https://shattered.io/static/shattered.pdf,2010-06-01 08:14:28.517 UTC,190,2948156,which is the best overall hashing algorithm in terms of complexity and security? md5 or sha1?    from what i know  md5  is faster than  sha1  but  sha1  is more complex than md5   am i missing anything,0.021052631578947368,4,so,algorithm|hash,algorithm complexity & security md5 or sha1,3,attacks|shatter|sensitive information,0.5048843026161194
18679,"no it isn t    md5 is weak enough to brute force in a matter of hours with a gpu rig and that s without truncating it!  double md5ing is not going to add significant additional production and truncating is frankly unjustifiable    you need to look at more modern schemes such as blowfish and at password salting    if you think the danger of brute forcing with gpus is being overblown see  http://arstechnica.com/security/2012/12/25-gpu-cluster-cracks-every-standard-windows-password-in-6-hours/   your way is very weak only 5 characters from md5 checksum that s only something over 1 million values all attacker needs is to find collisions don t do the substr store full hash with salt and use stronger hashing function like sha256  in general you do not want to try and roll your own encryption since there are many excellent algorithms out there and an algorithm that is home brewed is more likely to have bugs or weaknesses  in your particular case the two major weaknesses are taking a substring and using md5   substring   taking a substring of the md5 hash reduces the possible hashes produced by your algorithm which increases the chance of a collision take the extreme case where we only use the first character of the md5 hash we now have a situation where any password has a 1 in 16 chance of having the same hash as any other password! worse brute forcing this would take very little time  your example uses a length of 5 but this is still very easy to brute force and has a high chance of collisions here s output from a script that creates 1,000,000 hashes using   to generate the values     number of hashes created 1000000   number of unique hashes 481247   collisions 518753    md5   md5 is fast for some applications this can be a good thing - say checking files for exact duplicates however for passwords this means that it s also fast for an attacker to check lots of passwords in addition there are a number of vulnerabilities in md5   alternatives   php 5.5 provides a password api that simplifies generating passwords and looks easy to use admittedly i haven t used it myself this also takes care of choosing a strong algorithm  http://www.php.net/manual/en/book.password.php    this question  has plenty of options for other php versions",2014-06-12 08:29:13.463 UTC,424,24179734,in change_password.php i do this   and in login.php i do the checking like this   please tell me if this code is enough to protect my passwords in this time of breaking encryption algorithms,0.02122641509433962,9,so,php,please someone help me to see if my code is enough to protect passwords,4,attacks|weakness|protection|vulnerability,0.5040823817253113
11791,you speak of slowing down validation as a problem but it is the only defense against a leaked hash and a brute force attack modern solutions hash the value repeatedly ie thousands of times just to raise the cost of the calculation   the  collision resistance  property of md5 has been broken for a long time note that that  preimage resistance  and second preimage resistance have not yet been cracked however as there are better algorithms out there sha-2 it would be wise to move to these rather than relying on a cryptographic hash that has already begun to lose its cryptographic properties note the collision resistance property  does not matter  when storing hashed passwords - what you need to make sure of is that the preimage resistance property is sound - that it is computationally infeasible to find the original password given a certain hash value and salt as i mentioned since one of the cryptographic properties is already broken i would be concerned that the others will follow soon  when you store a password hash you should build in some protection that the original password cannot be retrieved in case an attacker manages to extract these hashes this is important because if an attacker manages to retrieve the password table only they can then use the data to either log into your system directly or to log into other systems where the user has reused the same password  when storing passwords it is important to use a  slow  algorithm such as bcrypt scrypt or pbkdf2 a legitimate user should only have to experience the delay once upon first login an attacker will have to experience the delay for every password that they guess - remember rainbow tables will not be being used here because the passwords are salted the attacker will be hashing each password guess in line with your chosen algorithm and iteration count  it is important to tune the number of iterations for your system just so the right strength is used not to cause legitimate users any real annoyance when logging into your system this is known as number of rounds or the iteration count for example iterating for about one second should be sufficient it may be safe to assume that an attacker can run through hashes at ten times the speed of your system s hardware therefore this limits the attacker to 10 guesses per second rather than two billion with md5  regarding dos attacks  yes the extra processing your application performs upto login could be a target for an attacker to submit either really long passwords to your application or to repeatedly hit it with login requests in order to consume cpu and memory resources on your server  you are right to be concerned   these type of attacks can be mitigated in the following ways   log the username and ip address of each login attempt after say 6 failed attempts introduce a delay in response from your application if that username or ip is repeated again this will also help mitigate password guessing attacks in general.  for example you could artificially delay by 1 second then 2 seconds then 4 and up to a reasonable value e.g 16 seconds  this has the advantage that an attacker can t lock out another account on purpose as the legitimate user only has to wait 16 seconds  an attacker could use a botnet and random usernames to bypass these checks however they would need a huge number of ip addresses than they would without this control and a more casual attacker would also not be aware that the delay in response was artificial    monitor the number of login attempts on your system once this is above a set threshold rate e.g 10 per second introduce a captcha to solve to continue the login process the threshold rate you choose very much depends on the user base and capacity of your system  implement two factor authentication only proceed to validate password by hashing once the one time password has been validated   the problem with md5 is exactly that it is so fast you can calculate about  9 giga md5/s  with common hardware to brute-force a whole english dictionary with about 200000 words you need only a fraction of a milli-second  this is why appropriate hash algorithms like bcrypt offer a cost factor the cost factor defines how much time is needed to calculate the hash and can be inreased in future 50 milliseconds for a login is hardly an obstacle but for brute-forcing it is deadly,2015-05-28 02:09:34.46 UTC,932,30496061,i have a friend which is a white hat hacker he says that md5 is not really that bad and actually is really secure just if we use it properly  i believe that he is right as i know there is 3 ways to break hashes   using rainbow tables which can be secured against by a long/random salt  collision which can be prevented by multiple salts or hashes - as in example bellow  generation time which is not much important if we use a long enough salt value per each user - afaik   i and my friend believe that blowfish is not really needy also it can be harmful because it can slow down password validation process and it can be used with ddos attacks to break down server even with fewer attak-resources  so i want to make sure that is following algorithm really secure or not? and is there a real reason to go with blowfish hash algorithm or not,0.02145922746781116,20,so,hash|md5|password-storage|passwords|security,why not use md5 for password hashing,6,ddos|leak|bypass|attacks|protection|denial of service,0.5024339556694031
24686,generally yes it does prevent  pre-computed  rainbow tables since you have a rather unique algorithm that probably nobody has bothered creating a rainbow table for  but the same password still hashes to the same hash so an attacker trying to brute-force all your password hashes has an easier time because he only needs to crack the same password once for all users  therefore it is still very advisable to use user-specific salts and if you re using user-specific salts with an already strong hashing algorithm it doesn t really matter whether you also do the bit shifting dance or not  if the attacker has control over  database and code  adding scrambled characters will help nothing at all only a negligible operation more if he has only the  database without code  sql-injection then he will recognize the bcrypt hash and can now brute force with bcrypt but because of the scrambling there aren t any weak passwords it s like the scrambled text would be the password to crack so a dictionary is of no use  this is security by obscurity but will be effective as long as the code is not known you can get the same effect easier by adding a fix hard coded salt key before using bcrypt with the unique salt  p.s the unique salt used in bcrypt will help against rainbow tables not the scrambling of your password a big rainbow table can also contain random combinations like your scrambled password,2012-07-12 06:58:25.363 UTC,344,11446491,i have an encryption method that has the following behavior:each character of the password is put through a method that gets the ascii value of that character and shifts the bytes one way and then the other way and returns the following      an example of a password before it is hashed     after this the resultant string formed from going through each character in the original password is hashed using bcrypt  does surrounding the passwords with these junk characters improve the strength of the passwords or protect them from being cracked via rainbow tables/dictionary attack,0.020348837209302327,7,so,encryption|php,php encryption method,5,attacks|protection|hard coded|sql injection|weak password,0.5022241473197937
3334,"by use memori hard algorithm the advantag of asic is limit ram cost around the same in an asic as it doe in consum hardwar https://www.youtube.com/watch?v=8qxfswszbyi a memori hard function that requir 1 gb ram will requir 1 gb ram no matter the design of the asic e.g if you have a gpu with 2688 core and 6 gb ram like tesla k20x then you can at most run 6 job in parallel due to the memori requir - the remain 2682 core will have to sit idl so a solut would be to pass the password through a memori hard function hash the result twice and store the result deriv a random look valu from the password in a way that is expens to do on asic memori hard asic unfriendli algorithm hash thi valu use a normal hash function to make the size small transmit thi to the server hash thi valu use a normal hash function store thi in the password file e.g store thi result as long as sha512 and argon2id are not broken then thi should make the advantag of use asic veri small guess a random look input valu from a sha512 valu is infeas - even with dedic hardwar the best pre-imag attack on sha512 is 2^511.5 https://en.wikipedia.org/wiki/sha-2 if argon2id is proven to be asic friendli replac that with anoth hash function that is asic unfriendli unfortun if the comput is run on the server then the server can be dos ed by start mani login simultan becaus if the server has to run argon2id1 gb ram it will be use 1 gb ram for each simultan login thi can be avoid by move that part of the comput to the client the commun is of cours encrypt so an attack cannot get the answer be sent from the client thi would otherwis make a replay attack possibl even for modern smartphon that have 8 gb ram use 1 gb ram for 1 g instruct is feasibl and for laptop it has not been a problem for a long time in practic the client program would need nativ support for thi to be fast enough e.g javascript would not be fast enough but for browser thi ought to be possibl to do use nativ plug-ins/extensions/add-ons/modul by do a singl hash on the server a leak password file cannot be use directli as comput the revers of a hash should be veri hard it basic correspond to the user have a 512-bit random password an ad advantag is that the password is never access on the server in cleartext if the site is complet taken over by an attack the attack will thu not know the password which the user might be use on other site for client that do not support thi e.g if they have less than 1 gb ram free we can give the user the option to do a captcha and if he complet that then the server will do the comput for the client here the password will be on the server in clear text when start the comput so all in all an attack will have to buy 1 gb ram for each parallel guess he want to run and spend 1 g instruct for each guess while the server onli need a few mb for a few millisecond to verifi a user that run the comput himself argument against an attack doe not need to guess the password he can just take the hash valu from the password file and comput a valu that would result in thi hash valu and use thi valu to login in other word find x where sha512x=valu thi is call find the pre-imag if x is in a small search space say all 10 word combin of 1000 english word = 1000^10 = 1000000000000000000000000000000 then thi may be doabl if x is a random number in a 512-bit search space then thi requir 2^511.5 oper accord to https://en.wikipedia.org/wiki/sha-2 thi is not doabl no matter the hardwar 2^512 = 13407807929942597099574024998205846127479365820592393377723561443721\76403007354697680187429816690342769003185818648605085375388281194656\9946433649006084096 recal that the server store salt,sha512sha512argon2id1 gb ram 1 g instruct salt password so an attack would have to find sha512argon2id1 gb ram 1 g instruct salt password or anoth valu that give the same sha512 in thi case the search space is 512-bit thi is becaus the output from argon2id1 gb ram 1 g instruct salt password is indistinguish from random data and thu the output from the sha512 will also be indistinguish from random data you cannot rule out valu like say aaaa...a becaus given random input sha512 may result in that valu an attack can just design an asic so effici that he can run mani thousand of guess in parallel it is import to understand the differ between memori hard function and a simpl hash function like sha512 by definit of a memori hard function the function requir the defin amount of memori to run 1 gb in the exampl here so the asic would have to includ 1 gb of ram for each comput core thi make a special asic with ram prohibit expens the attack would have to have at least 8 g transistor on the chip for a singl thread more realist he could buy the ram in stick like the rest of us he would thu need to buy 1 gb of ram for each thread he want to run in parallel if the attack has to buy 1 gb ram for each thread and each thread has to run for 1 g instruct for each guess it push the bar way higher than for the server which has to do a singl sha512 for each login will thi run on small clients? it will requir 1 gb of free memori on the client so veri small machin think iot will not be abl to do thi they may use digit certif instead but that is outsid the scope of thi question it may howev be interest that it can run on small server if the run-the-computation-on-the-serv solut describ below is not includ so if the iot-devic act like a server thi may be a secur way to login will it run on slow client e.g in javascript? the 1 g instruct will most like make thi imposs so for browser you would need a plugin/extension/modul that is written for the nativ cpu if it is import to support slow client the server could also offer a secondari login where the server doe the memori hard comput after the user solv a captcha what is the differ between a memori hard function and say sha512? sha512 can be run in less than 1 mb a memori hard function requir a certain amount of ram to run a simpl exampl probabl not cryptograph safe would be can we make password guess of an attack so hard that even if he has the hash password file he will not get a major advantag over test the password against the server - even if he has special asics? no we cannot what you are impli is that somehow is possibl to a standard common server to take the same time to valid a password as anoth server built onli for password break no matter what you do how you hash the algorithm you use if you encrypt or not a common server will be slower than a purpose-built one the onli real defens is to forc user to choos good long password there are servic out there that allow you to check offlin if the chosen password is on a leak list so you can ask the user to choos anoth password and allow onli long password 16 char or more as thi two measur will defeat both dictionari attack no leak password and bruteforc a 16 byte password properli salt and hash will surviv for millenia there are a few thing that can be done and are done one approach is to leverag tpm to creat a secret that the server cannot theoret leak thi secret is use upon the password dure the hash process i believ the term for thi is a pepper as oppos to a salt which is consid public inform the offlin attack could beat on thi password file as long as they like but unless they can guess the server s secret store in a tpm modul they don t have anyth to break obvious thi can be done with any sort of secret but i point out tpm becaus that tend to reli on hardwar solut to provid secur for these number and that can make it harder to leak the secret than lose a password file thi is highli relat to the issu the fbi has publicli had when tri to unlock iphon the inform they need is lock away in a chip that is intent difficult to get into the other approach is to make asic ineffici the power of asic is typic that they can do a massiv parallel attack on the password file you can design the hash process to consum a larg amount of resourc which are expens to parallel there are hash algorithm out there design to do thi one common approach is to have a veri memori intens hash routin forc the asic design to expend larg amount of chip real estat on bank of memori fundament though if the attack can get two comput which are equival to your server they can attack it twice as fast offlin thi is a simpl inform theoret realiti howev these approach make it difficult to get an equival to your server one approach doe so by creat a hard to duplic secret the other approach doe so by use algorithm that are not trivial paralleliz in hardwar in respons to your comment about your self answer if you involv the client and have them do the calcul for you some interest solut becom option consid look at zero knowledg proof they are design to reveal that you know a number or password without reveal any inform other than that you know the number if the server queri the client use one of these zero knowledg protocol to ascertain whether the client know the password or not the server doesn t need to know anyth about the password at all so there is no password file to leak at most there would just be a databas of data structur to use in the zero knowledg protocol which are mathemat proven to contain no inform about the password or at least statist noth an attack who acquir thi file would be abl to verifi that a client that connect to them! is a valid client but could not crack the password any better than you could -- which is not at all my boldfac can we make password crack so difficult that even if he has the hash password he will not get a major advantag over test the password against the server - even if he has special asics? the problem with the way you ve formul the question here is you haven t given us any concret criterion as to what advantag count as major but we can definit say that even with state-of-the-art memory-hard password hash function like argon2 the attack definit still has some advantag for exampl in the v.9 draft for the argon2 rfc section 8.2 discuss argon2 s secur against time-spac tradeoff attack time-spac tradeoff allow comput a memory-hard function store fewer memori block at the cost of more call to the intern compress function the advantag of tradeoff attack is measur in the reduct factor to the time-area product where memori and extra compress function core contribut to the area and time is increas to accomod the recomput of miss block a high reduct factor may potenti speed up preimag search the idea here is there are strategi to reduc the memori usag need to comput the function but at the cost of slow down the comput thi is call a time-memori tradeoff tmto we use a time-area product metric as a proxi for the cost of comput the password hash a time-memori tradeoff is advantag to the attack if it allow them to comput the function at a lower cost measur by time-area product than the canon algorithm that the defend use and the answer for argon2 and similar algorithm is that there are definit time-memori tradeoff advantag to the attack but their size is bound the best tradeoff attack on t-pass argon2d is the rank tradeoff attack which reduc the time-area product by the factor of 1.33 the best attack on argon2id can be obtain by complement the best attack on the 1-pass argon2i with the best attack on a multi-pass argon2d thu the best tradeoff attack on 1-pass argon2id is the combin low-storag attack for the first half of the memori and the rank attack for the second half which bring togeth the factor of about 2.1 the best tradeoff attack on t-pass argon2id is the rank tradeoff attack which reduc the time-area product by the factor of 1.33 so actual we expect that an asci attack could comput argon2d or multi-pass argon2id at someth like half the cost to the defend and one-pass argon2d for some 75% of the cost whether that count as major is up to you but function such as scrypt and argon2 are seen as major improv in thi field and can we avoid thi open a way of dos ing the server with mani parallel login requests? there s a few server relief techniqu that have been propos for shift the comput of the costli function over to the client but none has realli caught on see for exampl https://tools.ietf.org/html/rfc5802 https://openwall.info/wiki/people/solar/algorithms/challenge-response-authent the downsid to these compar to server-sid hash is that the server disclos the usersid salt to any attack who probe for that user id which mean that an attack can start precomput hash for user of interest befor they manag to steal the password hash store on the server beyond that there s password-authent key exchang protocol that use asymmetr cryptographi and are rather more complic",2020-01-22 14:12:39,2590,224629,previous authent system store password in cleartext thi made it trivial for an attack to log in to an account if he had access to a leak password file later password were hash onc and the hash valu store if the attack had a leak password file he could try hash guess and if a hash valu match use that guess to login then password were salt and hash thousand of time on the server and the salt and the result hash valu was store if the attack had a leak password file he could use special asic to hash guess and if a guess match use that password to login can we do better than that? can we make password crack so difficult that even if he has the hash password he will not get a major advantag factor of 10 over test the password against the server - even if he has special asics? and can we avoid thi open an way of dos ing the server with mani parallel login requests? we can assum the attack has access to the hash password but that he cannot intercept commun between the server and a client,0.02084942084942085,54,sse,denial-of-service|hash|password-cracking,password hash that is resist to asic-assist crack without risk dos of server,3,leak|attacks|denial of service,0.5005561709403992
66982,you could in addition to using a slow hash function limit the number of requests per second a particular user could send for instance if you re getting 30 failed login requests from a particular ip address then you could delay the responses to that address or drop them all together    slowing hash functions is designed to make it impractical to carry out a brute force attack if an attacker gets access to the password database there should still be precautions built into the server as well,2016-02-04 05:09:54,122,112743,it is generally recommended that one use a slow hash function to hash passwords however does this create a dos risk,0.040983606557377046,5,sse,denial-of-service|hash|passwords,how can one securely hash passwords server side while avoiding dos,2,attacks|denial of service,0.5000041127204895
9545,your idea to pre-generate random strings to get around the integer overflow would work it s not really a dictionary attack at that point more a rainbow table approach it s going to take up a lot of space too  i might suggest trying a dictionary attack of 24 character passwords before generating your brute force list perhaps with the lists from  https://crackstation.net/  or somewhere else filtered for length 24 of course if you want to get creative/more thorough you could add rules to the passwords from these lists like l33tspeak substitutions,2017-12-31 14:16:18.167 UTC,198,48043188,i do know the password length is 24   i also do know the search space lower case upper case and digits  the combinations should be 26+26+10^24 = 62^24 right?  i ve tried with hashcat to set the charset and the mask    but it won t start because of an integer overflow error due to a huge keyspace i guess the mask is too large!  someone can suggest another solution? i was thinking maybe i could write a script that generate a file of random 24 length strings and then using this to perform a dictionary attack,0.030303030303030304,6,so,brute-force|cracking|hash|hashcat|passwords,cracking hash knowing password length,3,attacks|overflow error|integer overflow,0.49878963828086853
1424,"first hmac-sha1 is not a good password hashing algorithm use something like  bcrypt   scrypt  or  pbkdf2   in this case it is absolutely trivial to begin cracking passwords on your system  completely random  passwords of at least ten characters may still be secure but any password that is shorter or makes heavy use of patterns or words will fall in a matter of minutes modern password crackers have extremely sophisticated engines that can take a base word like password and hash every plausible permutation of it p4$$word! pass;word1234 etc. in a fraction of a second some recent systems have even surpassed speeds of  348 billion hashes per second   this is why your current algorithm is not good enough and why it is an absolutely critical vulnerability that attackers can reveal your salts and hashes password-specific hash algorithms typically known as slow hashes help defeat these kind of gpu-accelerated password cracking machines by requiring large amounts of work to compute a single password pbkdf2 for instance is a scheme that uses a repeated hmac by doing 100,000 or more iterations you slow down these password-cracking machines by the same factor it may now take you half a second to verify a user s password in legitimate cases but attackers now require days just to enumerate through even the most common of passwords  algorithms like scrypt go even farther requiring a configurable amount of memory in order to generate the hash if your passwords require 128mb of memory to compute this will barely be noticeable by a standard webapp but for a password cracker it would require extremely large quantities of memory to compute hashes in parallel which is how most of them achieve such great speeds  in your current situation you  must  close down the vulnerability that allows people to reveal salts and hashes existing passwords should be considered compromised unless they can be unambiguously proven otherwise you should migrate your passwords to one of the recommended hashing algorithms as soon as is practical you can simply compose the existing hashes with the new algorithm so given   calculate   and use   as the authenticator  what is bad in revealing the password hash-and-salt is that it allows  offline dictionary attacks  since the password hash and the salt are sufficient to  verify  a potential password then by definition they allow an attacker to  try  passwords on his own machines limited only by the number of pc or other hardware he can muster this contrasts with  online  dictionary attacks where each try must go through your server and your server will not accept to try billions of passwords per second  password hashing is a  second line of defence  you don t want attackers to obtain enough information to run offline dictionary attacks but when they do you at least prefer it if they have to pay the full price of the attack i.e you show them only  password hashes  not the passwords themselves plus you want a  slow  hash function with millions of iterations and the salt to prevent parallel attacks and other similar optimizations like precomputed tables  all of this is about protection of  passwords  as in secret data that human users remember human brains being what they are passwords tend to be vulnerable to dictionary attacks however if you can convince your human users to remember sequences of 20 random letters  not  sequences that they choose themselves but randomly generated letters then you can show the hash values because sufficiently random passwords are out of reach of dictionary attacks anyway -- arguably these are no longer  passwords  but  keys  note that this is not a matter of  length  but of  randomness  a password is not strong because it is long but because it was produced with enough randomness in it you just need length to make room for randomness   if you need a reference to smite unbelievers  then invoke  nist special publication sp 800-118  still in draft form but published nonetheless which notably includes this quote from the executive summary     password  cracking attacks can be mitigated by using strong  passwords choosing strong cryptographic algorithms  and implementations for password hashing and  protecting the confidentiality of password hashes    emphasis is mine.   password hashing is a difficult art you say that you use hmac/sha-1 but hmac is not a hash function it is a  mac  algorithm mac algorithms use a  key  what you  probably  mean is that you use a password hashing function which has been designed over one or several invocations of hmac/sha-1 as a building block  pbkdf2  is such a password hashing function it includes an extra parameter which is the  number of iterations  and that s an important one  see  this answer  for a detailed discussion on password hashing its theory and its practice  if all passwords are long and truly random then it isn t bad  as long as all of the hashes that can leak are not vulnerable to being attacked based on current hashing performance then there isn t any problem  that said you can t guarantee that users won t do stupid things and you pretty much can guarantee some will.    in general offline attacks against hashes are very fast so the required complexity gets very high very quickly  if the only option is online attacks then rate limiting can be applied which helps prevent attacks against the logon and greatly increases security however it is still good for hashes to be resistant if leaked  i don t know of any industry documents but the simple answer is that people using creatively stupid things as their password is the reason why it is bad p@ssw0rdpa$sw@rdpas$word isn t secure even if it is long",2013-08-12 17:10:59,1039,40482,what are some published industry standards about why revealing the password hash and salt are poor or bad practices from a security perspective?  i am dealing with a vulnerability where anyone can anonymously obtain the password hash and salt remotely  the fix is to use a secure and long password 12+ characters  the hashing algorithm is hmac-sha1  i need solid references for why disclosure of the password hash and salt is a poor practice even with a long password,0.025024061597690085,26,sse,hash|salt,need references about why public access to a password hash and salt is poor practice,5,leak|attacks|protection|bad practices|vulnerability,0.49707168340682983
11098,"no it s not worthless    to successfully attack an account an attacker needs to know the salt for that account and every account s salt should be different the hashing algorightm used  and  the final stored password hash  given  all  of that information you can write a program that keeps trying to hash different potential passwords until it finds one that matches    if it s a  bad  salt too simple or short this can be made much faster because the program can use rainbow lookup tables to match the final stored password hash to the string that was hashed and then just subtract the salt  but they still need all the information  if it s a  shared  salt this is bad because an attacker and use the salt to generate a rainbow table in advance that s good for any account on your system  if an attacker knows the salt the hashed password and the hash algorithm then they can mount a brute-force dictionary attack or rainbow attack  assuming brute force attack of md5,sha1,sha256 algorithms with gpu has a throughput greater than 1 billion of tries per second and sha512 around 300m/s if you use one of these algorithms it will slow down hacker who used rainbow table less likely but it will not slow down hacker who used brute force attack more likely it will definitively not protect you it just add a bit of security against outdated rainbow table for these algo a bit is better than nothing   but if you use a strongest algorithm eg bcrypt salt definitively worth it even if stored with hash because brut force is not feasible in term of time so rainbow make sense  have a look at this   article  and to summarize     if you are a user      make sure all your passwords are 12 characters or more ideally a lot more i recommend adopting pass phrases which are not only a lot easier to remember than passwords if not type but also ridiculously secure against brute forcing purely due to their length      if you are a developer      use bcrypt or pbkdf2 exclusively to hash anything you need to be secure these new hashes were specifically designed to be difficult to implement on gpus do not use any other form of hash almost every other popular hashing scheme is vulnerable to brute forcing by arrays of commodity gpus which only get faster and more parallel and easier to program for every year       posted by jeff atwood   this should give you an idea of how it works  lets say you want to encrypt a word secret after it is encrypted lets say it now looks like this 00110010   if a hacker knows the encryption algorithm they can create a table of words and their corresponding encrypted values so they take the encrypted password 00110010 and find it in the table now they know that the password used to generate 00110010 was the word secret if you salt the word first then a generic lookup table will be useless to the hacker a generic lookup table being a table of unsalted dictionary words and their encrypted values  if you salt the word first saltsecret now the encrypted value will look different and the hacker wont find it in the lookup table  however they can still start creating their own lookup table from scratch using your salt and eventually they will be able to reverse lookup passwords   so to answer the question if the passwords are sufficiently complex it will take ages for the hacker to figure them out you could change your salt every year and they would have to start creating a table all over again  knowing the salt makes it possible to do a brute-force attack but that doesn t make it useless salt prevents the attacker from using an already generated rainbow table which you could find on the web  the best way to prevent brute-forcing is simply to use long complex passwords  salting was introduced or at least made popular in unix /etc/passwd file which was world-readable it is usually assumed that the salt as well as the encrypted password is known to the cracker the purpose of the salt is the slow-down of the cracking process since the same password won t map to the same encrypted string it is  not  a secret in itself  no they re not useless  so long as you use a unique salt for each row then the salt will  prevent  slow down an attack the attacker will need to mount a brute force attack rather than using  rainbow tables  against the password hashes  as mentioned in the comments you should ensure that the salt is a sensible size",2009-07-08 14:26:12.723 UTC,869,1098452,let s say i have a table of users set up like this   when a user is created a randomly-generated salt is produced and stored in the database alongside the results of something like    i m wondering that if a malicious user gets their hands on this data would they be able to use it to crack users s passwords?  if so what s a way that it could be prevented,0.018411967779056387,16,so,hash|language-agnostic|security,are salts useless for security if the attacker knows them,3,attacks|protection|vulnerability,0.49676841497421265
32886,interesting problem but the int in java is limited to 2^31-1 you could crox this limit use long or biginteger,2014-01-11 19:00:11.21 UTC,126,21066549,for those unfamiliar with the problem  here  it is  i am getting a stackoverflowerror with the following code    }  high level flow  -for numbers 2 - 100000 generate a collatz sequence for that number.-chain list is a list to store the length of the sequence generated for each number i.e sequence size for number 13 is 10 see example.-if the current sequence size is bigger than the max in the chain list clear chain list and add the new max also store the value of i in maxreq to remember the starting number that produces the longest chain,0.023809523809523808,3,so,java|stack-overflow,project euler 14 - java stackoverflowerror,2,overflowerror|stack overflow,0.49503108859062195
1152,,2020-05-03 19:58:14,72,61580821,today i heard of the term catastroph cancel and would like to know what is the right way to comput the varianc of veri small numer valu in general? sinc i use python veri often i am mainli interest in whether numpi s implement take catastroph cancel into account? what can i do if that is not the case,0.013888888888888888,1,so,precision|python|underflow,how to comput the varianc of veri small numer valu,1,underflow,0.4948453903198242
11484,sha-1 is weak because of collision attacks   https://en.wikipedia.org/wiki/sha-1#attacks      in an interview yin states that roughly we exploit the following two weaknesses one is that the file preprocessing step is not complicated enough another is that certain math operations in the first 20 rounds have unexpected security problems     https://www.schneier.com/blog/archives/2005/02/sha1_broken.html      collisions in the the full sha-1 in 2^69 hash operations much less  than the brute-force attack of 2^80 operations based on the hash  length      collisions in sha-0 in 2^39 operations      collisions in 58-round sha-1 in 2^33 operations   there is a collision attack on sha-1 s compression function that requires only 2^57 sha-1 evaluations,2016-06-26 13:15:53.497 UTC,246,38038841,microsoft google and mozilla have decided not to accept it in 2017 because of the flaws found by researchers and i saw sha-1 is depreciated nearly everywhere simply because people think that it s not safe anymore but unlike md5 currently there is no known collision found on sha-1 and even the fastest algorithm for the collision of sha-1 still requires nearly 2^60 evaluations which is still a very large number   my question is why sha-1 a hash algorithm which hasn t been found any collisions and which still requires a large amount of time to find a collision is depreciated? i understand sha-2 is more secure but it is also slower than sha-1 why most people recommend to trade performance for the so-called security where the security of sha-1 is not even an issue for most situations,0.032520325203252036,8,so,cryptography|hash,why is sha-1 considered insecure,4,flaws|attacks|exploit|weakness,0.4922197461128235
18603,"the point of salting is to prevent the amortization of the attacker s effort  with no salt a single table of precomputed hash-password entries e.g md5 of all alphanumeric 5 character strings easy to find online can be used on every user in every database in the world  with a site-specific salt the attacker has to compute the table himself and can then use it on all users of the site  with a per-user salt the attacker has to expend this effort for every user separately  of course this doesn t do much to protect really weak passwords straight out of a dictionary but it protects reasonably strong passwords against this amortization  simply put salting does not prevent a hash from attack bruteforce or dictionary it only makes it harder the attacker will either need to find the salting algorithm which if implemented properly will make use of more iterations or bruteforce the algo which unless very simple is nearly impossible salting also almost completely discards the option of rainbow table lookups..  in simplest terms without salting each candidate password need only be hashed once to check it against every user anywhere in the known universe collection of compromised databases whose password is hashed via the same algorithm  with salting if the number of possible salt values substantially exceeds the number of users in the known universe each candidate password must be hashed separately for each user against whom it will be tested  the idea behind dictionary attack is that you take a hash and find the password from which this hash was calculated  without  hash calculation now do the same with salted password - you can t   not using a salt makes password search as easy as lookup in the database adding a salt make attacker perform hash calculation of all possible passwords even for dictionary attach this significantly increases time of attack   salt makes  rainbow table  attacks much more difficult since it makes a single password hash much harder to crack imagine you have a horrid password of just the number 1 a rainbow table attack would crack this immediately   now imagine each password in the db is salted with a long random value of many random characters now your lousy password of 1 is stored in the db as a hash of 1 plus a bunch of random characters the salt so in this example the rainbow table needs to have the hash for something like 1   so assuming your salt is something secure and random say  %isldghasklu %#%# the hacker s rainbow table would need to have an entry for 1*%isldghasklu*%#%# now using a rainbow table on even this simple password is no longer practical   also - one more imporatant point - using a user-specific salt prevents the detection of two users with the same password - their hashes would match that s why many times the hash is hashsalt + username + password   if you try and keep the hash secret the attacker also can not verify the hashes   edit- just noticed the main point was made in a comment above  a dictionary is a structure where values are indexed by keys in the case of a pre-computed dictionary attack each key is a hash and the corresponding value is a password that results in the hash with a pre-computed dictionary in hand an attacker can instantly lookup a password that will produce the necessary hash to log in  with salt the space required to store the dictionary grows rapidly&hellip so rapidly that trying to pre-compute a password dictionary soon becomes pointless  the best salts are randomly chosen from a cryptographic random number generator eight bytes is a practical size and more than 16 bytes serves no purpose   salt does much more than just make an attacker s job more irritating it eliminates an entire class of attack&mdash;the use of precomputed dictionaries  another element is necessary to completely secure passwords and that is key-strengthening  one round of sha-1 is not good enough  a safe password hashing algorithm should be  very  slow computationally   many people use pbkdf2 a key derivation function that feeds back results to the hash function  thousands  of times the bcrypt algorithm is similar using an iterative key derivation that is slow  when the hashing operation is very slow a precomputed table becomes more and more desirable to an attacker but proper salt defeats that approach   comments  below are the comments i made on the question   without salt an attacker wouldn t use the method demonstrated in update 2 he d simply do a lookup in a pre-computed table and get the password in o1 or olog n time n being the number of candidate passwords salt is what prevents that and forces him to use the on approach shown in update 2   once reduced to an on attack we have to consider how long each attempt takes key-strengthening can cause each attempt in the loop to take a full second meaning that the time needed to test 10k passwords on 10k users will stretch from 3 days to 3  years &hellip and with only 10k passwords you re likely to crack zero passwords in that time   you have to consider that an attacker is going to use the fastest tools he can not php so thousands of iterations rather than 100 would be a good parameter for key-strengthening it should take a large fraction of a second to compute the hash for a single password   key-strengthening is part of the standard key derivation algorithms pbkdf1 and pbkdf2 from pkcs #5 which make great password obfuscation algorithms the derived key is the hash  a lot of users on stackoverflow refer to  this article  because it was a response to jeff atwood s post about the dangers of rainbow tables it s not my favorite article but it does discuss these concepts in more detail   of course you assume the attacker has everything salt hash user name assume the attacker is a corrupt hosting company employee who dumped the user table on your myprettypony.com fansite he s trying recover these passwords because he s going to turn around and see if your pony fans used the same password on their citibank.com accounts   with a well-designed password scheme it will be  impossible  for this guy to recover any passwords  yes you need just 3 days for sha1salt | password that s why good password storage algorithms use 1000-iteration hashing you will need 8 years  salts are implemented to prevent rainbow table attacks a rainbow table is a list of pre-calculated hashes which makes translating a hash into it s phrase much more simple you need to understand that salting isn t effective as a modern prevention to cracking a password unless we have a modern hashing algo   so lets say we re working with sha1 taking advantage of recent exploits discovered with this algo and lets say we have a computer running at 1,000,000 hashes/second  it would take 5.3 million million million years to find a collision  so yeah php can work 300 a second big woop doesn t really matter the reason we salt is because if someone did bother to generate all common dictionary phrases  2^160 people welcome to 2007 era exploits  so here s an actual database with 2 users i use for testing and admin purposes   in fact the salting scheme is your sha1registration time + user name go ahead tell me my password these are real passwords in production you can even sit there and hash out a word list in php go wild  i m not crazy i just know that this is secure for fun sake test s password is      you would need to generate an entire rainbow table perpended with   for  just  this user that means i can actually allow my passwords to not all be compromised by a single rainbow table the hacker needs to generate an entire rainbow table for 27662aee8eee1cb5ab4917b09bdba31d091ab732 for test and again f3f7735311217529f2e020468004a2aa5b3dee7f for briang think back to the 5.3 million million million years for all hashes think of the size of storing just the 2^80 hashes that s well over 20  yottabytes  it s not going to happen   don t confuse salting as a means of making a hash something you can t ever decode it s a means of preventing a rainbow table from translating  all  your user passwords it s imposable at this level of technology  to be more precise a  dictionary attack  i.e an attack where all words in an exhaustive list are tried gets not impossible but it gets  impractical   each bit of salt doubles the amount of storage and computation required    this is different from pre-computed dictionary attacks like attacks involving rainbow tables where it does not matter whether the salt is secret or not  example with a 64-bit salt i.e 8 bytes you need to check 2 64  additional password combinations in your dictionary attack with a dictionary containing 200,000 words you will have to make     200,000 * 2 64  = 3.69 * 10 24    tests in the worst case - instead of 200,000 tests without salt  an additional benefit of using salt is that an attacker cannot pre-compute the password hashes from his dictionary it would simply take too much time and/or space   update   your update assumes that an attacker already knows the salt or has stolen it this is of course a different situation still it is not possible for the attacker to use a pre-computed rainbow table what matters here a lot is the speed of the hashing function to make an attack impractical the hashing function needs to be slow md5 or sha are not good candidates here because they are designed to be fast better candidates for hashing algorithms are blowfish or some variations of it   update 2   a good read on the matter of securing your password hashes in general going much beyond the original question but still interesting       enough with the rainbow tables what you need to know about secure password schemes     corollary of the article use salted hashes created with  bcrypt  based on blowfish or  eksblowfish  that allows you to use a configurable setup time to make hashing slow  it doesn t stop dictionary attacks  what it does is stop someone who manages to get a copy of your password file from using a  rainbow table  to figure out what the passwords are from the hashes  eventually it can be brute-forced though  the answer to that part is to force your users to not use dictionary words as passwords minimum requirements of at least one number or special character for example   update   i should have mentioned this earlier but some most? password systems use a different salt for each password likely stored with the password itself  this makes a single rainbow table useless  this is how the unix  crypt  library works and modern unix-like oses have extended this library with new hash algorithms  i know for a fact that support for sha-256 and sha-512 were added in newer versions of gnu crypt",2010-08-25 13:50:01.423 UTC,2478,3566504,"update please note i am not asking what a salt is what a rainbow table is what a dictionary attack is or what the purpose of a salt is i am querying if you know the users salt and hash isn t it quite easy to calculate their password?   i understand the process and implement it myself in some of my projects   in the database you store   every implementation of salting i have seen adds the salt either at the end of the password or beginning   therfore a dictionary attack from a hacker who is worth his salt ha ha would simply run each keyword against the stored salts in the common combinations listed above  surely the implementation described above simply adds another step for the hacker without actually solving the underlying issue?  what alternatives are there to step around this issue or am i misunderstanding the problem?  the only thing i can think to do is have a secret blending algorithm that laces the salt and password together in a random pattern or adds other user fields to the hashing process meaning the hacker would have to have access to the database and code to lace them for a dictionary attack to prove fruitful update as pointed out in comments it s best to assume the hacker has access to all your information so this probably isn t best  let me give an example of how i propose a hacker would hack a user database with a list of passwords and hashes  data from our hacked database   common password dictionary   for each user record loop the common passwords and hash them   i hope this illustrates my point a lot better  given 10,000 common passwords and 10,000 user records we would need to calculate 100,000,000 hashes to discover as many user passwords as possible  it might take a few hours but it s not really an issue   update on cracking theory   we will assume we are a corrupt webhost that has access to a database of sha1 hashes and salts along with your algorithm to blend them  the database has 10,000 user records   this site  claims to be able to calculate 2,300,000,000 sha1 hashes per second using the gpu  in real world situation probably will be slower but for now we will use that quoted figure     95^4/2300000000/2*10000 = 177  seconds   given a full range of 95 printable ascii characters with a maximum length of 4 characters divided by the rate of calculation variable divided by 2 assuming the average time to discover password will on average require 50% of permutations for 10,000 users it would take 177 seconds to work out all users passwords where the length is &lt;= 4  let s adjust it a bit for realism     36^7/1000000000/2*10000 = 2 days   assuming non case sensitivity with a password length &lt;= 7 only alphanumeric chars it would take 4 days to solve for 10,000 user records and i ve halved the speed of the algorithm to reflect overhead and non ideal circumstance  it is important to recognise that this is a linear brute force attack all calculations are independant of one another therfore it s a perfect task for multiple systems to solve  ie easy to set up 2 computers running attack from different ends that would half the exectution time  given the case of recursively hashing a password 1,000 times to make this task more computationally expensive     36^7 / 1 000 000 000 / 2 * 1000  seconds = 10.8839117 hours   this represents a maximum length of 7 alpha-numeric characters at a less than half speed execution from quoted figure for  one user   recursively hashing 1,000 times effectively blocks a blanket attack but targetted attacks on user data are still vulnerable",0.018966908797417272,47,so,dictionary-attack|hash|salt|security,why do salts make dictionary attacks  impossible ,6,attacks|exploit|protection|vulnerability|weak password|dictionary attack,0.49165835976600647
6802,any unbroken  n -bit cryptographic hash function has a collision resistance of 2  n /2  this means that if you want to have a 2 128  collision resistance you need to use at minimum a 256-bit hash function as 2 64  operations are achievable you would not want to use a 128-bit hash a 160-bit hash that is a 2 80  security level is borderline using a 256-bit hash will give you a 2 128  security level which should be completely fine for the foreseeable future sometimes weaknesses in the hash function itself result in it being easier to break than the output digest would suggest the sha-2 family of hashes is one example that is currently unbroken sha-1 on the other hand  is broken  it has a 160-bit output but only a 2 63  security level against collisions rather than 2 80   i suspect you may be mixing up collision attacks with other types of attacks a collision attack will not allow an attacker to find input that hashes to an arbitrary value the formal definitions     preimage attack  - given  h  where  fm = h  find any  m   such that  fm  = h . find input that hashes to an arbitrary value     2nd preimage attack  - given  fm = h  find any  m   such that  m ≠ m   and  fm  = h . modify an input without changing the resulting hash     collision attack  - find any pair of  m  and  m   such that  m ≠ m   and  fm = fm  . find any two inputs that have the same hash    each attack has different implications a collision attack is problematic for certificates as they can be used in signatures that are valid for both benign and malicious versions of the same software a preimage attack is problematic for verification imagine if an attacker could modify an executable without changing its hash clearly a preimage attack is far more severe than a collision attack but it is also thankfully far more difficult to pull off the infamously insecure md4 algorithm for example is so bad at collision resistance that it is cheaper to find a collision than it is to run the hash function itself twice however as broken as it is preimage attacks against it are highly theoretical  if on the other hand you simply want to check for  accidental  corruption and are not intending to protect against an active attacker then a crc with a properly-selected polynomial would be ideal a crc can actually  guarantee error detection  up to a point  a perfect hashing algorithm will output random bits for any unique input assuming a perfect hashing algorithm and an infinite amount of memory the number of hashes you can generate before you will on average observe a duplicate is around   a hashing algorithm with a  digest length  of 32 bits will probably produce a duplicate after 77000 hashes or if you want to have a chance of only 0.0001% that you find a duplicate you may only generate 93 hashes using that algorithm that s definitely possible with current hardware  an algorithm with 64 bits output can already be used for over 6 million outputs before you have a chance higher than 0.0001% that a duplicate is produced but still 6 million we can do  so to have a high level of confidence and still being able to generate as many hashes as you like you need a certain number of bits but how many bits you need depends on what confidence level you want to achieve 128 bits is a good level for almost anything  another consideration is resilience against attacks often an algorithm is not completely broken but only weakened more and more until it becomes feasible to do realistic attacks in real-time if ever if your hashing algorithm is weakened you will still want to have enough confidence remaining that s why most protocols output a little more than necessary such as 256 bits  more about the math  https://en.wikipedia.org/wiki/birthday_attack,2018-03-22 17:12:54,728,182089,there are a number of hashing functions in wide spread use now seems the general range of bits is from 128 md5 to 512 sha-2  to prevent different types of hash collisions what are the recommended &amp minimum number of bits?  collisions   identifiers pgp keys bitcoin addresses etc...  files integrity checks  hmac transmission integrity  ,0.03021978021978022,22,sse,hash,minimum number of bits to prevent hash collisions,3,attacks|weakness|protection,0.49162283539772034
20225,let s say your password is   a dismally weak password  a   even without the hash it would take me about .. well hardly any time at all to crack that with a brute force approach  the hashes may make it easier but lack of the hashes doesn t necessarily make it impossible    a  this is why i always use the password   - it takes twice as long to crack :-  password hashes and salts make the password secure when stored in the database they do not protect against front-end cracking  a brute force cracker will run agains a website trying everything from a-999 etc.. a simple password of   will be cracked in no time at all no matter how secure the hash and salt is,2013-01-29 03:26:26.473 UTC,258,14575178,i m seeing it this way if a company can properly protect their hashes and they can prevent brute force attacks against their authentication system then why do passwords need to really have any strength at all?   if someone can t get the hashes and they can t brute-force then the passwords are relatively safe no?  of course the easy answer is yeah but the hashes aren t always kept safe but look at it from the perspective of the business if they require users to keep a strong password they re basically admitting that they can t both protect the hashes and prevent brute force attacks so it s a company who is putting the onus on their users for their own failings no,0.023255813953488372,6,so,brute-force|hash|passwords,why does password strength matter,3,attacks|protection|weak password,0.4904957115650177
8222,use    https://www.php.net/manual/en/function.password-hash.php  which will continually use the most secure method as php continues to release new versions asking users to update passwords isn t a huge issue whirlpool doesn t have any known cryptographic flaws but there are certainly better algorithms to use source  https://www.novatec-gmbh.de/en/blog/choosing-right-hashing-algorithm-slowness/   additionally the salt function shouldn t use   because it s not cryptographically secure if an attacker knows what the salt is before getting access to the database it can speed up crack time once access has been gained  all salt should be globally unique as much as can be reasonable  the algorithm you describe is normal fast hash such hashing algorithms should only be use to check integrity of files but not for hashing password whirlpool and similar algorithms are designed to be fast and thus make brute force attacks easier   use key stretching / password derivation  for instance use pbkdf2 scrypt or argon2 the are designed to make brute force attacks inefficient and thus to make password protection better  whirlpool itself is a cryptographically-secure hash function like sha-512 and has no known weaknesses that would be relevant to hashing secrets however using it directly for password hashing is a bad idea because it is fast allowing an attacker to guess many passwords per second this is not unique to the whirlpool hash a memory-hard kdf like argon2 should be used or  at least  a slow kdf like pbkdf2  see  how to securely hash passwords?  for information on why this is important,2019-08-30 18:42:37,336,216279,i copied the title from  another question  but it wasn t answered there and my case is a bit different  i have inherited an old website to maintain i noticed that the password hashing is not up to modern best practices but i would like to understand if it is reasonably safe or needs changing  here is the code that does the hashing   should i consider this procedure flawed/vulnerable for passwords of 8 characters and longer,0.026785714285714284,9,sse,hash|password-management|passwords|php,is a whirlpool hash with random salt secure enough,5,flaws|attacks|weakness|protection|vulnerability,0.4904916286468506
25865,the wikipedia article on md5 has a  section on security  which includes collision and preimage vulnerabilities     md5 uses the merkle–damgård construction so if two prefixes with the same hash can be constructed a common suffix can be added to both to make the collision more likely to be accepted as valid data by the application using it furthermore current collision-finding techniques allow to specify an arbitrary prefix an attacker can create two colliding files that both begin with the same content all the attacker needs to generate two colliding files is a template file with a 128-byte block of data aligned on a 64-byte boundary that can be changed freely by the collision-finding algorithm an example md5 collision with the two messages differing in 6 bits is       both produce the md5 hash 79054025255fb1a26e4bc422aef54eb4.[38] the difference between the two samples is the leading bit in each nibble has been flipped for example the 20th byte offset 0x13 in the top sample 0x87 is 10000111 in binary the leading bit in the byte also the leading bit in the first nibble is flipped to make 00000111 which is 0x07 as shown in the lower sample   if you just want to find any two inputs with the same md5 and don t care what that hash is the collision attack is for you  if instead you need to find an input that generates a particular output you need a  preimage attack  the references the article includes for the full theoretical preimage attack with computational complexity of 2 123.4  complexity are   yu sasaki kazumaro aoki 16 april 2009  finding preimages in full md5 faster than exhaustive search  springer berlin heidelberg  ming mao and shaohui chen and jin xu 2009  construction of the initial structure for preimage attack of md5  international conference on computational intelligence and security ieee computer society 1 442–445 doi:10.1109/cis.2009.214 isbn 978-0-7695-3931-7   note that this complexity is only slightly less than the data complexity of storing every possible md5 hash output without storing the string that generates the given hash which may be very long 2 128  * 2 7  = 2 135  this is ~10 27   tera bytes which is several orders of magnitude greater than the entire hard disk capacity of the entire world  and of course there is always the brute force approach you can compute md5 hashes for incremental inputs in a loop until you find an input that produces the given hash this might take a very long time if you store these values you re essentially creating a  rainbow table  but you mention a salt which is the traditional means of defeating a rainbow table but if you know the salt will remain fixed as in you re attacking a particular password in a database then you can brute force it with this method  finally you can always do a web search for a given md5 google will happily tell you that   is the  md5 for  password   and will probably succeed for an md5 of any common english word or even an arbitrary input string below a certain length if you have weak passwords and weak salts you might get lucky with this approach,2014-10-14 20:36:42.21 UTC,581,26369835,i m using the following lines to create a hashed string    my question is how can i find a collision? meaning finding two different strings which create the same output? what would be the best strategy? thanks,0.01721170395869191,10,so,hash|md5|random,what is the best strategy to create a collision with md5 hash algorithm,4,attacks|weakness|vulnerability|weak password,0.49044427275657654
2958,first salts are not meant to be secret so i m not sure what is accomplished by this approach  second if  you  have to brute-force the salt then the  attacker  can too what i m assuming is that your approach depends on the attacker not knowing your custom encryption scheme and this is a bad idea the strength of encryption is that it is secure even if an attacker knows your scheme if you are using a secure encryption algorithm then you don t need to try to improve it often improving a known scheme weakens it    assuming that you re using per-user salts i.e a different salt for each user account rainbow tables are already an impractical means of attacking the hashed passwords rainbow tables are typically used where there is either no salt or a salt which is common to all accounts and preferably known to the attacker ahead of time  where you re using a password hashing scheme such as bcrypt if you re looking to make this harder for an attacker it s best to use the mechanisms provided by the scheme e.g the work factor rather than inveting your own,2015-02-23 17:23:34,284,82346,would it be a good idea security-wise to store salts with their last character removed and then bruteforce the last character to further the amount of time it would take to create rainbow tables and such? this is assuming the hashing algorithm in use is whirlpool  this would make logins take a little longer but it would be a restriction on bruteforcing speed that cannot be bypassed like other methods can  thank you in advance,0.028169014084507043,8,sse,databases|hash|salt,is bruteforcing the last character of a salt a good idea,3,bypass|attacks|weakness,0.4903750717639923
15746,i think the key here is the word feasible in crypto-land feasible means reasonable amount of time compared to the value of whatever it is i m trying to break or maybe less time that it would take using brute-force depending on how you look at things  so if i can find 1 collision feasibly then i can find n collisions feasibly because   is still small  there will still be  some  n where    would this apply to other hash functions? i believe so but i could be wrong  let the flaming commence  this is not a property of all hash functions but a weakness of the  merkle–damgård construction  which md5 and sha-1 are based on known as  length extension  the weakness involves the fact that you can resume the hash calculation with specially-selected appended data for full details of how this is used to generate arbitrarily many collisions see     multicollisions in iterated hash functions application to cascaded constructions   antoine joux   for a related attack based on this idea see    understanding hash length extension attacks    flickr s api signature forgery vulnerability,2010-10-25 16:46:58.93 UTC,305,4016875,from wikipedia i read      joux[3]  noted that 2-collisions lead to n-collisions if it is feasible to find two messages with the same md5 hash it is effectively no more difficult to find as many messages as the attacker desires with identical md5 hashes     but why is this so? i can t imagine why? the algorithms are open right people can read the maths which generates the hashes which is the digest machinery so if we know one collision why does it help find new ones?  is it just making small iterations to both of the first collision messages and then monitoring their changes to remap them,0.022950819672131147,7,so,cryptography|digest-authentication|hash|security,why does a collision detection in a cryptographic hash function make finding other collisions easier,4,attacks|forgery|weakness|vulnerability,0.48877212405204773
26922,"if you store the salted password sha-1 is fine for practical purposes.sha-2 is considered more secure but sha-1 is not a problem unless you have a reason to be truly paranoid  here is what nist  says       the results presented so far on sha-1  do not call its security into  question however due to advances in  technology nist plans to phase out of  sha-1 in favor of the larger and  stronger hash functions sha-224,  sha-256 sha-384 and sha-512 by   2010   serious vulnerabilities have been discovered in sha-1 that make the search much faster than brute force it is still largely intractable but that isn t expected to be the case for too much longer paranoid programmers favour something from the sha-2 family  from  this article  regarding the original 2005 result     it s time to walk but not run to the fire exits you don t see smoke but the fire alarms have gone off   it s not that the current cryptanalysis makes sha-1 unsafe but rather that the crypto community is worried that worse news might be just around the corner this fear also applies to sha-2 which exhibits the same flaws as sha-1 albeit over a much larger search space hence the ongoing quest for  sha-3   in short sha-1 is safe right now and probably will be for some time come but the crypto community is uncomfortable with the prognosis  your description sounds accurate for the current state of the art  you shouldn t be using a single iteration of any hash function though at the very least you should iterate many times 1000 iterations of the hash increases the attacker s work 1000-fold it increases your work by the same amount but you re doing a lot less password hashing than they are  ideally however you should use an existing password storage primitive such as those described  here   the previous answers don t make any mention of gpus which can parallellise sha-1 hashing to the extent that an entire database can now be brute forced in minutes or hours rather than days or weeks even if the passwords have been salted  modern password hash algorithms such as bcrypt or scrypt are designed specifically to be difficult to run on gpus due to the fact that they are block ciphers with much higher memory requirements and memory access in a gpu can not be parallellised to the same extent they also have a work function which allows them to be made slower on the fly as technology improves  in short you should only use the best tools for the job and sha-1 falls very far short of the state of the art  for further reading    https://crypto.stackexchange.com/questions/400/why-cant-one-implement-bcrypt-in-cuda    http://codahale.com/how-to-safely-store-a-password/    http://www.codinghorror.com/blog/2012/04/speed-hashing.html    https://security.stackexchange.com/questions/4781/do-any-security-experts-recommend-bcrypt-for-password-storage/6415#6415    sha1 is a  message digest  it was  never  meant to be password-hashing or key-derivation function although it could be used a building block for a kdf such as in pbkdf2 with hmac-sha1.  a password-hashing function should defend against dictionary attacks and rainbow tables several algorithms have been designed to achieve this goal  currently the best choice is probably  argon2  this family of password hashing functions won the password hashing competition in 2015  if  argon2  is not available the only other standardized password-hashing or key-derivation function is  pbkdf2  which is an oldish nist standard other choices if using a standard is not required include  bcrypt  and the  scrypt   wikipedia has pages for these functions    https://en.wikipedia.org/wiki/argon2    https://en.wikipedia.org/wiki/bcrypt    https://en.wikipedia.org/wiki/scrypt    https://en.wikipedia.org/wiki/pbkdf2    the short answer to your question is sha-1 is as secure as you can get md5 would be fine too even md4 but it could make some investors nervous for  public relations  it is best to use a better hash function e.g sha-256 even if you truncate its output to 160 or 128 bits to save on storage cost some of the  sha-3 round-2 candidates  appear to be faster than sha-1 while being arguably more secure yet they are still a bit new so sticking to sha-256 or sha-512 would be a safer route right now it would make you look professional and cautious which is good  note that as secure as you can get is not the same as perfectly safe see below for rather lengthy explanations   about known attacks   the known attacks on md4 md5 and sha-1 are about collisions which do not impact preimage resistance it has been shown that md4 has a few weaknesses which can be only theoretically exploited when trying to break hmac/md4 but this does not apply to your problem the 2 106  second preimage attack in the paper by kesley and schneier is a generic trade-off which applies only to very long inputs 2 60  bytes that s a million terabytes -- notice how 106+60 exceeds 160 that s where you see that the trade-off has nothing magic in it  the rest of this message assumes that the hash function you use e.g sha-1 is a black box with no special property that the attacker may use that s what you have right now even with the broken hash functions md5 and sha-1   about rainbow tables   the rainbow attack is actually cost-sharing of a dictionary or brute force attack it is a derivative from the  time-memory trade-off  first described by hellman in 1980 assuming that you have  n  possible passwords that s the size of your dictionary or 2  n   if you consider brute-forcing a hash function with an output of  n  bits there is a time-sharing attack in which you precompute the  n  hashed passwords and store them in a big table if you sort the hash outputs you can get your password in a single lookup a  rainbow table  is a smart way to store that table with a much reduced space you store only  n/t  hashed passwords and you crack passwords with o t  2  lookups rainbow tables allow you to virtually handle precomputed tables much larger than what you can realistically store  however rainbow or not the attacker still has to run the full attack at least once this can be viewed as several successive optimization layers   the brute-force / dictionary attack has cost  n  for cracking each password  with a pre-computed table the attacker pays that cost  n   once  and can thereafter attack  many  passwords with very small extra cost per password  if the pre-computed table is a rainbow table then  n  can be somewhat bigger because  storage  cost is reduced the bottleneck on  n  becomes the cpu power that the attacker can muster not the size of his harddisks   if  n  is large enough that the cpu-cost of hashing  n  passwords is ludicrous then such an attack is not feasible regardless of whether rainbow tables are used or not this means that a preimage-resistant hash function with an output of 80 bits or more is enough to make brute-force attack infeasible   about salts   salts are a way to defeat pre-computations in the description above the salt brings back the attacker to step 1 salting prevents the attacker from sharing the o n  cost between several attacked passwords pre-computed tables  a fortiori  rainbow tables are no longer feasible  you want salting because when the hashed data consists in  passwords  i.e something which fits within the brain of a random human being then  n  can be quite low humans are really bad at choosing and remembering passwords this is what dictionary attacks are about that s using a reduced space of potential passwords the dictionary under the assumption that many user passwords will be in that specially selected space  hence salting will at least prevent the attacker from using pre-computed tables in particular pre-computed rainbow tables this assumes that the attacker  will be  able to break one password or two we do not want him to break 1000 other passwords with little extra overhead  also salting is good for public relations   about sha-1 cost   the elementary cost of sha-1 is about hashing a 64-byte block that s how sha-1 works data is padded then split into 64-byte blocks the cost of processing a single block is about 500 clock cycles on an intel core2 system and that s for a single core md5 and md4 are faster count about 400 and 250 cycles respectively do not forget that most modern cpu have several cores so multiply accordingly  some salting schemes prescribe huge salts e.g what enters the hash function is actually 40000 successive copies of a single 128-bit salt followed by the password itself this makes password hashing more expensive by a factor of 10000 with my example both for the legitimate user and for the attacker whether this is a good idea depends on the setup for login on a desktop system this is good the user will not even notice that it took 10ms to hash his password instead of 1µs but the cost for the attacker has risen by a very noticeable factor 10000 on shared servers with thousands of clients per second the aggregate cost may become prohibitive conceptually raising the bar by the same factor for the legitimate user and the attacker is not ultimately good security but it can be a worthwhile idea in some specific situations   about online attacks   all of the above is about defeating  offline  attacks an offline attack is an attack where the attacker has all the data he needs in order to test passwords e.g the attacker could get a copy of the database holding the hashed passwords in an offline attack the attacker is limited only by his computational resources conversely an  online  attack is an attack where each guess by the attacker must go through an honest verifier e.g the attacker simply tries to log in on the attacked system online attacks are thwarted by enforcing limits on how many passwords can be tried per second extreme examples are smartcards which shut down after three wrong pins  usually for password security it pays off much more to arrange the system for not letting an attacker build an offline attack that s what unix systems do the hashed passwords which used to be in the world-readable   file are now in the   file which is protected against read access except by a few privileged applications the assumption here is that if the attacker can read   then he probably has enough control over the system that he does not really need passwords anymore..  as of feb 2017 sha-1 should no longer be considered secure google has reported success with collision attacks against the full non-reduced-round sha-1  link to report  for google s announcement  click here   edit as pointed out by others passwords are not vulnerable to hash collision attacks however as a general guideline i would not choose sha-1 for security-related applications there are better alternatives out there",2010-05-05 09:38:09.807 UTC,2483,2772014,"conclusion  sha-1 is as safe as anything against preimage attacks however it is easy to compute which means it is easier to mount a bruteforce or dictionary attack the same is true for successors like sha-256. depending on the circumstances a hash function which was designed to be computationally expensive such as bcrypt might be a better choice   some people throw around remarks like sha-1 is broken a lot so i m trying to understand what exactly that means let s assume i have a database of sha-1 password hashes and an attacker whith a state of the art sha-1 breaking algorithm and a botnet with 100,000 machines gets access to it having control over 100k home computers would mean they can do about 10^15 operations per second. how much time would they need to   find out the password of any one user?  find out the password of a given user?  find out the password of all users?  find a way to log in as one of the users?  find a way to log in as a specific user?   how does that change if the passwords are salted? does the method of salting prefix postfix both or something more complicated like xor-ing matter?  here is my current understanding after some googling please correct in the answers if i misunderstood something   if there is no salt a rainbow attack will immediately find all passwords except extremely long ones  if there is a sufficiently long random salt the most effective way to find out the passwords is a brute force or dictionary attack neither collision nor preimage attacks are any help in finding out the actual password so cryptographic attacks against sha-1 are no help here it doesn t even matter much what algorithm is used - one could even use md5 or md4 and the passwords would be just as safe there is a slight difference because computing a sha-1 hash is slower   to evaluate how safe just as safe is let s assume that a single sha1 run takes 1000 operations and passwords contain uppercase lowercase and digits that is 60 characters that means the attacker can test 10 15 *60*60*24 / 1000 ~= 10 17  potential password a day for a brute force attack that would mean testing all passwords up to 9 characters in 3 hours up to 10 characters in a week up to 11 characters in a year it takes 60 times as much for every additional character. a dictionary attack is much much faster even an attacker with a single computer could pull it off in hours but only finds weak passwords  to log in as a user the attacker does not need to find out the exact password it is enough to find a string that results in the same hash this is called a first preimage attack as far as i could find there are no preimage attacks against sha-1 a bruteforce attack would take 2 160  operations which means our theoretical attacker would need 10 30  years to pull it off limits of theoretical possibility are around 2 60  operations at which the attack would take a few years. there are  preimage attacks against reduced versions of sha-1  with negligible effect for the reduced sha-1 which uses 44 steps instead of 80 attack time is down from 2 160  operations to 2 157  there are collision attacks against sha-1 which are well within theoretical possibility  the best i found  brings the time down from 2 80  to 2 52  but those are useless against password hashes even without salting   in short storing passwords with sha-1 seems perfectly safe did i miss something?   update  marcelo pointed out an article which mentions  a second preimage attack in 2 106  operations   edit  as  thomas explains  this attack is a hypothetical construct which does not apply to real-life scenarios. i still don t see how this spells danger for the use of sha-1 as a key derivation function though are there generally good reasons to think that a collision attack or a second preimage attack can be eventually turned into a first preimage attack",0.031010873942811115,77,so,cryptography|hash|sha1,is sha-1 secure for password storage,7,flaws|attacks|exploit|weakness|protection|weak password|vulnerability,0.48872482776641846
1610,"given that bcrypt generates a random salt per password and that the github database wasn t compromised how was a remote brute force attack possible and how long* was that process?   two different attacks are being confused here  brute forcing password hashes means the attacker has got ahold of the database table containing the password hashes and if they are properly salted and hashed then it is very time-consuming to brute force    in this case github was attacked by guessing passwords at the login page so it doesn t matter how the passwords are salted or hashed or anything  it just matters if the user in question used an easy password  github says we aggressively rate-limit login attempts which means it locks an account out after x tries for a certain amount of time  the attacker had resources 40,000 unique ips to guess from many accounts to choose from and a long period of time so they could lazily keep guessing and waiting for the lockout to expire  even with all those resources their bot can t guess fast enough to break into accounts with anything but dead simple passwords   it is really great that github posts a security history at  https://github.com/settings/security  for your account to see account activity failed logins logins etc.  if you are bruteforcing based on characters then you would have about 219 trillion needed guesses if just using letters and numbers for passwords of 8 characters of length that s a lot   with weak passwords people are often refering to commonly used passwords like azerty123,a1234567 .. or words in a dictionary if you are using a dictionary you can reduce the amount of possible passwords to maybe 1 million which means all you need is 1 million guesses   the biggest issue is that people tend to reuse passwords across websites so when adobe got hacked their database was leaked due to their bad way of encrypting not hashing of passwords attackers could generate a list of commonly used passwords they could for instance then have used the associated email address to try and log into github using the same password because as said people often use the same password everywhere",2013-11-21 10:19:49,446,45834,github faced a  brute force password-guessing attack  recently that involved nearly 40k unique ip addresses  passwords were also stored properly using bcrypt salt + hashed  given that bcrypt generates a random salt per password and that the github database wasn t compromised how was a remote brute force attack possible and how long* was that process?     these addresses were used to slowly brute force weak passwords or passwords used on multiple sites,0.026905829596412557,12,sse,bcrypt|network,how were weak passwords brute forced in github,3,leak|attacks|weak password,0.48862966895103455
19712,salting is usually used in password hashes to avoid dictionary attacks there are plenty of web based reverse hash dictionaries where you enter the hash say 1a79a4d60de6718e8e5b326e338ae533 and get back the text example with salt this becomes next to impossible if you prepend a password with random salt the dictionary attack become more difficult  as for collisions i don t think you need to worry about entire files having the same md5 or sha1 hash it s not important the important use of the hash is to prove the file you receive is the same as the file that was approved by someone who is an authority on the file if you add salt to the file you need to send the salt so the user can verify the hash   this actually makes it  easier  for the attacker to spoof your file because he can provide a false salt along with the false file the user can usually tell if the file is faked because it no longer serves the purpose it is supposed to but how is the user supposed to know the difference between the correct salt and the attacker s salt?  adding salt to your hash function doesn t really serve any purpose if the digest function has been compromised because the salt will have to be made public to be used and the attacker can adjust their file to factor this in too  the solution to this problem is to use a secure hash function  md5 has shown to be vulnerable to hash collision but i believe sha-1 has not so far  you are mixing up two different uses of hash values    checksumming for guarding against random non-malicious errors    computing cryptographical message digests for storing passwords signing messages certificates ..    crcs are a good choice for the first application but totally unsuited for the second because it is easy to compute a collision in math-speak crcs are linear this is what your friend is essentially telling you  md5 and sha1 are cryptographic hashes intended for the second kind of application however md5 has been cracked and sha1 is considered weak these days still even though md5 can be cracked it takes a long time to find md5 collisions days to weeks  as for salt it makes the computation of the cryptographic hash local by mixing in some random non-secret value this value is called the  salt  this prevents computing global tables which make it easy to compute possible values e.g passwords from the hash value the computation of the tables is extremely expensive but without salt the cost would be amortized over many cracked passwords  the attack against crc-32 is irrelevant if the hash you are using is not crc-32 -  md5  and  sha-1  are not vulnerable to that kind of attack yet   the current attacks against md5 are where an attacker creates two documents with the same hash  salts are used for password verification - they prevent an attacker performing an offline attack against the password database - each user s password has a salt attached to the plain-text before the hashing - then a pre-computed rainbow table of plaintext &lt;-> hashed text is useless,2009-03-09 09:27:03.313 UTC,604,625542,i remember a guy telling me that if i let him change 4 bytes he can make a file have any checksum he wants  crc-32   i heard mention of salting a hash i am wondering if someone had his file match my file would salting the md5 or sha-1 hash change the result so both files no longer collide? or does it change the end hash value only,0.024834437086092714,15,so,collision|hash,hash and salt collision,4,attacks|spoofing|weakness|vulnerability,0.4883653223514557
64259,,2019-09-11 13:14:18,44,57890060,the calculation i have right now is as follows  2^36 / 2^14 = 2^228 bytes x 2^22 = 2^25,0.022727272727272728,1,so,cpu|hardware|memory|memory-leaks|memory-management,what s the maximum size of a page table if you have 64-bit addresses max 64gb and 16 kb pages,1,memory leaks,0.4880177974700928
14820,double hashing is often supported out of the box by password crackers have a look at  hashcat s algorithms  section  even with a more clever scheme you should be aware that one can brute-force about  100 giga md5 per second  with a good gpu if you are interested in more indepth information you can have a look at my  tutorial  about secure password storage  using any scheme you might come up with yourself to store passwords is inherently less secure than using one of the schemes specifically designed by cryptographers for password storage there are a lot of potential vulnerabilities with hashing schemes which can be prevented that way including susceptibility to brute-force attacks or dictionary attacks mathematical issues resulting in a weakening of the hash function e.g with md5 and other issues  specifically md5 even with a salt or multiple rounds is trivially insecure so that even good passwords can be broken by just brute-force in seconds  you should never store passwords hashed with md5 anywhere!   as an example with  hashcat  on my notebook i can brute-force about 1.8 billion md5 hashes per second this number can be significantly increased my several magnitudes by using one or more gpus or by leveraging more knowledge about the passwords e.g with pre-computed rainbow tables or by using variations of dictionary words  thus instead of using a generic hash function you should always use a modern hash algorithm designed for password storage specifically one of these   argon2  scrypt  bcrypt  pbkdf2   each of these algorithms was specifically designed for password hashing i.e to be slow to not allow easy and/or cheap brute-force attacks  there are mature libraries supporting any these algorithms for most languages available if in doubt try to use a libsodium-based library for your programming language in general you should use the secure solutions available for your programming language or framework instead of inventing your own schemes  see  https://libsodium.gitbook.io/doc/password_hashing  for details about the challenges of password hashing and available modern solutions,2019-08-14 10:55:49.4 UTC,406,57493323,is there any vulnerability associated with self-hashing?   let s say i want to keep a database of md5-hashed passwords the issue is that most people will use easy pws like  password   admin  etc all prime candidates for reverse md5 lookup services my idea is to hash the hash i.e   in real-life examples we use salts but is the above even a bad idea does it really bring inherent flaws,0.019704433497536946,8,so,md5|password-hash|passwords,vulnerability of serial self-hashing,4,flaws|attacks|weakness|vulnerability,0.48762381076812744
11143,i am not an expert at this sort of thing but i keep up with the security community and a lot of people there consider the md5 hash broken i would say that which one to use depends on how sensitive the data is and the specific application you might be able to get away with a slightly less secure hash as long as the key is good and strong  here are my suggestions for you   you should probably forget md5 if you anticipate attacks there are many  rainbow tables  for them online and corporations like the riaa have been known to be able to produce sequences with equivalent hashes  use a  salt  if you can including the message length in the message can make it very difficult to make a useful hash collision  as a general rule of thumb more bits means less collisions by pigeonhole principle and slower and maybe more secure unless you are a math genius who can find vulnerabilities   see here for a paper detailing an algorithm to create md5 collisions in 31 seconds with a desktop intel p4 computer   http://eprint.iacr.org/2006/105   it would be a good ideea to take a look at the  blake2  algorythm  as it is described it is faster than md5 and at least as secure as sha-3 it is also implemented by  several software applications  including winrar  which one you use really depends on what you are using it for  if you just want to make sure that files don t get corrupted in transit and aren t that concerned about security go for fast and small  if you need digital signatures for multi-billion dollar federal bailout agreements and need to make sure they aren t forged go for hard to spoof and slow  i would like to chime in before md5 gets torn apart that i do still use md5 extensively despite its overwhelming brokenness for a lot of crypto  as long as you don t care to protect against collisions you are still safe to use md5 in an hmac as well and you do want the speed sometimes you want a slower hash then you can still use md5 confidently  in md5 s defense there is no known way to produce a file with an arbitrary md5 hash the original author must plan in advance to have a working collision thus if the receiver trusts the sender md5 is fine md5 is broken if the signer is malicious but it is not known to be vulnerable to man-in-the-middle attacks  update  times have changed we have a sha3 winner i would recommend using  keccak  aka  sha3  winner of the sha3 contest   original answer  in order of weakest to strongest i would say   ripemd broken should never be used  as can be seen in this pdf   md-5  broken should never be used  can be broken in 2 minutes with a laptop    sha-1 broken should never be used  is broken in principal attacks are getting better by the week   sha-2 weak will probably be broken in the next few years  a few weaknesses have been found  note that generally the higher key size the harder the hash function is to break while key size = strength is not always true it is mostly true so sha-256 is probably weaker than sha-512  skein no known weaknesses  is a candidate for sha-3  it is fairly new and thus untested  it has been implemented in a bunch of languages   md6 no known weaknesses is another a candidate for sha-3 probably stronger than skien but slower on single core machines like skien it is untested some security minded developers are using it in  mission critical roles    personally i d use md6 because one can never been too paranoid if speed is a real concern i d look at skein or sha-256   all hash functions are broken   the  pigeonhole principle  says that try as hard as you will you can not fit more than 2 pigeons in 2 holes unless you cut the pigeons up similarly you can not fit 2^128 + 1 numbers in 2^128 slots all hash functions result in a hash of finite size this means that you can always find a collision if you search through finite size + 1 sequences it s just not feasible to do so not for md5 and not for  skein   md5/sha1/sha2xx have no chance collisions  all the hash functions have collisions its a fact of life coming across these collisions by accident is the equivalent of  winning the intergalactic lottery  that is to say  no one wins the intergalactic lottery  its just not the way the lottery works you will not come across an accidental md5/sha1/sha2xxx hash ever every word in every dictionary in every language hashes to a different value every path name on every machine in the entire planet has a different md5/sha1/sha2xxx hash how do i know that you may ask well as i said before no one wins the intergalactic lottery ever   but .. md5 is broken    sometimes the fact that its broken does not matter    as it stands there are no known  pre-image or second pre-image attacks  on md5  so what is so broken about md5 you may ask? it is possible for a third party to generate 2 messages one of which is evil and another of which is good that both hash to the same value  collision attack   nonetheless the current rsa recommendation is not to use md5 if you need pre-image resistance people tend to err on the side of caution when it comes to security algorithms   so what hash function should i use in .net?   use md5 if you need the speed/size and don t care about birthday attacks or pre-image attacks    repeat this after me  there are no chance md5 collisions  malicious collisions can be carefully engineered even though there are no known pre-image attacks to date on md5 the line from the security experts is that md5 should not be used where you need to defend against pre-image attacks  same goes for sha1    keep in mind not all algorithms need to defend against pre-image or collision attacks take the trivial case of a first pass search for duplicate files on your hd    use sha2xx based function if you want a cryptographically secure hash function    no one ever found any sha512 collision ever they have tried really hard for that matter no one ever found any sha256 or 384 collision ever     don t use sha1 or ripemd unless its for an interoperability scenario    ripmed has not received the same amount of scrutiny that shax and md5 has received both sha1 and ripemd are vulnerable to birthday attacks they are both slower than md5 on .net and come in the awkward 20 byte size its pointless to use these functions forget about them   sha1 collision attacks are down to 2^52 its not going to be too long until sha1 collisions are out in the wild   for up to date information about the various hash functions have a look at  the hash function zoo   but wait there is more   having a  fast  hash function can be a curse for example a very common usage for hash functions is password storage essentially you calculate hash of a password combined with a known random string to impede rainbow attacks and store that hash in the database   the problem is that if an attacker gets a dump of the database he can quite effectively guess passwords using brute-force every combination he tries only takes a fraction of millisecond and he can try out hundreds of thousands of passwords a second    to work around this issue the  bcrypt  algorithm can be used it is designed to be slow so the attacker will be heavily slowed down if attacking a system using bcrypt recently  scrypt  has made some headline and is considered by some to be more effective than bcrypt but i do not know of a .net implementation   in cryptography hash functions provide three separate functions     collision resistance  how hard is it for someone to find two messages  any  two messages that hash the same   preimage resistance  given a hash how hard is it to find another message that hashes the same? also known as a  one way hash function    second preimage resistance  given a message find another message that hashes the same    these properties are related but independent for example collision resistance implies second preimage resistance but not the other way around for any given application you will have different requirements needing one or more of these properties a hash function for securing passwords on a server will usually only require preimage resistance while message digests require all three  it has been shown that md5 is not collision resistant however that does not preclude its use in applications that do not require collision resistance indeed md5 is often still used in applications where the smaller key size and speed are beneficial that said due to its flaws researchers recommend the use of other hash functions in new scenarios  sha1 has a flaw that allows collisions to be found in theoretically far less than the 2^80 steps a secure hash function of its length would require the attack is continually being revised and currently can be done in ~2^63 steps - just barely within the current realm of computability for this reason nist is phasing out the use of sha1 stating that the sha2 family should be used after 2010  sha2 is a new family of hash functions created following sha1 currently there are no known attacks against sha2 functions sha256 384 and 512 are all part of the sha2 family just using different key lengths   ripemd i can t comment too much on except to note that it isn t as commonly used as the sha families and so has not been scrutinized as closely by cryptographic researchers for that reason alone i would recommend the use of sha functions over it in the implementation you are using it seems quite slow as well which makes it less useful  in conclusion there is no one best function - it all depends on what you need it for be mindful of the flaws with each and you will be best able to choose the right hash function for  your  scenario,2009-04-29 02:47:23.253 UTC,1973,800685,the .net framework ships with 6 different hashing algorithms    md5 16 bytes time to hash 500mb 1462 ms  sha-1 20 bytes 1644 ms  sha256 32 bytes 5618 ms  sha384 48 bytes 3839 ms  sha512 64 bytes 3820 ms  ripemd 20 bytes 7066 ms   each of these functions performs differently md5 being the fastest and ripemd being the slowest  md5 has the advantage that it fits in the built-in guid type  and it is the basis of the type 3 uuid   sha-1 hash is the basis of type 5 uuid  which makes them really easy to use for identification  md5 however is vulnerable to  collision attacks  sha-1 is also vulnerable but to a lesser degree   under what conditions should i use which hashing algorithm?  particular questions i m really curious to see answered are     is md5 not to be trusted? under normal situations when you use the md5 algorithm with no malicious intent and no third party has any malicious intent would you expect any collisions meaning two arbitrary byte[] producing the same hash    how much better is ripemd than sha1? if its any better its 5 times slower to compute but the hash size is the same as sha1     what are the odds of getting non-malicious collisions when hashing file-names or other short strings? eg 2 random file-names with same md5 hash with md5 / sha1 / sha2xx in general what are the odds for non-malicious collisions?     this is the benchmark i used,0.01824632539280284,36,so,.net|c#|cryptographic-hash-function|cryptography|hash,which cryptographic hash function should i choose,7,flaws|attacks|spoofing|weakness|protection|vulnerability|man in the middle,0.4838414490222931
21801,you should set maxreceivedmessagesize=2147483647 to increase message size try to change config to this   but it is a bad practice to increase you message values to max value this can lead you to serious troubles with dos leaks    updated,2013-10-24 10:58:45.933 UTC,78,19564047,the maximum message size quota for incoming messages 65536 has been exceeded to increase the quota use the maxreceivedmessagesize property on the appropriate binding element,0.038461538461538464,3,so,asp.net|wcf,wcf-the maximum message size quota for incoming messages 65536 has been exceeded,3,leak|bad practices|denial of service,0.48376429080963135
4323,with this scheme you are certainly making an attacker’s life harder here is the general attack algorithm for cracking bcrypt-protected plaintext passwords   obtain hashes obviously   generate candidate passwords for password cracking tool  perform the cracking attempt   and here is the attack algorithm for cracking your sha1-hashed-than-bcrypted passwords   obtain hashes  generate candidate passwords for password cracking tool  calculate corresponding sha-1  perform the cracking attempt   so as you can see the attacker must perform one cheap additional step to crack the password so your scheme should provide at least the same security level as using bcrypt to protect plaintext passwords  as a small bonus you are getting scheme which is not implemented out-of-box in common cracking tools it will make password cracking even harder if the attacker is not very sophisticated and will not be able to figure out hashing scheme but this feature is security by obscurity so it is better to expect that attacker will know all about your password hashing approach  i see no faults there when using bcrypt it s often actually a good idea to use a regular hash first because bcrypt has a little known downside of being limited to only 72 characters of input using a hash before makes the input size effectively unlimited while not reducing security at all so not only do you get the extra strength of bcrypt but continuing to use sha-1 improves bcrypt s flexibility rather than improving its strength which it only improves by an insignificant amount,2016-04-06 08:19:16,416,119612,let s say we have a database with usernames and passwords where the passwords are naively secured using a single round of sha-1 such as was kinda best practice a number of years ago  now let s say we wish to upgrade the security of the credential storage but users must remain blissfully unaware that any change is taking place  obviously we don t have the passwords in plaintext but i am considering just treating the sha1password as if it was the plaintext password and secure on top of that  here s my thinking   generate random salt per user 128 bit  replace   in the database with    change the validation-code to validate passwords by first doing sha1 then bcrypt the result with the salt n times and then check   are there any glaring faults in this strategy?  while not perfect is it reasonable to say that this vastly improves the security of the users passwords in the case of a breach,0.026442307692307692,11,sse,bcrypt|hash|sha,upgrading security of hashed passwords,3,attacks|protection|plaintext password,0.4807855486869812
67740,quantum computers do offer more efficient attacks against some methods of encryption for example  shor s algorithm  when combined with a quantum computer will allow faster prime factorization of semi-primes this in particular would allow for faster attacks against algorithms such as rsa that depend on the hardness of semi-prime factorization  however this does not imply that all cryptography will be obsolete   post-quantum cryptography  specifically focuses on algorithms that will remain secure against the properties of quantum computers furthermore  quantum cryptography  specifically makes it possible to use quantum physics to perform cryptographic operations in new sometimes more secure ways,2015-10-30 04:48:39,125,104122,since quantum computers has potential to become super computers of next generation will that make cryptography obsolete..,0.024,3,sse,cryptography|encryption|passwords|penetration-test,quantum computation and its threats,2,attacks|penetration test,0.4793373644351959
67054,attack using 100 mbps link means you have to create roughly 210k packets to fully utilize your 100 mbps connection   100 mbps = 12500000 bytes  12500000 / 60 bytes = 208333.3333333333 packets  depending on your machine you will require multiple processes to generate this amount of packets single process won t generate these many packets in one second,2012-06-11 11:44:38,109,15909,given the size of a tcp packet is 60 bytes there is a syn flood attack on a computer made ​​by a 100 mb bandwidth the question is how many syn packets are sent in a second? is there any formula for calculating,0.03669724770642202,4,sse,attacks|exploit|network,syn flood size per second,2,attacks|exploit,0.4772922694683075
17369,"absolutely you can not use adler32 for password hashing  for short inputs substantial information can be gained from a 8-bit crc typically for a cryptographic hash a one bit change in the input data 50% of the output bits will change  it isn t designed to avoid collisions it is a crc many hash functions are designed for other purposes where collisions are not a problem such as dictionary lookups or the storage bins at a bottle club which just uses the last couple of digits of a membership number to achieve somewhat even distribution of bottles across storage bins   password security   just using a hash function is not sufficient and just adding a salt does little to improve the security instead iiterate over an hmac with a random salt for about a 100ms duration and save the salt with the hash use functions such as pbkdf2 password_hash bcrypt and similar functions the point is to make the attacker spend a lot of time finding passwords by brute force  one reason why sha-512 with a salt is not sufficient is a laptop mine can execute 750,000 per second this would be applied to a list of 10,000,000 passwords sorted by frequency of usage then there are special programs that fuzz those unless it is spear-fishing an attacker will probably be satisfied with 90% of the passwords cracked so by lengthening the computer time from &lt;2us to >100ms it takes the attacker 50,000 times as long he will probably move on the the next site  protecting your users is important please use secure password methods   here is why an attacker hits your site gets the md5 passwords uses brute force with a list of common passwords and has the user s username and password now the attacker uses this on another site to gain access to more sensitive data since most users re-use passwords you have just helped compromise the user note a decent hacker rate could be 1 billion/second attacker will love your site and you will not even know it was successfully attacked  one does not decrypt a hash and a hash does not hide the real text -- encryption not hashing does that but if you mean can adler-32 be used as a cryptographic hash? then absolutely not the requirement for a cryptographic hash is that it be extremely hard effectively impossible for foreseeable hardware and mathematics to construct a message with a given hash it is quite easy to do that for an adler-32 and in fact is easy with  any  32-bit hash 32-bits is simply not enough",2016-10-28 18:33:55.947 UTC,477,40311228,i heard that we shouldn t rely on adler32 and i want to ask why   why shouldn t we trust adler32 to hash ? is it reversible ? or can we just see the real text with ease ,0.018867924528301886,9,so,adler32,is adler32 easy to decrypt and why,3,attacks|protection|sensitive data,0.4769497513771057
2465,mayb it s sha-256 encod or any other 256 bit hash algorithm becaus if you base64 decod it and hex encod you get the first has an length of 16 and the second a length of 64 that s probabl not a coincid edit mayb it s hash multipl time an iter hash as thi post say it is better to decompil the softwar,2020-03-06 18:59:21,129,60570097,be a pentest i have encount a hash divid in two part the first one probabl be the salt seemingli encod in base64 but i am unabl to find out the encrypt type the input that gave me thi hash is the string password is anybodi abl to give me a hint ? thank in advanc,0.015503875968992248,2,so,base64|hash|penetration-testing|reverse-engineering,what is thi hash algorithm,2,pentest|penetration test,0.4761877954006195
47914,can i use it as a random number generator?   you really shouldn t use it as a random number     why?   you shouldn t because race conditions are undefined behavior  this random number doesn t have a good entropy as it is depend on timing  there are much better way to generate random numbers in    undefined behavior should not be used as source or one of sources of entropy in c++ code even when it appears random that is because anything like slight change in computer or compiler settings or upgrade of something or taking different computer may result with something lot less random,2018-08-31 13:28:12.957 UTC,141,52116524,i simulated a race-condition between threads in c++ by pthread.hwhen running it it gots error at a random loop iteration  can i use it as a random number generator?and why,0.028368794326241134,4,so,c++|race-condition,race-condition as a random number generator,1,race condition,0.47555750608444214
17845,no this is not possible while some weaknesses do exist in md5 and sha-1 they do not generally permit preimage attacks of this form -- most of the known weaknesses involve the construction of collision pairs  for a very detailed outline of a sha-1 exploit see   https://hashcat.net/p12/js-sha1exp_169.pdf   for such a small input sample you can build an in-memory rainbow table of all possible input values and their hashes in milliseconds  i doubt you would measure any significant difference using an exploit vs brute force    further for such a small input range collisions are extremely unlikely therefore there will almost certainly be no collision pairs,2013-01-30 17:49:33.823 UTC,271,14610451,i recall hearing that some weaknesses were discovered with sha-1 making it easier to find the plaintext input given the output hash i also know that md5 has been determined to be weak for some applications i m trying to create a program to demonstrate the different complexities of 2 approaches a brute force search to find the input and an exploitation of a weakness in sha-1 or md5 to find the input  the plaintext inputs will be of length &lt;4 and will consist of only a-z so brute force isn t impractical    my questions are is there a c/c++ implementation to reverse sha-1 by exploiting the weaknesses? is there a c/c++ implementation to reverse md5 by exploiting the weakness?  my current feeling is that any approach to exploitation of the weakness will not have enough of a difference in time-complexity to demonstrate a benefit for such a small sample size,0.055350553505535055,15,so,algorithm|c++|hash|md5|sha,reversing sha-1 or md5 given small input length without brute force,3,exploit|attacks|weakness,0.47407442331314087
15678,what you re asking for is a weak hashing algorithm strong hashing algorithms are hard weak ones are easy  here s one off the top of my head in pseudocode   here each hash byte is the   of ten data bytes very easy to solve yes? i m sure you can think of equally easy ones  a simple example is the tea-based hash function that was used in the xbox.this hash function was susceptible to a simple second preimage attack.the flaw was that it used the cipher tea in a davis-meyer construction.the attack exploits that tea has equivalent keys hash collisions canbe found by simple bit flipping  a book that describes this attack is hacking the xbox by andrew bunnie huang,2011-03-26 02:08:34.61 UTC,270,5439919,i am looking for an example of a simple hashing algorithm that can be shown to be vulnerable to multiple preimage attacks for a specified input length   for example if i know that the input data is exactly 160 bytes and generates a 16 byte hash then there exists some method of finding other inputs that are 160 bytes in length and generate an identical 16 byte hash without resorting to pure brute forcing   please note that i do not mean using something like md5 or sha1 i understand that these are designed to make this impractical this algorithm would be a textbook example of the flawed design of a hashing algorithm and how that flaw can be practically exploited i have tried google but much of what turns up is theoretical research on known secure algorithms,0.05555555555555555,15,so,algorithm|encryption|hash,example of simple flawed hashing algorithm vulnerable to preimaging attacks,5,flaws|attacks|exploit|weakness|vulnerability,0.4724436104297638
23539,an attacker would not be able to create a precomputed lookup table i.e a rainbow table of hashed values password + salt because it would require a large computation for each salt a simple dictionary attack is still very possible although much slower since it cannot be precomputed  source   rainbow tables can help you go from hashes to short sequences with limited character sets for example a rainbow table might support all alphanumeric sequences less than 10 characters long  a salt is much longer and uses a wider character set if you use a 128-bit random salt creating a rainbow table becomes physically intractable,2012-11-25 22:14:38.413 UTC,246,13556108,possible duplicate     how does password salt help against a rainbow table attack?      before you mention it i already read  this question  i didn t follow  here s my understand of salts/rainbow tables please correct me where i m wrong    user inputs raw        is concatenated with   to give   or       /  is hashed to value      enter hacker    hacker employs rainbow tables to reverse   into  /     hacker has in hands example the string     given   doesn t this simply mean that there are two options    password is   and salt is      password is   and salt is      so you see why i m confused my understand is clearly flawed because if this was how it worked obviously salts would be useless  or is my understanding correct and it s the whole iteration scheme that completely undoes this obvious weakness,0.02032520325203252,5,so,cryptography|rainbowtable|salt,why do salts stop rainbow tables,3,flaws|attacks|weakness,0.4720868766307831
2453,"a  brute force attack  means probing the complete keyspace on the algorithm  a  dictionary attack  means that you probe only passwords/keys from a dictionary which does not contain the complete keyspace  a brute force attack is primarily used against the encryption algorithm itself you can also use this against passwords but there you use dictionary attacks most time  a dictionary attack is primarily used against passwords encryption algorithms are seldom attacked with a dictionary attack because most times they use a random number as key is you use a weak prng then a dictionary attack could be feasible a typical dictionary for this attack would contain the most used passwords  a  rainbow table  is used to attack a hashed password in reverse that means i have a table with possible hashes and look up a matching password  to prevent attacks using rainbow tables each hashed password should be differently salted as then i would need a rainbow table for every hash and every salt   dictionary attack  the attacker tries a list of known or commonly used passwords thus s/he tries a list dictionary of passwords generally dictionary attacks succeed because many people have a tendency to choose passwords which are short and easy to remember like     etc   brute force attack  does not use a list of passwords instead it aims at trying all possible combinations in the password space  for example let s assume the password is a four-digit pin code   in this case the dictionary attack will try to use a list of common used pin codes such as       and so on see the list of  most common pin codes   conversely a brute force attack will try all possible pin codes which means it will try 10^4 = 10000 times until it finds the right pin code with probability 100% 4 because we have 4 digits and 10 because each digit can be any value between 0 and 9  in theory brute force attack will discover the password however it could take very long to try all possible combinations usually an attacker starts with the dictionary attack and if it fails he moves to the brute force attack   rainbow table  not directly linked to brute force or dictionary attack it is  very important  to not store the password in db or file in plain text the passwords are hashed using secure hash functions like   and this hash is stored a rainbow table attack is a method that aims at guessing the plain text of the password from the hashed value to thwart the attack one adds unique values - salts while hashing   similarities both a dictionary and brute force attack are guessing attacks they are not directly looking for a flaw or bypass either can be an offline attack or an online attack   an  online attack  tries automated routines providing input to a legitimate system they are not looking to create an exploit in functionality but to  abuse expected functionality    an  offline attack  attempts to emulate the encryption/hashing and requires a known output of that process i.e you don t attack the system you already have the hashed/encrypted password    brute force  attack   definition  attempts to determine a secret by trying  every possible combination     qualities    the number of attempts is limited by the maximum length and the number of characters to try per position or byte if considering unicode passwords  the time to complete is greater but there is greater coverage of likely cleartext value all possibilities only if set to the maximum length and every possible character is considered in every position    physical world example  given a combination lock which requires three numbers to be taken in sequence you try every possible combination - e.g first 1-2-3 then 1-2-4  note a brute force attack may not necessarily try all options in sequential order  an advanced brute force attack may make certain assumptions e.g complexity rules require uppercase first character more likely to be upper than lower case    dictionary  attack   definition  typically a guessing attack which uses precompiled list of options rather than trying every option only try complete options which are likely to work   qualities    the dictionary or possible combinations is based upon some likely values and tends to exclude remote possibilities it may be based on knowing key information about a particular target family member names birthday etc. the dictionary may be based on patterns seen across a large number of users and known passwods e.g whats the most globally likely answers the dictionary is more likely to include real words than random strings of characters  the execution time of dictionary attack is reduced because the number of combinations is restricted to those on the dictionary list  there is less coverage and a particularly good password may not be on the list and will therefore be missed    real world examples    access to a secret club requires knowing the owner s name you guess rob or jake rather than computer  given the same lock example above you try a combinations equating to the birthday of the lock owner or the lock owner s friends and family      trade off  the main trade off between the two attacks is coverage versus time to complete if you have a reasonable thought about what the password will be you can skip unlikely answers and get a response in a faster amount of time this is important because passwords are often subject to change and because as password length increases the time to guess every possibility grows really really fast  hybrids  there are of course attacks which leverage both techniques in the interest of balancing the tradeoff for example if the attacker believes a user is likely to form a password by concatenating a dictionary word and then adding a number which he increments each time he is required to change his password then the guesses being executed may combine the word list and then append numbers e.g mypassword2014 and then mypassword2015 hybrids may also combine words in a brute force manner consider a requirement for a user to change his password every 90 days he may form passwords like mypasswordsummer and then mypasswordfall the attacker then builds a hybrid attack which will take a dictionary word and then append other dictionary terms potentially the same of different dictionaries to make guesses   rainbow table versus dictionary/brute force  a rainbow table is generally an offline only attack in a brute force attack or dictionary attack you need to spend time either sending your guess to the real system to running through the algorithm offline given a slow hashing or encryption algorithm this wastes time also the work being done cannot be reused  a rainbow table is precomputed listing you actually work backwards from the hashed/encrypted text the attacker will run through the algorithm to get every possible output given every possible input the list of inputs may be brute force dictionary or hybrid based on the list of outputs the attacker now has a reuseable table mapping inputs to known outputs  with the precomputed table a simple lookup is now possible given the encrypted/hashed version of the password if you can find the victim s encrypted/hashed version you can easily return the real plaintext password rainbow tables are used to reduce redundant work there is a trade off with doing the work up front and storing the tables for example if you were just doing a brute force or a dictionary attack you can stop as soon as you find your answer however the rainbow table must be fully calculated   if you were to run a a rainbow table attack and the 5th entry out of 500 million entries was your match then all of the effort and time used to create the other 499,999,995 passwords may be considered wasted however if you are looking to break multiple passwords to reuse the table over multiple attacks the time savings can add up",2014-09-19 06:10:37,1356,67712,can someone explain the major differences between a brute force attack and a dictionary attack does the term   has any relation with these,0.04424778761061947,60,sse,attacks|brute-force|dictionary|hash|passwords,what are the differences between dictionary attack and brute force attack,6,flaws|bypass|attacks|exploit|weakness|plaintext password,0.4718918204307556
49244,,2018-04-15 12:21:50.01 UTC,158,49841845,"i m trying to write in python3 to determine whether a number is a prime number or not  i was specifically demanded to only use the following method  divide the input with all the positive prime number smaller than it s square root  for example if the given number is 33 and then i would have to divide 33 with [2,3,5] smaller than 5.xx the square root of 33  meanwhile in the process of finding [2,3,5] i can not use any method other than the demanded one  so my code are as follow   and the code can not properly run when the input number exceed 7 figures it just stop responding  apparently somewhere in my code there is an infinite loop that i can t figure out  i d be very grateful if someone can help me out with this one",0.012658227848101266,2,so,infinite-loop|primes|python-3.x|square-root|time-limiting,python3 exceed time limit when finding prime number using square root method,1,infinite loop,0.4708477258682251
53852,,2019-07-24 15:07:39.78 UTC,120,57186046,in penetration-testing crunch command generates wordlist list of passwords to crack wpa2 key here is the command   the above command will generate wordlist of passwords min and max 8 cheracters and all possible combinations of abc123   i want to limit it in a way that no letter should be repeat more then twice consecutively and once it repeated twice it may come only single again      aaa111aa   is incorrect because there are aaa and 111 and aa again     jabbaar2   is correct because there is aa once and the single a there are no aaa and aa is only once and not repeated again  is there a way to do that?  any help will be appreciated,0.016666666666666666,2,so,penetration-testing,limit wordlist in crunch command,1,penetration test,0.4700973629951477
20931,"definitely a question for information security stack exchange as thriggle pointed out in comments but regardless i ll pitch in some thoughts  if you interleave the letters of words you face a few pros and a few cons  pros   not susceptible to standard dictionary attacks  easier to remember  ability to create longer passwords just by remembering a few words with many letters   cons   if anyone knows this is your password scheme then you become vulnerable because a dictionary attack can again be applied with custom edits to target passwords of that nature  while you are less susceptible to standard dictionary attacks and you can make longer passwords these passwords could be crackable based on the probable occurrences of letters one of the more interesting weaknesses i see in this proposal.  what about numbers and punctuation? highly important for preventing plain dictionary attacks and obviously english words don t have numbers and special symbols in the words themselves.  if you were setting this as your personal password technique you would have to decide on some standards to dictate what you are going to do about words of different lengths just three 5-letter words for every password? there s a finite number of 5-letter words.  no capital letters which is a correctable issue but again you would have to decide on some more standards.   as i find the probability of letters in the password to be the most interesting negative effect of this system i ll dive into that one a little further  if i know your password uses three 5-letter words interleaved with one another then i would start out with a dictionary of all five-letter words in the english language and begin with words that start with one of the following   most frequent first letters in english words from that sublist i would choose words that end with   most frequent last letters in english words and then just combine and interleave after all for each series of 3 words there are only 3 possible options  in the worst case scenario i find out you used       however even if you did use the last possible word combination i will still find your password in a relatively low timeframe compared to using even 15 random base64 characters because there are only 323,727,260,476 323.7 billion combinations of 5-letter english words  using a rather complete 12,478 word list of 5-letter english words  on the other hand had you just used 15 random base64 characters there would have been 1.2 octillion possible combinations      now all of that said that s assuming an attack where i know that you re using three 5-letter english words if i don t know anything and i still try a dictionary followed by the combinations of 15 base64 characters then the password is perfectly fine because i will never be able to bruteforce it however the key weakness here is that if i know anything your password pattern then a significant amount of security is immediately lost because the pattern is not overly complex so this password scheme is a case of security through obscurity  which is generally discouraged",2017-05-03 20:58:55.933 UTC,571,43769912,i was just thinking about passwords is interleaving the letters of several words a good method of creating a strong password? ie   so   it seems like it would be strong against a dictionary attack and brute forcing is there a method that this is weak against,0.017513134851138354,10,so,passwords,interleaving words as a password,3,attacks|weakness|vulnerability,0.46749159693717957
32957,,2019-06-20 09:45:06.377 UTC,234,56683215,"i need to unsalt some salted hash it s in md5 i can t run any attacks because they ve include spaces and words that cannot be found in a diuctionary as well as abbreviations that i will not be able to search for i tried to used a dictionary compiled of over 450,000 of the most common words to no avail before realising that you can only run attacks on single word/ numbered passwords is it possible to unsalt hash that has already been salted and is it possible to decrpt md5 that is not stored on a database also i can t be certain if it contained salt but i m guessing because the owner of the original text keeps saying that they ve got a pairing key   i tried to run a variety of attacks on ssh and i then did a dictionary attack of course this didn t work as i realised that the original text contains more than one word and words that aren t even in the dictionary  no code..  dictionary and ssh gives no results i set it to show no results found and that s what i got every time",0.021367521367521368,5,so,caesar-cipher|dictionary|dictionary-attack|encryption|sha1,do you need to retreive a key for a password hashed in md5 - is it possible to decrypted a password in md5 when it s not already stored ona database,2,attacks|dictionary attack,0.46730682253837585
27314,you could use key stretching iterate the hash say a million times after salting.then store hash value salt and iteration count.then an attacker could check a million times fewer passwords per second than when you hash once but a very short password will still fall note that you yourself to verify the legitimate password need to do the same operation suppose you accept a 1 second time for yourself to check then the attacker can also check passwords at 1 second per password on a similar machine and more if he used more or faster machines! and 1 second per password can still be enough to check for weak and short password standard dictionaries etc there really is no defending against it only making it harder  re-encryption doesn t help with your problem  the only thing you can do is create a multi part hash and hope the attacker doesn t get all of them i usually use a two part salt  one part is a random per user value stored in the database alongside the password  the other part is a per application salt you can store it in the application config or in a special password store the os offers  the advantage of that split is that it s not enough if the attacker just gains access to the database but he needs to get access to wherever your application salt is stored so for example a simple sql injection stealing your database isn t enough if the attacker can execute code it probably won t help at all   and you should use some method to slow down hashing typical hash functions are fast so brute-force is fast too but if you iterate the hash-function a million times it still doesn t slow down a valid login much but slows down brute-force a lot  you can achieve that using a  password based key derivation function  such as pbkdf2  there are two problems with your main assumptions the first one is about the problem of storing the salt you already do for the salted password solution with your new approach salt would change over time and that s it so you could have used this method and the only extra cost would be the re-calculation of the hash value at every login when you actually have the password string itself  the second problem is the more important one a re-hashing will not change anything as soon as your attacker gets hold of one salted hash value it will be enough to mount a dictionary attack the fact that you change your salts and the hash in your database will not make it any more difficult so there is no need to recalculate a hash after the first one is created,2011-02-27 13:30:46.413 UTC,639,5133466,i am always looking for ways to secure my user s passwords i am currently using some combination of hashing algorithm with random salt  the main thing in this problem is when my user set a very very weak password no matter how hard my mixed-up hashing algorithm and how long my salt is i think it can be cracked in less than 1 year  i ve been thinking for a new way i ve made a script that will re-encrypt the password every time the user sign-out by adding a random salt on the old hashed password then encrypt it again so every time the user come back the encrypted password is different get it?  but the main problem on this idea is i must store the new salt every time the user sign-out imagine my code will look like if the user is sign-in and sign-out everyday?  any idea?  oh i have an idea how about regenerate new encrypted password every year,0.017214397496087636,11,so,encryption|md5|passwords|sha256|sha512,how to secure encrypted password even the weakest one,4,attacks|weakness|weak password|sql injection,0.46565642952919006
7062,an attacker that knows the password and the hash but not the pepper is in the same position as the attacker that knows the salt and hash but not the password  putting in another way    on the classic leaked database attack the attacker knows the salt and the hash not the password on your case he knows the password and the hash looks different but it s exact the same nothing different at all  difficulty? it depends on the length of the pepper if it s a 128 byte random string is as difficult as bruteforcing a 128 byte random password  there s three potential ways the attacker can determine the pepper    implementation bugs in the oracle  if the oracle has a bug such that it leaks information about the pepper buffer overflow etc the attacker might be able to determine the pepper    low entropy pepper  if the pepper has very low entropy of say 32 bits it s trivial to give the oracle a known password then on your own computer simply do sha256known_password+n for all 32 bits of n until you get back the value the oracle gave you  if the pepper is sufficiently large this is infeasible    a newly found design flaw in sha256 that leaks information about what s hashed  there are no known design flaws like this as of june 2018    the easiest way is through brute force which depends on the size of the pepper otherwise it would take a cryptographic attack against sha-256 itself no such attacks are currently known     is there a way an attacker can guess the pepper? how would the attacker call the oracle to get that pepper?   if the unknown input is sufficiently small regular exhaustive search may be able to find it otherwise it would take a severe cryptographic vulnerability in sha-256 itself     do the same attack applies if the oracle uses a hash_hmac sha256  password pepper method instead of simple sha256password + pepper?   an hmac mitigates length extension attacks which are of no use to the attacker here there are some attacks which an hmac can make harder such as collision attacks md5 is vulnerable to collisions with 2 18  difficulty but hmac-md5 is not with around the expected 2 64  difficulty for example these attacks are not relevant here either in theory hmac may make some attacks harder but assuming sha-256 is an ideal prf an hmac provides no benefit     do this applies for sha256pepper + password instead of sha256password + pepper?   the former is more secure in theory this is because you could calculate what  ha || b  would be even if you are given only  ha  by exploiting a length extension attack as such  ha || b  is provably no less secure than  ha  when done the other way around this may be less secure in theory this is because  hb || a  is equivalent to  h a  where  h   is sha-256 with  hb  as the iv a preimage that allows the attacker-controlled  b  to resolve to a weak iv may make  h a  weaker,2018-06-14 13:37:26,716,187768,saying we have an oracle that an attacker can use as many time as they want the attacker can send a non-empty   to this oracle the oracle hashes the password using   and sends this to the attacker    the pepper value never changes  it s a bad constant salt  is there a way an attacker can guess the pepper? how would the attacker call the oracle to get that pepper?  do the same attack applies if the oracle uses a   method instead of simple  ? do this applies for   instead of  ?  i ve seen the question  is it possible to get the salt if i have the hash and original password?  but there we have one more condition attacker can get as many   couple as they want to recover the constant pepper so this condition might change a lot?  my guess is that one can do retrieve the pepper but i m not sure how it would be done the process to retrieve it is not that much important but i want to get a proof that such process exists and is do-able,0.051675977653631286,37,sse,hash|password-cracking|salt,can one find the value of a constant salt if they can set the password and get the hash,7,leak|flaws|attacks|exploit|weakness|vulnerability|buffer overflow,0.465351939201355
3632,i can see where you re coming from but the answer is basically that s not what the salt is for  if i m trying to discover a  particular user  s password the salt doesn t help much if it s right beside the hash  however it s not  expected  to be useful in that situation - cracking an individual user s password is difficult enough already  the point of the salt is that it s something to add to each password before hashing such that each password is unique - if two users both have   as their password the hashes need to be different  say an attacker is fishing through a database or a whole range of databases to find everyone whose password is    without a salt they can do the hash once and search for the hash  without a salt if an attacker spots two or more identical hashes in the database it indicates that this is a weak password since multiple users have chosen the same one and can concentrate on cracking this first  but the salt is a user-specific prefix/suffix that is added to the password before hashing so even with a public salt an attacker has to re-calculate the hash for every user - so checking one password against 10000 users isn t any more efficient than checking 10000 passwords against one user which is the point,2015-09-22 16:36:30,361,100898,understand the need to protection credentials with hashes that are expensive and to use cryptographically random salts   what i would like to understand is why you would store the salt along side the hash in the database does this not defeat the point of having one?  take a sql injection vulnerability in which i can dump the data in the user table if i have access to the hash as well as the salt does it not make my brute force attempt easier as apposed to not knowing the salt?   if storing the salt with the hash - does this not defeat the point of protection against rainbow attacks? if an attacker has access to the database,0.027700831024930747,10,sse,appsec|credentials|hash,why store a salt along side the hashed password,5,attacks|protection|sql injection|vulnerability|weak password,0.4648614525794983
13646,if you put a compound index on   in your   table and change the query from   to   you will go a long way towards making queries which match and queries which don t take roughly the same amount of time if your hashes are all the same length that will help too   if all these queries take the same amount of time it will make timing attacks much harder obviously you can do more to defeat timing attacks by sleeping a pseudorandom amount of time on each query try this   it ll add a random time between 0 and 0.2 secs to each query that randomness will dominate the near-constant time to carry out the indexed   clause  yes the string comparison and/or index lookup may in principle leak how many identical leading bytes the password hash stored in the database and the one computed from the entered password share  in principle an attacker could use this to iteratively learn a prefix of the password hash byte by byte first they find a hash that shares its first byte with the hash in the database then one that shares its first  two  bytes and so on  no this will almost certainly not matter  why?  well for a number of reasons    a timing attack  may  allow the attacker to learn a part of the user s password hash  a well-designed password hashing scheme using a salt and  key stretching  however should remain secure assuming of course that the passwords themselves are not easily guessable even if the attacker knows the  entire  password hash  thus  even if the timing attack succeeds the passwords themselves will be safe     to carry out the attack the attacker must submit passwords whose hash value they know  the hash value depends on the salt  thus  unless the attacker somehow already knows the salt this attack is not possible   it is true that in most security analyses of password hashing schemes the salt is assumed to be public information  however this is only because such analyses assume the worst-case scenario mentioned above where the attacker has already obtained a copy of the entire user database salts and hashes and all  if the attacker doesn t yet know the hash there s no reason to assume they would know the salt.    even if the attacker knows the salt in order to carry out the iterative attack described above they ll need to generate passwords that hash to a value with a desired prefix  for any secure hash function the only practical way to do this is by trial an error which means that the time needed to do so scales exponentially with the length of the prefix  what this means in practice is that in order to extract sufficiently many bits of the hash to be able to carry out an offline brute force attack against it which need not be all of them just more than the effective amount of entropy in the password  the attacker needs to carry out about as much computation as required to crack the password itself   for a well designed password hashing scheme and a securely chosen password this is not feasible    what the iterative attack  can  give the attacker in principle is the ability to do most of the brute force computation locally at their end while only submitting a fairly small number of passwords to your system  however even this only holds if they receive detailed and reliable timing information back from  each  password submitted  in practice  real timing attacks are extremely inefficient  and require many often thousands or millions queries to yield  any  useful information at all  this will very likely cancel out any potential performance advantage that the timing attack could provide for the attacker  this point is amplified you use a proper key-stretching password hashing scheme since such schemes are deliberately designed to be slow  thus the string comparison in the database will likely take a negligible amount of time compared to hashing the password in the first place and any timing variations caused by it will thus be lost in the noise,2014-11-19 22:29:23.487 UTC,816,27028284,i m deploying a classic hashing protection for user passwords the submitted password on login is salted hashed and then compared to the already stored hash in the database  but instead of using a php function call to compare the now hashed user input and the stored hash the comparison is done in database - more precisely using a   clause note the salt is already known for various reasons at the time this comparison begins but the password is not  since usernames are unique the following query effectively tells if the username + password pair is a match or not    is this approach vulnerable to timing attacks?     edit  sql injection is not a concern it is taken care of,0.03553921568627451,29,so,hash|login|mysql|php|security,is comparing strings in mysql vulnerable to timing attacks,5,leak|attacks|protection|vulnerability|sql injection,0.46486136317253113
12123,it is actually called  pepper  the salt is stored in db but pepper is stored somewhere else then db  the wikipedia states as     a pepper performs a comparable role to a salt but while a salt is not secret merely unique and can be stored alongside the hashed output a pepper is secret and must  not be stored with the output  the hash and salt are usually stored in a database  but a pepper must be stored separately  e.g in a configuration file to prevent it from being obtained by the attacker in case of a database breach    when the database hacked the attacker cannot access the pepper as a result password search would be impossible even for weak passwords  in short yes recommended  however bcrypt is old one should use argon2 as the winner of the  password hashing competition,2018-12-21 17:38:40.8 UTC,257,53888820,i found a blog post about bcrypt and i am not sure what is the benefit ob adding the hard-coded salt ^y8~jj to the password?  the  hashtostoreindatabase  containing the salt and the crypted password but not the hard-coded salt  y8~jj  so if somebody steal the database it s   useless for the hacker to generate an own rainbowtable with the salt containing in the database and the hashed password because they never get the hard-coded salt  y8~jj   i knew that is already safety to save the salt and passwordhash togheter because a rainbowtable is expencive to generate  is this using of bcrypt recommended?  quote from  https://www.codeproject.com/articles/475262/useplusbcryptplustoplushashplusyourpluspasswords,0.027237354085603113,7,so,bcrypt|cryptography|salt,bcrypt generated + hard-coded salt is this more safety,3,attacks|hard coded|weak password,0.462100088596344
6696,specialized password hashing functions like bcrypt are designed to protect secret values that are relatively easy to guess  and that to put it more technically means secret values that the following strategy can guess with good practical chances of success   guess likelier values ahead of comparatively unlikely ones  try a very high volume of guesses at very large speeds   specialized password hashing functions take it for granted that #1 is just a fact of life for passwords so they try to thwart the attacker on point #2 by slowing them down  but if you re generating byte strings uniformly at random from a strong cryptographic generator then #1 is no longer a problem so you don t need a specialized password hashing function  the only defense you need is to make the random byte strings long enough  64 bytes is 512 bits way more than you need and in fact if you re passing it through sha-256 that already pegs your security level at 256 bits so you gain nothing for using more than 32 bytes  but actually a 16 byte string 128 bits should be enough for example the cipher that my browser is using to connect to this site uses a 128-bit key which is an absolutely commonplace security level  you re confusing cryptographic hashes with password hashes a cryptographic hash takes an input and gives a random looking output a  password hash  takes a password and a salt and  very slowly  gives a random looking output  what you are worried about here is  preimage attacks  you want to ensure that knowing a hash   the attacker cannot find a value   such that   80 bits of complexity is still generally considered good though not great and possibly vulnerable to a well funded attacker such as a nation-state what this means is that you want a hash where the best preimage attack has a complexity of over 80 bits  analysis  let s start with   since it s something of a poster child for outdated crypto the best known preimage attack on   is  116.9 bits  well above 80 but you want to protect 1  trillion  secrets which lets the attacker do some optimizations  let s assume the attacker is able to use a hash table and many terabytes of ram to check if a hash matches any of the 1 trillion in   with 1 trillion secrets this reduces the complexity to break a single secret by log 2 10 12  ≈ 40 bits which brings   down to around 76 bits still not terrible but worrying if you re facing a nation-state or want to keep these secret for many years  but you re doing the right thing and using   the best preimage attack i could find on   was for reduced steps and still had complexity over 250 bits so you re looking at over 210 bits of complexity there in this case an attacker will be running into  some issues with physics,2018-02-08 20:50:07,637,179419,i read from a lot of sources that   is not secure because it is fast most links that i read suggested   however i just want to know that if data is  64 cryptographically random byte will attacker be able to find original string somehow?   scenerio     is 64 cryptographically random byte data  user download   we store hashed version of it he will upload this data to site when he wants to get access to some information work process can not be modified it is stable system we just need to secure it  system has 1 trillion   and all of them should be protected by some hash algorithm  is it possible that attacker will get even one of the   if he got access to all hashes,0.02511773940345369,16,sse,cryptography|encryption|hash|php|sha,is it secure to use sha256 to hash 64 cryptographically random byte data with 12 byte salt,3,attacks|protection|vulnerability,0.46152523159980774
332,no hashing passwords using only one round of sha-256 for any web application is not secure!  sha-256 is a hashing algorithm primarily designed for data integrity verification this means it was optimized for speed being optimized for speed makes it vulnerable to bruteforce and dictionary attacks which consist of guessing the password many times  suppose that your login database was leaked something that happens even to the largest webapps like banks or large corporations your password hashes would be exposed to an adversary what would they do to get to those juicy passwords? they would constantly guess passwords until they found the right one bitcoin mining uses a similar mechanism of hash guessing for mining and there are sha256 asics that can perform terahashes per second would you feel comfortable with an attacker being able to guess your password trillions of times per second?  a more secure approach would be to use a modern kdf like scrypt or argon2 modern kdfs are designed to be memory heavy which limits hashing to the speed of ram and makes it very difficult to build efficient asics for because kdfs are slow it is best to execute the kdf on the client side then send the kdf hash to the server and hash the kdf hash one last time on the server side with a fast algorithm like sha-256 this would allow you to offload the processing to the clients without a hashes from a leaked database being usable as passwords  note javascript key derivation can be slow if you want client-side key derivation to be faster you could potentially use webassembly to accelerate it try not to reduce parameters too much it will make the algorithm easier to bruteforce  furthermore storing a salt in a git repo sounds like you re planning to have one salt for the entire web application this is a bad idea as it means that an attacker can use one iteration of your hashing function/kdf to guess a single password for all your database entries it s best to generate a random salt for each password and store it with the password in the database  while we re here you might want to protect against timing attacks as well when comparing the hashes using a timeable comparison function like a simple == would allow attackers to bruteforce a single character in the hash at a time to log in to the system using a constant-time comparison function like   would protect against this type of attack  exposing your source code to the world should not make it insecure if you are using modern security practices please mind the best security practices listed above along with others when making your web application,2020-01-20 06:04:54,577,59817759,if i m creating a password for something that is open source python - flask and i m hashing passwords is it secure to just hash them like i have below? or should i create a config file on the server that isn t in the git repo that stores a salt?  is it less safe when people can see exactly how someone is hashing a password?  if someone was able to get their hands onto the database and knew the exact method that was used to hash a password like the code below would they be able to reverse it easily? or is there something that i can add to make that difficult,0.019064124783362217,11,so,flask|python|python-3.x,storing and hashing passwords flask - python,4,leak|attacks|protection|vulnerability,0.45452359318733215
24656,"maybe you should have a look at this small  article  there are several things wrong with your approach   a salt does not protect against a dictionary attack it protects against rainbow-tables if correctly used  use a unique salt for each password the salt should be a random value not derrived from known information it has to be stored with the password  do  not use md5  for hashing passwords md5 is considered broken and it is ways too fast to hash passwords with an off-the-shelf gpu you are able to calculate 8 giga md5-hashes per second  in 2012  that makes it possible to brute-force a whole english dictionary with about 500000 words in less than 0.1 milliseconds!  use  bcrypt  for hashing passwords it is recommended to use a well established library like  phpass  and if you want to understand how it can be implemented you can read the article above   if you want to add a secret to your hash function like a hidden key or a hidden function you can add a  pepper  to the password the pepper should  not  be stored in the database and should remain secret the pepper can protect against dictionary attacks as long as the attacker has only access to your password-hashes sql-injection but not to the server with the secret  your system will be no more secure - you end up with several single salt databases instead of one in principle it may be even less secure since you helpfully provide the attacker with 7 hashes to the same string to choose from and he only needs to guess one these multiple hashes of the same plaintext may also lead to implications to cryptographic strength of the encryption used for passwords not sure on that one and it will depend on the algorithm used  i do not believe multiple hashes are going to help you in this scenario primarily because when someone compromises your database they will notice that you have 7 different salts to go against and may make an educated guess that they are based on days of the week  there is nothing fundamentally wrong with md5 as so many people like to jump on that bandwagon  the types of people that say md5 is a broken hash have a fundamental misunderstanding between a hash function and a cryptographic hash function i would recommend ignoring them  in the event you need a cryptographic hash function use sha-2 or something from that family or greater    you will need to salt the user input as you know a random value is generally recommended,but it can also be a value you store in a separate application space outside of the database you just have to protect that information as well    i highly recommend making the password hashing function take several thousand iterations for any input as this will slow down the automated process of matching hashes on the database  if your users use easy to guess passwords dictionary attacks will beat you every day cant protect against stupidity",2012-10-05 19:45:43.48 UTC,631,12753062,if you use a salt before hashing a password - it will make the hash more secure it makes sense because rainbow table attacks become much more difficult impossible?  what if you use multiple salts? for example - you check if the day is monday or the month the hour etc or some combination then you have a database which stores the fields userid hash1 hash2 hash3...   would this make the information any more or less secure?  example  1 user registers with password  pass .2 system php in this example stores values md5$password.$this_day for each day 7 passwords into table password column hash_monday hash_tuesday etc.3 user logs in and script checks password where  hash_ .$this_day matches what is entered,0.022187004754358162,14,so,hash|passwords|salt|security,multiple salts to protect passwords,4,attacks|protection|sql injection|hidden functionality,0.45418885350227356
42172,those are two strings of hex digits 32 characters in length that s the length of an md5 hash so it might be the result of md5 using md5 is not recommended for password hashing but some people have used it in the past  but i agree with the answer from @capibar -- hashes are one-way not reversible  hashes can t be decrypted first it s one-way crypting.and this is not a md5 so long maybe sha256,2019-05-22 05:54:46.26 UTC,166,56250154,these are outputs for data in column password from a table    how do i decrypt this given that i ve ran it through different hash decrypting programs and keep getting an error?  what kind of hash is this? md5 etc...?  i ve used python /usr/bin/findmyhash and different online hash decrypting sites  i tried hashcat but totally unfamiliar with it and would need to study it  qlmap -u   https://fake-site  informationschema fakesite -t tq_user -c password --dump,0.018072289156626505,3,so,mysql|sql-injection|sqlmap,how do i decrypt the following hash from a sqlmap output,2,sqlmap|sql injection,0.45407673716545105
34736,,2015-12-02 13:37:24.72 UTC,89,34044134,i m programming some audio plugin inside an environment called kontakt.i see that cpu usage is limited if i try some fancy task   for example usually a infinite loop should eat all cpu   instead i see that cpu for that task is limited to 40% and i can use the rest of the tools as well  my question is why? who limits the cpu usage of that task? is kontakt that will limit the amount? just curious.,0.02247191011235955,2,so,cpu|infinite-loop|scheduled-tasks|task,who limit the cpu usage inside a task,1,infinite loop,0.45261260867118835
26505,it depends on whether the attacker tries to bypass your security or if he tries to find the password  if the attacker relies on finding a  collision  with a given hash of your database thanks to the cryptographic weakness of the hashing algorithm  the salt will not have any effect  i have a bunch of bits and i d like to find some input to the xxx hash algorithm which give me the same bunch of bits in the output  if he is trying to bruteforce the password by trying each of the possible combinations then any information he could gather on the original password will help    length  composition alpha-numerics characters special symbols ...  salt  ..   by creating your own salt algorithm you are in fact trying to do security through obfuscation  which will indeed  limit  anyone who doesn t know your algorithm from bruteforcing the password but it doesn t strengthen the hashing algorithm  first question  the position of the salt has no impact on the security of a particular hash a good hash function has perfect entropy in that for every input bit that changes each output bit has a 50% chance of changing  any possible security benefit from a certain order would be entirely from the relative slowness of the algorithm used to concatenate the salt with the prospective password e.g if   is slower than   use the former however most programming languages don t have that kind of performance  issue   second question  if the salt is stored explicitly in the database the attacker will know the salt and be able to launch a brute-force hashing attack if the salt is unknown a brute-force password attack can still be used though this can easily be rendered ineffective by inserting delays between attempts also the attacker may be able to reverse engineer the program and retrieve the hash field  as for the security of the hash if the user has the same email and password two different places this negates one of the benefits of a random salt in that the same hash will be visible both places  personally i think the best method for hashing is to use   this has the benefit of being simple and no less secure than most other methods of security  when you work on password security always assume that if the attacker has access to your database then he has access to your code  so when it comes to salting just generate a hash of a random value say   and use it to salt the password before hashing and store it in the database in a column next to the password  when it comes to adding the hash to the password my personal opinion is that it doesn t really matter if you put it first or last or in the middle  do you want security? then use a slow hashing algorithm i highly recommend  phpass  as it uses   blowfish-based as the default hashing algorithm,2012-06-20 12:16:50.24 UTC,735,11119373,before i start i know the md5 is compromised collision attack and speed of hashing and shouldn t be used to hash passwords but just for the sake of it bear with me  my questions are:how does the salt position when hashing with md5 affects the quality or the strength of the hash?  say i have the the following piece of code which hashes a userspassword using parts of his email address as salt   is that code going to slow down a brute force / dictionary / rainbow tableattack than say   is it worth trying to salt a password like in the first code or it yeldsthe same result as the second code? are there any benefits from that?does the first code delay cracking a list of passwords hashed that way thanthe second way of doing it?  which leads me to a second question is it secure storing the salt in the database than obtaining a saltfrom a user id say the email address ?the way i see it once an attacker has obtained a copy of the databasewhich also cantains the salts it makes his life a little easyer trying to crack the hashes but if the salts are not stored the attacker would also need the algorithm that creates the salts please correct me if i m wrong  i hope i made my self clear thanks for any answers in advance,0.017687074829931974,13,so,hash|md5|php|salt,md5 password hashing and salt position,3,bypass|attacks|weakness,0.45123884081840515
32249,there s no problem to keep the salt on database the salt is only there to make sure an attacker don t use some table that has a lot of calculated hashs and its corresponding passwords to find out the passwords of your compromised database with salt the only option for attackers is brute force  so to make it harder for attackers to brute force and find out the passwords from hashs i suggest a long and secure random salt for each user  good explanation can be found on  why do we use the &quot;salt&quot to secure our passwords?   you have to keep the salt on the server side of the code   when you authenticate a user you send the password to the server the server appends the salt and then hashes the password the salt should be stored somewhere in the serverside code or in the database  you should  never  send the salt to the client side of the application,2014-10-22 11:04:52.053 UTC,332,26506032,i am developing a web application now from security perspective salted hashing is required for the password while it is sent from client to server.      now my problem is if i  randomly generate a salt append it to the password and hash the combination how would this password be verified  as the salt generated is at random hash of the salt+password combination would be different every time     if  i send the same salt generated along with user credentials to the server this will expose the salt  exposed salt will create similar trouble as person trying to crack the password can append the exposed salt with different passwords to get the hash and match it i have checked numerous websites and question on stack overflow but nothing matched my need exactly  there are tools that can read the memory of browser and steal passwords entered therefore salted hashing is required at client side also,0.015060240963855422,5,so,javascript|owasp|penetration-testing|saltedhash|sha256,how do i verify a password which is hashed using a random salt,3,owasp|attacks|penetration test,0.45038512349128723
27394,in some situations the inner hash  may  make the task more difficult for the attacker but not necessarily for instance if you use md5 for both hash functions then a collision for the inner hash would also imply a collision for the outer hash given the iterated structure of md5  so adding the inner hash function will not necessarily  increase  resistance to collisions and preimages on a pure theoretical point of view it may actually  decrease  resistance although this is quite improbable especially if the hash functions are secure but if the functions are secure then the construction is pointless on a more practical plane this double hashing increases computational work load more cpu and possibly bigger code -- hence more l1 cache usage -- if the two functions are not the same so my advice would be not to do that instead use a single believed secure hash function such as sha-256 the hash function will not be the biggest weakness in your application or more precisely if the hash function  is  the biggest weakness in your application then you are a programming god and/or  donald knuth   as an illustration  ssl/tls  uses md5  and  sha-1 simultaneously as an attempt to resist weaknesses on either of the two functions but the newer  tls 1.2  version switches to sha-256 only  if the outer hash algorithm is broken the inner hash could help but you have to consider how likely that scenario is with a well respected algorithm  if the outer hash is so small that a brute force attack is feasible the inner hash wouldn t help much at all instead of finding a message with the same hash the attacker would have to find a message plus inner hash with the same outer hash which pretty much amounts to the same thing  so make the hash as large as you can and concentrate on making sure there are no back doors in the rest of your system 64 bits is probably just about ok unless you are anticipating a government or major corporation taking an interest in breaking your hash  your proposal has something reminds me of  hmac  this is a construction that allows one to create message authentication codes keyed hashes if you wish  however i don t see the point of using 2  hash functions pick a one of the standard ones that have resisted attacks so far and go with it if you assume one of them will get broken why use it in the first place? sha-2 or any of the final candidates for the sha3 competition should be fine if you want strong security more info here  http://ehash.iaik.tugraz.at/wiki/the_sha-3_zoo,2011-04-07 20:51:36.44 UTC,545,5587496,a message digest is being used to verify that a message is the intended one  by how much would bundling a hash digest with contents to form the message increase the difficulty of collision and preimage attacks against the message?  for example to encode   to verify message using message_hash     and   could be completely different types of hash functions  my reasoning for this was that any attack would have to break both hash functions - faking the outer digest would require constructing a message with a valid inner hash,0.01834862385321101,10,so,cryptography|hash,does an internal hash digest in a message strengthen an outer digest,3,attacks|weakness|back door,0.4494629502296448
14262,this is not possible using just an algorithm you are better off splitting off your system into parts with specific roles to protect your data  scrypt bcrypt and pbkdf2 are all deterministic password based key derivation functions pbkdfs as long as the salt stays the same they will reproduce the same result for the same input parts of the salt may be a server side secret if the salt gets exposed those functions will however not help much if the password is weak  it is not easy to give a good answer without more context  a server side secret can only protect the ids? for storing them server side database even then it only protects the ids as long as the secret is not known the problem of every two-way encryption  a one-way hash on the other side would protect the ids even when the code and the database was stolen a bcrypt/pbkdf2 hash would slow down brute-forcing even with a static salt though the static salt would allow to build a single rainbow-table to get all hashes at once  using bcrypt and afterwards encrypt the hash with a server side secret is possibly your best bet though it is difficult to say without knowing more of the scenario,2014-04-14 16:35:12.827 UTC,379,23065439,"i searching for a strong one way hashing function which encrypts really weak passwords 10^9 combinations the crypt function also must fulfill some requirements     alwas same hash from same clear text so scrypt/bcrypt and public/private key methods are not possible or may i be wrong here?    no shared secret as in aes as the same hashes have to be created by different clients    no salts    so what could be done to increase the difficulty of bruteforcing against a such small character space? i already tried key stretching with multiple rounds of sha256 but i am not how many rounds are required to significally increase computation time must be in the billions i guess.the only thing i came up with so far is using a server side secret which gets added to the password but in case of corruption it is difficult to guarantee that the secret is still a secret...i would be glad for some hints or ideas!regards,r0cks",0.018469656992084433,7,so,aes|bcrypt|cryptography|hash|scrypt,strong one way hash for weak passwords,3,weakness|protection|weak password,0.4481564462184906
10067,each user should have their own unique salt   there s no point updating the salt each time a user logs in this serves no real purpose with regards to security   the salt should be randomly generated and not linked to the password in any way   the purpose of a salt is to protect against pre-computation attacks such as rainbow tables so if two users have the same password they won t have the same final hash if the salt is system wide and not per user then this wouldn t be the case and an attacker only need to pre-compute all of the passwords for your system once if each user has their own salt then a pre-computation attack would need to be done on each user individually making the attack infeasible  using a salted hash does not protect against brute force dictionary attacks you would need to use other methods to protect against those  the real purpose of a salt is to prevent against precomputation attacks as the salt itself is not supposed to be secret i.e it is fine to have it accessible from the outside world  it is therefore not intended to provide security against brute forcing since it s almost as easy to hashsalt+password as it is hashpassword  if you believe someone will actually build a precomputed table of your one single salt to a database of passwords then look up the salted password hashes they found in your database with said table then you should use a unique salt for each password  otherwise don t worry about it  storing a unique salt per user is a good idea in my opinion re-generating the salt/hash combination every time the user logs in is a bit pointless unless you ve got cpu cycles to burn i d recommend using something like the     class to generate a secure salt/hash combo  simple example of generating a hash from a password   and the corresponding checking of a password   as adam already mentioned hashing and storing the password each time a user logs in serves no real purpose  rather than rolling your own you might want to look into using  bcrypt.net  a .net implementation of a proven password hashing algorithm   usage is very simple   it allows you to vary the computational costs of calculating a password hash if you want to making it more difficult for someone to do a dictionary attack on a database they might have obtained for example this is done by adding a parameter to the   method call  the details of the bcrypt algorithm  can be found here,2010-07-28 15:11:31.717 UTC,590,3354284,i had an idea about hashed passwords and salt values  since i m rather new to hashing and encryption i thought i d post this to you  would it be more secure to generate a unique salt for each user account then store the salt and hashed values in the database?  or keep a single salt value securely stored and re-use that each time i hashed a password?  for example a user would use the password   my code would generate a salt value of    then hash the result to get   the hashed result and salt would then be stored in the database in the users profile when their account was created  then each time the user logged on a new salt would be generated the password and salt rehashed and stored in the database  any thoughts?  like i said this is a sanity check on an idea i had,0.022033898305084745,13,so,c#|hash|password-protection|salt,sanity check salt and hashed passwords,3,attacks|protection|sanitization,0.44619330763816833
14295,from what i understand bcrypt is safer it s made to be slower this makes it harder for an attacker to brute-force a password it can be configured to iterate more and more which is useful since cpu s are getting more powerful  that s the point of having configurable slowness you can make the function as slow as you wish or more accurately as slow as you can tolerate indeed a slow function is slow for everybody attacker and defender alike  these links might be of some help   https://security.stackexchange.com/questions/61385/the-brute-force-resistence-of-bcrypt-versus-md5-for-password-hashing    https://www.bentasker.co.uk/blog/security/201-why-you-should-be-asking-how-your-passwords-are-stored    what&#39;s the difference between bcrypt and hashing multiple times?    https://www.quora.com/what-is-the-difference-between-bcrypt-and-general-hashing-functions-like-md5    https://security.stackexchange.com/questions/4781/do-any-security-experts-recommend-bcrypt-for-password-storage/6415#6415      but does this not require my hash to already be present with the attacker to compare to? and if he/she doesn t have the hash in the first place then how does the hashing algorithm that i use affect my sites security? and eventually he ends up having to brute force my login page anyways?   first no many sites allow login attempts without a rate limit with md5 assuming the servers can handle it a user could very rapidly attempt to brute-force passwords just by trying lots of passwords in quick succession bcrypt s slowness guarantees that such an attempt will be much slower  second a key security concept in computing is  defense in depth  you don t want just one level of security - it s fairly easy to accidentally write a sql injection vulnerability that might let an attacker dump password hashes by using bcrypt you  limit  the  damage  such a vulnerability can cause,2016-01-15 14:39:36.343 UTC,457,34813483,please read the updates too since my actual confusion is in there   it has been quiet sometime since joomla! started supporting the   hashing algorithm alongside the   that has been the defacto since joomla! 1.5  now my question is as an end user what benefits do i get if i start using   right away in comparison to the current algorithm viz  ? does it even make any difference for a normal blog with a few hundred visitors daily?   update:-   also i read somewhere that due to the speed of   hashing  my password could be easily calculated in a matter of days/months @ most  but does this not require my hash to already be present with the attacker to compare to?and if he/she doesn t have the hash in the first place then how does the hashing algorithm that i use affect my sites security? and eventually he ends up having to brute force my login page anyways?  and if its down to brute forcing their way through then isn t   equally vulnerable to password guessing?    note  why the downvotes this a logical concern as an end user ,0.019693654266958426,9,so,bcrypt|hash|joomla|md5|php,how is bcrypt better than md5 + salt,3,attacks|vulnerability|sql injection,0.4456239938735962
2475,generally locking accounts as proposed by raz is a very bad thing - it leads to helpdesk load annoyed customers and is not needed to actually prevent brute force attacks  temporary suspensions  are  used by more and more systems often some delay that foils brute force like 5 or 30 minutes or by using an escalating scale eg doubling the timeout each failed login attempt   brute-forcing is normally used in the context of an  offline-attack  where the attacker already knows the password-hashes and can use the full power of his system gpu to find a matching passwords  you are talking about  online-attacks  an increasing delay would indeed make brute-forcing unpracticable though it is not so easy to implement as it may sound an attacker could then e.g block a user from a successful login brute-forcing an online system does not allow for enough attempts anyway   online bruteforce attacks   indeed sites that don t implement some form of protection against online bruteforce attacks are leaking their users  passwords many sites don t have any form of protection in place and many others do  simple blocking has issues   if we block the user s ip after x failed tries a 10 000 strong botnet gets to try x* 10 000 passwords at 100 000 tries a lot of the weaker passwords are already cracked  if we block the user s account after x failed tries any attacker can block any user s account   this implies that the defense needs to be more sophisticated or we can simply ignore defense and try to blame the user when it eventually blows up which is less than ethical more sophisticated schemes use some combination of detection of suspicious activities and conditionally stronger login requirements  to detect suspicious activities we track positive signs based on successful past successful logins such as the isps of the user and some identifier of the devices that the user logged in with before we can also use negative signs such as ips from tainted isps or countries or a rapid increase in login attempts  in terms of stronger login requirements we can use short timeouts captchas multi-factor authentication and even blacklists   offline bruteforce attacks   all the above measures are useless once our disgruntled system administrator sells a copy of our password database on the black market because then the attacker can just hash the password himself and see if it matches the database cracking the the hashes can be cheap and easy or still easy but very very expensive depending on how we hashed the passwordssee  how to securely hash passwords?   salt to make it harder on the attacker to crack the whole database we use a salt which makes each hash slightly unique every user has a different salt which is stored next to the hash this means the attacker can t just calculate the hash of one password and compare it against every user s hash but instead the attacker has to calculate a new hash for every password he tries against every user using a salt does not increase the cost to get the password of a single user  expensive hash no attacker can bruteforce 100 billion passwords a second unless the hashing scheme was set up by a moron which is plausible some attackers can calculate 100 billion basic hashes like sha256 per second but the solution is simply to not use a basic hash to hash the password a trivial solution is that our hash isn t actually   instead it s   where we use around 100 000 iterations of sha256 making any attack 100 000 times more expensive in reality this defense is too simple to break with specialized hardware so in practice we use special key derivation functions tike   and   to mix multiple basic hashes in ways that are expensive for any attacker to calculate regardless of hardware,2014-09-25 15:21:56,811,68190,i am into bitcoin right now and the password threats scare me a little..   not so much for me because i use lastpass and generate new passwords at 20 characters and it includes symbols upper and lower case letters as well as numbers  however i know that we are nearing a time where 100 billion passwords a second can be attempted as a brute force and i was thinking about the amount of people who use teddy567 or qwertyqwerty for passwords and i think to make the internet safer we should come up with a way to stop brute force attacks  so can you help me to understand why sites can t implement a 3 second delay or hell even a 1 second delay in any missed password  this would in my mind totally end all brute force attacks  it also would not even be noticeable by humans,0.029593094944512947,24,sse,bitcoin|brute-force|protection,why don t sites implement a system where a wrong password causes a 3 second delay,4,leak|attacks|weakness|protection,0.4451441466808319
1633,bcrypt doesn t compare hashes at all bcrypt just hashes if you compare the result with a naïve   then you re theoretically susceptible to timing attacks this has been raised in a  github ticket for phpass  with the response being     i ve looked into this previously and all my research indicates that this is unnecessary when dealing with proper password hashing techniques even the article you link states using one-way hashing should defeat this because it’s very hard to work backwards from timing leaks when the attacker derived input keeps changing hashes to a different digest each time a byte changes in the original plaintext input      i ll be happy to look into this again but these issues are related more to accidental data disclosure than brute-force password cracking when hashes are in use   so phpass does not explicitly try to prevent such timing attacks and the author does not deem the issue to be a problem  the official bcrypt implementation for password hashing in php is     and     the latter of which does  implement a constant-time xor comparison  there s a  userland implementation  for php versions below 5.5  the cool thing about hashing is that even a one-bit change to the input will completely change the output keep this in your mind! how does that relate to your question? well bear with me a little  in the vast majority of cases access to the salt implies access to the password hash this is especially true in the case of bcrypt since almost all implementations store the salt the hash and the iteration count in the same string with delimiters also keep this in your mind!  since the comparison is happening on the password s  hash  and any change to the input password will completely change resulting hash remember?  the yield of the timing attack will be information about the hash itself  at best  however that s  only true if  the attacker has access to the salt remember what we said? an access to the salt implies access to the hash this means that  the timing attack in this case is irrelevant   if the attacker doesn t have access to the salt then the timing attack will give practically speaking zero information thus it will give the attacker zero advantage   bottom line  for hashing schemes such as bcrypt scrypt etc timing attacks are irrelevant using comparisons such as   and   isn t a problem having that said using constant time comparison is a good practice as part of a  defence-in-depth policy  in fact many bcrypt implementations already used timing-safe comparison just in case     in  py-bcrypt  for example   update   there is indeed a way to extract some information about the hash without knowledge of any of its components salt iterations or plaintext value it s described in a  comment by oasiscircle      this can still leak parts of the hash using the early-exit byte  comparison example attacker computes tons of hashes and stores them  looking for a hash that starts with 0x00 upon finding a plaintext  with that value send that plaintext listen for timing on response.  do this for 255 more values until one value takes slightly longer.  that s the first byte value of the hash created from the plaintext   having that said i still hold the same opinion expressed in the original answer about the irrelevance of the timing attack when using a hashing scheme such as bcrypt while there s indeed a possibility of such attack it s extremely infeasible the computation and storage requirements for such attack are enormous and they keep grown exponentially with every bit discovered from the hash,2013-11-29 07:44:46,671,46212,i saw this function   while reading  salted password hashing - doing it right  which uses a byte-level xor comparison to avoid timing attacks i was wondering whether this is what bcrypt also does to avoid timing attacks i use  openwall phpass  which has its own    method  which doesn’t seem to xor how does it avoid timing attack,0.029806259314456036,20,sse,bcrypt|passwords|php|timing-attack,does bcrypt compare the hashes in length-constant time,3,leak|attacks|timing attack,0.4449448883533478
26675,bcrypt is comparatively stronger than pbkdf2    even though modern gpus and fpgas may have enough memory to handle bcrypt the higher memory requirement still slows them down in comparison to the sha hash family the basis of most pbkdf2 implementations  blowfish is a very hard algorithm to vectorize unlike the sha family vectorizing sha is very gpu-friendly and allows a lot more parallelism    it might not matter though   bcrypt has a linear advantage over pbkdf2 at best that is if the enemy can crack a pbkdf2 system in x time they could crack a bcrypt system in n*x time for some constant n>1 and n is probably not big enough to make a game-changing difference  the way i look at it is if my password is weak enough to be broken in any reasonable amount of time then multiplying that amount of time by n by changing algorithms is not going to change things that much  the more advanced the encryption the more advanced the technology becomes to beat it   bcrypt is good because it allows you to scale the size making it harder for a traditional computer attack however if you are really worried about more sophisticated attacks research how to prevent those type of attacks specifically you will probably have to use a combination of security measures depending on the level of sophistication the attack has  but for the majority of sites just looking to protect passwords of their users bcrypt is a pretty darn good option,2013-08-22 21:22:32.227 UTC,350,18391066,bcrypt was designed over 10 years ago  according to opinionated internet voices bcrypt is better than alternatives like pbkdf2 because of its gpgpu attack mitigation features high memory mutating tables even though both algorithms have a modifiable work factor which slows down brute-forcing attempts  i ve read that bcrypt really only takes 4k of mutable memory and that for modern gpus or fpgas it s no better than alternatives  how true is this? does bcrypt even have an advantage versus these sorts of attacks anymore,0.025714285714285714,9,so,algorithm|cryptography|encryption,is bcrypt really better at mitigating gpu attacks than other key derivation algorithms,3,attacks|weakness|protection,0.4447457492351532
2952,your first assumption that an attacker who gained access to the database can find all the passwords is invalid even a single invocation of md5 with no salt and no iterations will keep the strongest passwords secured  but only a very small percentage of users use a password which is strong enough to remain secure under such a simple hashing  calling rainbow tables a weakness is not exactly correct rainbow tables is a method for attacking a very specific weakness in some password hashes but there are other ways to attack the same weakness even some methods that consume less memory and cpu time than using rainbow tables  this weakness is easily addressed by using a salt a salted hash will defeat both rainbow tables and most other brute force attacks state of the art password hashes have been using salt for multiple decades even the old and by now obsolete des based password hash used a salt  your approach to salting does share a weakness with the salts used in the des based passwords and that is the length of the salt the salt is supposed to be long enough that it never repeats not even by coincidence if every computer system in the world used the same password hashing algorithm for all passwords stored for centuries  using a salt which is long enough is practically free so there is really no reason for using a salt with only 20 bits of entropy  appending salt and hash is common practice in that respect your approach is practically identical to standard methods standard methods however include a format specification in order to preserve compatibility in case the algorithm need to be changed they also usually put an explicit separator between salt and hash  leaving out the separator and pretending that will slow down an attacker is security through obscurity it doesn t work  obscurity is more likely to confuse the developer and cause a security flaw to be left in the code during development than it is to actually prevent an attack  the way you are generating randomness looks fishy but i don t know enough about the specific platform you are developing for to say if it is a sensible approach  there are other aspects about password hashes that may impact your security but there isn t entirely consensus on those points  there is agreement that it is better to use newer hashes with no known weaknesses rather than using md5 however none of the known weaknesses in md5 could actually be used to attack the password hashing in your question an attack against it would surely focus on weak passwords as well as the short salt  then there is the question about whether password hashing should use iterations or do all kinds of other expensive computations such approaches will slow down legitimate usage and brute force attacks by the same factor as such they are not very effective increasing the length of a password from 10 to 11 characters would only slow down legitimate usage by 10% but it could slow down brute force attacks by an order of magnitude this is why longer passwords is a better way to slow down attacks  that s not to say that iterated hashes are useless they do slow down attacks but they also slow down legitimate usage on your own server which makes it an easier target for dos attacks  if someone gets access to the database with passwords in they might well have enough access on the system to read the code implementing the hashing as well if they do then they know the structure of the hashed passwords  it is also common for the attacker to know at least one password on the system their own so if they get the hash for their own password then they may be able to work out the structure by comparing their own password and salted password  the good news is that salting works even if the attacker knows how the salt is constructed and stored for very little cost when setting &amp checking passwords you make it enormously more expensive/slow to crack passwords in bulk so there is no need to worry about hiding the structure unix/linux shadow password files do not for instance - instead just worry about generating salts with enough entropy that they make the attacker s job much slower  while a hash can always be reversed with enough time the goal is only to make it slow/expensive for the attacker so a the attacker gives up and move on to an easier target b you can notice that you have been hacked and tell users to change their passwords before many passwords have been compromised  if i understand your php right it cuts off some of the md5 hash to replace it with the salt - directly reducing the security of your hash trading off the strength of the hash for a little obscurity is a poor tradeoff since salting works even if the attacker knows how it is done ideally use an existing implementation like crypt  since its implementation on modern systems at least should reflect best practices,2015-02-21 22:42:37,1120,82249,what i m about to say is probably going to make me look like an idiot but it s better to be wrong and learn than have unanswered questions  ever since i started dealing with passwords i wondered what the big fuss about hashing passwords is anyway since if somebody got a hold of your database they could find out any password no matter what hashing technique is used  so basically the big flaw of old hashing functions were rainbow tables something that the currently modern algorithm used by php in their password hashing api blowfish deals with of course it is pretty complex and undoubtedly it s built by people way smarter than me but still i just couldn t help but wonder if i built my own hashing function that wasn t vulnerable to rainbow tables and didn t store the salt in obvious places such as extra columns in the database or use user information like name or email as salt would that be any good?  i just spent like 5 minutes to prepare this example for you which uses of course md5 as i know it is the most hated around php developer critics     it will produce different hashes for the same input every time and the salt is blended in the final output hash if you don t explicitly know where it is i don t think it can be found but that s why i m posting here to find out if i m wrong,0.027678571428571427,31,sse,passwords|php,password hashing and blowfish,6,flaws|attacks|weakness|vulnerability|weak password|denial of service,0.4426575303077698
3572,a  collision  for a cryptographic hash function  h  is a pair of data elements two sequences of bits  m  and  m   which are distinct from each other but hash to the same value  m  ≠  m   but  h  m  =  h  m   since a hash function accepts as input sequences of bits that can be much longer than the fixed output size the number of possible inputs to a hash function exceeds the number of possible outputs therefore it is a mathematical certainty that collisions exist for any given hash function a  cryptographically secure  hash function is such that actually finding a collision is computationally infeasible we know that collisions exist but we do not want people to be able to find them  a  collision attack  is an attack that exploits the structural weaknesses of a specific hash function to produce collisions this works only for hash function for which a weakness is known i.e functions that by definition have been demonstrated not to be cryptographically secure   what a collision attack would allow for an attacker in your case is unclear your envisioned virus database stores hash values computed over sequences of bytes that have been deemed malicious and should be shot on sight you want to store hash values and not the malicious sequences themselves mostly to make lookups more efficient and to keep the storage costs low if the attacker arranges for two of his virus to hash to the same value then this means that inclusion in the database of the common hash value would resulting in banning  both  virus in one go which is about the opposite of what the attacker would like the attacker wants his virus to go undetected not to be spotted even more efficiently  collision attacks would be a huge problem in the case of a database of non-virus if you make a database of sane executables then the attacker would try to make a collision between a harmless executable  m  and some malware  m   he would then submit  m  for verification so that  h  m  is added to the list of verified harmless files since the attacker arranged things so that  h  m  =  h  m   that collision would mean that  m   the virus would also be declared sane   while it is unclear what could be gained for the attacker from hash collisions in a virus database it is nevertheless considered better to use a cryptographically secure hash function just in case the main example of a  non -secure hash function is md5 collisions can be very efficiently produced for this function the situation of sha-1 is less clear no collision has been exhibited yet but a theoretical attack has been described which should result in collisions with a computational effort that is huge 2 61  evaluations of the function but still substantially less than the efforts which would be required for a perfect hash function with the same output size 2 80   the usual recommendation for a secure hash function is  sha-256  or more generally any function of the  sha-2 family  sha-256 is one of the currently six members of that family,2015-08-31 17:10:11,571,98213,i have been told if i wanted to create virus signatures from infected files to use for detection in the future i should be careful as some of them are prone to collision attacks,0.04903677758318739,28,sse,antivirus|hash|virus,what is the most secure hash for virus definition databases in 2015,5,virus|attacks|exploit|malware|weakness,0.440811425447464
22414,md2 produces a hash of 128 bits you can guarantee at least one hash collision by hashing 2^128+1 strings   well the best attack according to wikipedia is     in 2009 md2 was shown to be vulnerable to a collision attack with time complexity of 2^63.3 compression function evaluations and memory requirements of 2^52 hash values this is slightly better than the birthday attack which is expected to take 265.5 compression function evaluations   it would however still be tricky to find a collision for a short email address  if you really require more security  and  a 128 bit hash value you are much better off by using the first leftmost 128 bits of sha-256 which is considered secure at the time of writing using the full 256 bits is of course preferred  the chances of creating a collision by accident are close to zero so if you just use this to create something unique i.e without considering targeted attacks then using md2 is fine even then if you can change the protocol use sha-256 instead  hash functions like md2 are like the name suggests message digest algorithms they take an arbitrary length input and give a fixed length output there are bound to exist collisions for unique inputs  you can use stronger hash functions like sha-256 where a collision between two strings is highly unlikely compared to md2 the birthday paradox applies here so you shouldn t use mdx functions or anything shorter than 256-bit hash functions  i see you re hashing e-mail addresses depending on your system this can be exploited with a collision for example some e-mail providers enable virtual addresses of the form myname+1@domain.com myname+2@domain.com and so on an attacker might use that fact to find a collision with a known other e-mail address to get for example the password reset e-mail or something like that,2015-05-28 08:47:30.907 UTC,340,30501521,i have a list of unique email addresses  - a very simple question if the email is unique will the output always be unique,0.020588235294117647,7,so,cryptography|hash|php|string-hashing,hashing unique strings with md2,3,attacks|exploit|vulnerability,0.43976786732673645
5006,i know that md5 is the most vulnerable hashing algorithm   well technically we are technical around here there are worse algorithms than md5     and particularly vulnerable to collisions   yes folks can create a desired hash with a different plaintext this is not likely to happen randomly but could occur maliciously     but the collision vulnerability is not very risky and somebody might use that as an advantage but that s with sheer luck   not sheer luck there are techniques to find a plaintext that produces a desired md5 that s a good subject for a different question     ok let s say i store passwords using md5   ouch the main reason you shouldn t use md5 is because it is a  general purpose fast hash   you should be using a  slow password hash  such as    bcrypt is commonly recommended  but be sure to run a quick sha-2 hash on the input data so super-long passwords will not be truncated by bcrypt  pbkdf2 but that is less gpu-resistant because it has lower memory requirements  scrypt is better than bcrypt  if  you have a high enough work factor otherwise it is worse against gpus again because of higher or lower memory requirements  the winner of the  password hashing competition  may be even better than the aforementioned but has not yet stood the test of time so don t use it just yet it s called argon2 and has separate work factor settings for cpu time and memory load nice!  repetitive sha-2 can be used instead of pbkdf2 still not gpu resistant but this is more tricky to implement the repetition efficiently i.e to be brute-force resistant because sha-2 is actually a general purpose fast hash   most of these options generate random salt by default but you should verify whether this is the case!  it is best to include some pepper ~72 bits of entropy before the password prior to hashing the pepper can be the same for all your users but should be stored in a file outside of the database so that component cannot be found via sql injection  make sure your work factor requires about 100ms with appropriate dos protection on your target hardware knowing that attackers will use faster hardware for brute force  of course no amount of hashing will protect weak  s so include password strength requirements     collision vulnerability .. is there any way that the attacker can use this as an advantage?   in the context of password hash storage this probably will not help the attacker  the main reason why hash algorithms are attacked nowadays has nothing to do with passwords i believe that md5 is still reasonably secure when it comes to password hashing provided you use salts to defeat rainbow tables and iterate the algorithm many times to slow down brute force guessing however  you really should use a standard algorithm instead  the problem is with digital signatures or x.509 certificates respectively  for 10 years now there is the chosen prefix attack on signatures using md5 in 2006 or 2007 a team around arjen lenstra from the technical university eindhoven created a rogue certificate which had the same md5 hash as one issued by a commercial ca the attack required significant computing power and the setup was extremely sophisticated but the result was that md5 was compromised see  here  and  here   the same technique was also employed in the famous flame attack  as you correctly noticed when it comes to password hashes the situation is completely different because a collision needs to be generated without knowing the plain text when it comes to digital signatures the plaintext is usually known and of course this opens up additional attack vectors as the work quoted above shows,2016-09-30 16:09:29,770,138363,i know that md5 is the most vulnerable hashing algorithm and particularly vulnerable to collisions but the collision vulnerability is not very risky and somebody might use that as an advantage but that s with sheer luck  ok let s say i store passwords using md5 there are some colliding strings here  https://crypto.stackexchange.com/questions/1434/are-there-two-known-strings-which-have-the-same-md5-hash-value but it s very less likely that a user might use these kind of strings as his password that s why it depends on luck  and now let s say a user has used one of these strings for an attacker to use this collision vulnerability he/she has to know the original password and know that there s another colliding string for it but if the attacker already knows the original password why bother with collision? is there any way that the attacker can use this as an advantage,0.032467532467532464,25,sse,algorithm|hash|md5,why is md5 considered a vulnerable algorithm,6,attacks|weakness|protection|vulnerability|sql injection|denial of service,0.4395348131656647
985,regardless of hashing the inherent weakness of passwords is that they are chosen and remember by humans humans are not good at such jobs they will choose and remember passwords from a rather small set of possible passwords namely words which make sense one way or another an attacker with a computer can try all plausible passwords at the speed of his  computer  which can be devilishly fast at that job up to several billions per second with an off-the-shelf gaming pc this is called a  dictionary attack   requiring the addition of a digit and a mix of uppercase/lowercase letters is an attempt to force human users to enlarge their set of possible passwords there are more possible passwords consisting of a meaningful word + one digit than possible passwords consisting of a meaningful word exactly ten times as many actually  such rules often backfire when asking for an extra digit most users will add a  1  and add it at the end which means no enlargement of the set of possible passwords at all and the extra length incites users to choose a shorter word from a shorter list thus actually  reducing  the size of the set of potential passwords.   hashing  is a second-layer defence system meant to thwart attackers who could  partially  breach the server and got a peak at the database of stored passwords the size of the set of potential passwords is important regardless of hashing when the attacker has access to the hashed password he can just run an  offline  dictionary attack where each try is just a matter of computing the hash function this makes the task easier for the attacker instead of having to talk to the server for each try which is an  online  dictionary attack but it does not change the core concept which is that users should use passwords from a  large  set  complexity still adds security  @thomas pornin is correct in his answer but i d like to provide a different way to think about it  assume that we are hashing  how resilient to attack is   upper/lower/special/symbols about 84 potential values iirc?  upper/lower/numbers 64 symbols?  hexadecimal numbers 16 symbols?  decimal numbers 10 symbols?  binary numbers 2 symbols?   obviously case #5 is reductio ad absurdium everyone should accept that 2 symbols are insufficient we want the symbol set to be as complex as we can make it without imposing an undue penalty on users or other system elements   there are three primary types of attack that can be done against hashes brute-force attacks dictionary attacks and pre-computation attacks   brute-force attacks  a brute-force attack involves selecting a range of characters e.g lowercase and numbers and computing the hash for every single possible permutation of those characters for a range of password lengths each hash is compared against your target hash and if it matches the password has been found for example we might choose   as our alphabet for passwords between 5 and 8 characters defending against such attacks is reliant on the computational cost of each hash operation the alphabet needed to successfully attack the password and the length of the password since modern technology allows for gpu-based acceleration of hashing it is important to use a slow key-derivation function e.g pbkdf2 or bcrypt instead of a single hash   dictionary attacks  dictionary attacks involve running through a large list of pre-chosen words that are  likely  to be used as passwords it is important to note that most dictionaries don t just include real dictionary words - they also include various pseudo-words and other values that are found in various database leaks and common password lists these attacks are more efficient than brute-force attacks in general because they focus on the kinds of passwords that humans choose rather than completely random values defending against such attacks almost entirely relies on not picking a common password or dictionary word   pre-computation attacks  instead of computing hashes repeatedly and comparing them to the target hash pre-computation attacks involve computing hashes for a set of chosen values like a dictionary attack and storing them in a file or database hash databases and rainbow tables are two common methods of doing this this provides a very fast lookup of plaintext for any known hash since it s just a case of looking up the hash in the index and returning the associated plaintext this can be defended against by using a salt i.e a random value appended to the password before hashing this makes computing rainbow tables for each possible salt value completely infeasible   so why are complicated passwords important? it depends really if you re doing password hashing properly using pbkdf2 or bcrypt with a reasonable cost factor complexity beyond not using common passwords isn t  actually  that important it s more important to avoid dictionary words and common passwords and complex passwords do usually offer that kind of protection however choosing a long and unusual non-dictionary password that is memorable e.g   works just as well if you do password hashing incorrectly e.g salted sha1 you need a much stronger password to remain safe because gpus can compute tens of billions of hashes per second  of course you re going to have to deal with the human aspects i think one of the best things you can do is  warn  users if they use a common password by storing a list of the ~2000 most common ones you can get lists of these online and checking against them as long as you re properly hashing passwords most users should be reasonably safe even in the case of a database leak  most of these attacks are based on the model of your site being hacked and your passwords stolen e.g via sql injection so it s important to adhere to secure coding practices and be aware of common vulnerabilities  further reading   owasp top 10  2010  /  2013 release candidate  list of common web-app vulns   how to securely hash passwords?  quick details on optimal password hashing   how to store salt?  covers the full evolution of password hashing and attacks   rainbow tables wikipedia,2013-01-18 08:24:45,1125,31636,some websites even the stackoverflow asks for atleast 1 digit 1 uppercase character in the password does this really matter when the developer uses a password-hash algorithm to store the passwords in the database?  i am building a project where the security really matters but the users are stubborn to use weak passwords i want to motivate them to use strong passwords by using rules that really matter and educating them about the reasons for such requirements  is it sufficient to prevent phone numbers their own name or dictionary words? why not,0.03111111111111111,35,sse,hash|passwords,what s the use of making users use digits uppercase-lowercase combination password if the passwords are hashed,9,leak|owasp|attacks|weakness|protection|weak password|sql injection|secure coding|vulnerability,0.4389317035675049
67129,only the size of the key/passphrase really given long enough it s always possible an access point could theoretically have some rate limiting built into it which would help against authentication schemes that can t be attacked offline but in practice the only solution is to use a long key/passphrase you also need strong encryption of course do  not  use wep it s broken similarly do not use any variant of ms-chap if using wpa-enterprise.  when discussing brute force attacks it s important to consider the size of the search space the collection of possible values this is often discussed in terms of  entropy  usually discussed in the base-2 logarithm of the number of possible values for example if there s about a billion 10^9 options that s 30 bits of entropy 2^30 is roughly a billion compare this value with rate that your attacker can try guesses at in general it s probably worth assuming at least a thousand guesses per second for an online attack though in practice home wifi access points can t support that many  completely random english letters and numbers case-sensitive are about six bits of entropy per character but the letters in english words are only a couple bits per character there are usually only a handful of possible options for the next character in a word this means in practice that human-memorable  passwords  are usually not very good but pass phrases  especially ones composed of randomly-chosen words can be quite good   xkcd  has a pretty good illustration of this concept  you can consider allowing connections only to whitelisted mac addresses and then even brute forcing the password won t succeed but this is only a defence against casual attacks as mac spoofing is relatively simple to achieve   what makes wifi encryption harder to brute force?  first a strong password preferably not set to the router default the best password is set using a cryptographically secure random number generator  csrng  to the full 63 ascii or 64 hex chars you can also use wpa-802.1x mode however this requires a authentication server and therefore is generally used for large networks  however a password built from five and more words would be far easier to remember without significantly compromising the security of the key  another issue is  wifi protected setup  specifically the pin method significantly reduces the entropy for an attacker  the pin is a 8 digit number checked in two chunks  this reduces the entropy from around 50 bits 5 word password to less than 15 bits 2 times a 4 digit pin a factor of more than 34 billion turn that off and you ll be relatively safe,2015-09-23 04:07:17,490,100926,i know that a coder is able to connect to a wifi through code so my question is is there anything that will make it difficult to brute force a wifi password,0.022448979591836733,11,sse,attack-prevention|attacks|brute-force|protection|wifi,what prevents a coder from brute forcing a wifi,4,attacks|spoofing|protection|attack prevention,0.4387456774711609
3518,the important thing is that the attacker should not be able to brute-force to gain access.that doesn t necessarily mean that the hashing algorithm has to be slow the end-to-end login flow should be slow enough this can be achieved in several ways   as you mention change the hashing algorithm to be slow  disallow login attempts for a certain duration after a certain number of wrong attempts  mandate passwords of greater lengths and complexity greater length increases the number of possibilities to be tried for brute-force attempt   option 1 and 2 are of no use if the hacker gets access to the hashed form of the password and the hashing algorithm   edited  with the suggestion from the comment  option 2 is of no use if the hacker gets access to the hashed form of the password and the hashing algorithm  option 1 is also useless if the slowness is largely due to sleep that can be worked around for example if the source code of hashing algorithm is available then it s easy to delete the sleep other options are possible without the source code too  hashes protect passwords in case the attacker gets hold of the password database if adding   to authentication would be enough to prevent brute forcing it would automatically mean that your threat model allows for storing the passwords in the clear because you re only worried about online attacks since almost no threat model assumes that your systems won t be compromised it would be very irresponsible to assume that you have to protect the passwords so that they are resilient against offline attacks and then whatever   you added to your code is completely irrelevant  passwords are hashed for the case that an attacker can read the hashes from the database e.g sql-injection afterwards he can brute-force with the full speed of his own environment often with a gpu this is called an offline attack  a sleep on the other hand could only protect from online attacks even then an attacker could make multiple requests and wait for the results which are a bit delayed but are not slower,2015-08-14 07:10:25,425,96933,i understand most of the concepts behind hashing password but this one still escapes me  i understand that you want the hash to take some time  a couple milliseconds so the attacker can t brutteforce  but at the same time you don t want the hashing to take too much computer resources  why don t you just hash and then   for 0.1 seconds,0.03058823529411765,13,sse,cryptography|hash|passwords,hashing algorithms costs vs sleep,3,attacks|protection|sql injection,0.438685804605484
20381,"the  hashcat  is the fastest and most advanced password crack utility it can run on cpus and gpus it can use multiple cores in the gpu and can be parallelized to use multiple cores and boards the number of the tested password depends on the applied password protection mechanism see a benchmark  here  the modern password protection mechanisms as bcrypt and argon2 has features against fast passwords searches as   and     a system administrator may use the hashcat to test the passwords of their users if not easily found with a threshold with time then it is a good one otherwise propose the user to change the password of course there should be rules that prevent simple passwords min length numeral alpha-numerals etc..  the attackers when they access the system download the password file then they can use hashcat it is not entering a password to login again and again if so the login system starts to delay the login mechanism or lock the user account  the real benefit is that people tend to use the same passwords for other sites too once the attackers find some of the user s passwords from hacked site x then can try another site to see that the password is the same or not  how fast passwords can be cracked varies - by hash type hardware capability software used and number of hashes there s also an arms race between attackers and defenders that ebbs and flows as time goes on so the answer to your question will only apply to the rough era that it s asked so even though another answer was already accepted and even though the question is probably a duplicate it s worth re-answering definitively once in a while  first it sounds like we need to clarify the difference between  online  and  offline  attacks  if someone writes software to automate the process of an  online attack  - trying a list of usernames and passwords against an active web interface - they will hopefully quickly run into mechanisms designed to stop that only 5 bad attempts allowed for a given username or from a given ip address in a specific window of time etc  by contrast most password cracking software is targeted at  offline attack  - where an attacker has acquired the hashes passwords stored in the back end and can move them to their own platform to attack in bulk   so password-cracking discussions are usually centered around about  offline  attacks because the threat model that matters is if a threat actor  steals your hashes and can attack them using a platform of their choosing   offline cracking speeds here are dependent entirely on a variety of factors    how well the password was stored how slow the hash is     the hardware available to the attacker usually more gpus = better     and for well-stored hashes that are salted how many hashes are being attacked fewer unique salts = faster attack so attacking a single hash would be much faster than attacking a million salted hashes etc.    so to put some real numbers to your question    one of the most common benchmarks used to compare password-cracking performance is  ntlm  the hash used by windows systems to store local passwords it s useful for benchmarks because it is extremely common of high interest in many attack models and also a very fast easier to crack hash recently feb 2018 hashcat demonstrated the ability to crack ntlm hashes on a  single  nvidia 2080ti card at the speed of   100 billion hashes per second   disclosure i m a member of team hashcat at speeds like that the vast majority of password-remembering strategies that people use are very likely to be crackable by an attacker with the right tools and know-how only the strongest passwords either random or random-passphrase based - and of sufficient length/entropy are out of reach for the attacker    by contrast one of the slowest hashes and best for the defender is  bcrypt  bcrypt has a  cost  factor that doubles the cost for the attacker with each iteration bcrypt hashes of  cost 12  or so are recommended but even a relatively fast bcrypt cost cost 5 on the same 2080ti gpu can only be cracked at a rate of about  28,000 hashes per second  at that speed only the weakest passwords can be quickly cracked middling-strength passwords have strength in numbers and are harder to crack in bulk but can still be cracked if a single person s hash is targeted and any reasonably strong password will usually be out of reach for the attacker    again these are point-in-time answers and have to be adapted to your specific threat model   also keep in mind that  password-hash leaks are forever  defenders should store passwords today in a way that will be resistant to cracking  for years into the future  including estimation of future hardware capabilities moore s law etc",2019-02-17 13:44:01.823 UTC,971,54733868,google searches reveal that password crackers can quickly try millions of combinations and easily crack many passwords  my research does not show whether they can practically make that many attempts so quickly in a real-world attack  how do these password-crackers actually have to interface with servers?  are they filling out the forms in an automated way?  when i submit a password irl it takes up to several seconds to get a response  this would multiply the time required for password-cracking by a large factor!  this should provide a lot of protection against these password crackers!  do password crackers distribute password attempts among many many machines so that they can try them simultaneously?  isn t this trivial for website servers to recognize as an automated attack?  is there some faster way that crackers are allowed to make many attempts and why would servers allow it,0.027806385169927908,27,so,passwords|security,how many attempts per second can a password cracker actually make,4,leak|attacks|weakness|protection,0.4382031559944153
16710,i would consider a 128 bits of entropy in a token to be the de-facto standard owasp and cwe both recommend this as a minimum,2015-08-11 05:45:04.273 UTC,151,31934040,i was researching about csrf prevention specifically the unpredictable token a random alphanumerical string i found that one of the best solutions to generate the token is:     some people says that 16 bytes is enough for the length parameter $bytes = 16 others say that 32 bytes is the right option $bytes = 32    however  in this page  owasp recommends a 512 bits random string to achieve this number i would have to pass 64 as the length parameter $bytes = 64 but i read too that as this length gets bigger it can seriously reduce performance    so i don t have an idea of wich length i should use what is the best option? any suggestions,0.039735099337748346,6,so,csrf|php|token,length in bytes for csrf token,3,cwe|owasp|cross site request forgery,0.4376285970211029
50708,if the list of z values is not known in advance you could probably try coroutine for this,2017-05-04 13:42:07.44 UTC,78,43784570,i need to iteratively generate number x which follow these conditions   x^z mod n * x &lt n  n is known z changes in every cycle   i need it because i m implementing timing attack on rsa and it s needed to generate such number to measure time without modular reduction thanks,0.02564102564102564,2,so,python|rsa|timing-attack,python generating numbers in modulus,2,attacks|timing attack,0.4373614192008972
19373,so it looks like the answer is neither exactly  from the comments in the  asp.net identity source code           version 0:    pbkdf2 with hmac-sha1 128-bit salt 256-bit subkey 1000 iterations          see also sdl crypto guidelines v5.1 part iii          format { 0x00 salt subkey }      ultimately the hashing algorithim is sha1 but it is not a simple sha1 hash of the password or even a sha1 + salt hash  it is worth pointing out that sha1 is considered broken for digital signatures due to a mathematical attack reducing the computational effort of generating a collision to just-about feasible levels    this does not apply to hashed passwords   links for further reading   is sha-1 secure for password storage?    https://www.schneier.com/blog/archives/2005/02/sha1_broken.html    https://en.wikipedia.org/wiki/pbkdf2    https://www.owasp.org/index.php/password_storage_cheat_sheet    rfc2898derivebytes  and  hmacsha1,2016-11-16 05:42:33.93 UTC,269,40624840,i m using the default identity stuff provided by asp.net 4.5 mvc and entity framework  i can create users with passwords and the hashed password shows up in the database  i m trying to figure out if that hash is generated using the no-longer-trusted sha1 algorithm or the sha2 algorithm be it sha256 sha512 etc   articles which seem to say it defaults to sha256    https://www.asp.net/whitepapers/aspnet4/breaking-changes#0.1__toc256770148    http://kosmisch.net/blog/dotnetessential/archive/2015/2/1/aspnet-membership-default-password-hash-algorithms-in-net-4x-and-previous-versions.html    articles which seem to say it defaults to sha1    https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/consumer-apis/password-hashing    https://msdn.microsoft.com/en-us/library/system.security.cryptography.rfc2898derivebytes.aspx   when i follow the chain down i end up inside the passwordhasher.cs class -> hashpassword -> crypto.hashpassword which i can see is using rfc2898derivebytes which then has a bunch of stuff about hmacsha1   so are my passwords getting hashed by sha256 or sha1?  easy way to default to sha256?   if it helps here is a dummy password taken from my local environment:aipfkvy5v59jmvzdppu9qfumotocq+rp3dbt7m9rwmkzai5/61rekn/0inctxkpuoq=,0.01858736059479554,5,so,asp.net|asp.net-mvc|password-protection|sha,does asp.net use sha256 or sha1,4,owasp|attacks|data protection|default password,0.4364723563194275
26969,security is always a tradeoff - while using entirely random passwords will give you more bits of entropy it also increases the risk that users will write it on post-it notes resulting an illusion of security as the weakest point won t be in password strength anymore a pass- phrase  from several words with a few symbols is easier to remember while still providing enough entropy you d need to make sure that the list you re using has long words which are not in the most common 1000 words  of course is this secure enough depends on what you re protecting - definitely not secure enough for fort knox but it would be sufficient e.g for an im account  if someone knew how your password generator worked it would be pretty easy to code up a dictionary attack that could exploit it  if someone didn t know how it worked or that a password used was generated by it in this fashion i don t think it would be vulnerable to a dictionary attack,2011-02-22 08:53:10.35 UTC,413,5075942,yet another password question i m afraid..  i ve been reading up on password strength and so forth and i have a question about dictionary attacks on a password if you ladies and gentlemen would be so kind as to answer  as far as i can tell from the documentation i ve been reading simply put a dictionary attack works by comparing the password hash against hashes generated from a word list with additional modifications such as o-0 1-1 e-3 substitutions and casing changes  now correct me if i am wrong if i prefix the word with a salt then this complicates the password sufficiently that a dictionary attack is much less likely to succeeed  taking this further if i create a password that consists of two randomly chosen words from a long list interspersed with random numbers and punctuation i should have a strong password that is relatively easy for a human to remember or am i talking gibberish here?  for example my password generator creates a password of 14simplified%^cheese96 which you have to agreee is a lot easier to remember than sl&amp;tcreq!/u9k6%-sn$8ca  now on all the password strength checkers that i ve found and had access to both passwords rate as strong very strong or in the case of the microsoft checker best but just how good is the first password given that it is based on two words,0.026634382566585957,11,so,dictionary-attack|passwords,dictionary attack on multiple-word password,6,attacks|exploit|weakness|protection|vulnerability|dictionary attack,0.43584203720092773
67328,it falls into a category of the side channel attacks same requirements as for rsa execution time with private key should be constant   also it s not necessarily that ram is compromised the operations can be executes on tee trusted execute environment    exploiting timing information is one possible attack against things like password authentication systems  conceptually   works by comparing two sets of binary data on a byte-by-byte basis in reality processors can compare multiple bytes at a time depending on optimizations but the same principles below will apply the function starts at the beginning of the data compares each byte sequentially and exits as soon as it finds a difference if no difference is found then the function returns a code indicating that the data matches  because the function returns as soon as it finds a difference an attacker with a sufficiently accurate clock can deduce secret information they can induce calls to   with different inputs and by measuring which inputs take longer they can deduce what a stored secret might be   example   consider a classical password hashing system suppose your password is stored as a secret hash say for example   that hash was generated using the des scheme provided by the linux   function with a salt of   and a password of   note that this function is  insecure  and you should not use it in real systems.  in a strong password system your hash   is stored but your password is  not  stored when you are asked to authenticate the system will take your password hash it and then compare the two hashes to one another if the resultant hashes match then you are granted access to the system if the hashes do not match then your password is rejected this allows the system to check to see whether or not you know your password without actually storing the password itself   how an attacker can use timing to attack this system   in order to attack this system an adversary really just needs to figure out what password hashes to the stored hash normally the stored hash is kept secret but the adversary can use timing information to deduce what the stored hash could be once the adversary deduces the stored hash it is vulnerable to much faster and off-line  rainbow table  attacks as well as circumventing on-line security measures like password retry limits   the password system above has to compare a candidate hash against the stored hash in order to function properly suppose it takes 10 nanoseconds to compare each byte of the candidate hash to the secret stored hash if no bytes match one comparison then   will take about 10ns if one byte matches two comparisons then   will take about 20ns your attacker generates a few passwords and runs them through the system recording how long each one takes suppose the first few hash comparisons take about 10ns each and then return indicating that none of the bytes of the candidate hash match the stored hash after a few tries one of the hash comparisons takes 20ns- which indicates that the first byte of the candidate hash matches the stored hash in the example above this indicates that the attacker has deduced that the first byte of the hash   is     hashes by design have the property that you can t predict what hash will correspond to what password so for example this  does not  tell the attacker that the password starts with   however the attacker could have a large table of pre-computed hashes a  rainbow table  so they can look up other passwords that hash to a string starting with   after they try enough hashes they ll eventually get an input that causes   to take 30ns which indicates that the first two bytes match and they have deduced that the first two bytes of the hash are   they repeat this process over and over until they deduce all or most of the hash at that point they either know the password or can brute force it with a traditional rainbow table attack   this is a little hypothetical but you can find lots of practical information about timing attacks elsewhere on the net for example   https://research.kudelskisecurity.com/2013/12/13/timing-attacks-part-1/   the matter is not really to argue that a side-channel attack is practical in whichever application you have in mind for  your  system  a better point to make is that    side-channel attacks tend to be possible in  more situations  than most developers will be able to think of offhand    in any given situation  reliably  convincing oneself that there is  no  opportunity for timing attacks is  vastly more work  at development time than simply using a secure comparison method as sop  more work means both more cost and a higher risk of getting it wrong    simply put it s low-hanging fruit  having a policy of never use   on security-critical data is superior to a policy of okay to use   when that is safe on almost every parameter i can imagine   if extraordinarily you  are  in a situation where shaving a fraction of a microsecond of cpu time off each  non-matching  comparison which is usually the non-common case in the first place! will save you enough money that it s worth spending effort on a proper security analysis then you will have plenty of business documents proving that fact  and even so just reading comics for the time it would take you to do that analysis while you let moore s law do the work for you will probably save you those cpu microseconds for you equally well,2017-05-30 16:33:48,1023,160808,from       do  not  use    to compare security critical data such as  cryptographic secrets because the required cpu time depends on the  number of equal bytes.         instead a function that performs comparisons in constant time is required   why is that so? my thinking is that if someone has access to the machine that processes this security critical data then these secrets are already compromised because that person can extract them from ram or if that person has no access to the machine then they cannot accurately measure cpu time anyway,0.019550342130987292,20,sse,time|timing-attack,why should memcmp not be used to compare security critical data,4,attacks|exploit|timing attack|vulnerability,0.43581709265708923
33502,the smallest representable  normal number  in a 64 bit floating point number is   that range can be extended a little more using  denormal numbers  which take the smallest down to   but   another relevant small number again for 64 bit numbers would be   because   but,2016-01-21 17:36:17.577 UTC,108,34930570,i am doing calculations with python numpy here is a resulting numpy array   those are pretty tiny at what point should i worry about underflow in my calculations? these need to be ultra-precise is there a definite value range to worry about or perhaps a reference which states this value,0.027777777777777776,3,so,numpy|python|underflow,at what point should i worry about underflow in numpy values,1,underflow,0.4350808262825012
4627,in general  this depends on what information you are asuming that the attacker has  first let s asume that the attacker is blind and perhaps trying to crack a large dump of breached accounts without knowing that you used that specific algorithm then you would be better protected if you discarded   if it comes up or more realistically passwords that are only lowercase dictionary words etc this is for the simple reason that any attacker is bound to try those first since in general they are more common  on the other hand if the attacker knows that your algorithm was used discarding passwords will only make her job easier if you dump 10% of the passwords that is 10% she does not have to try saving her 10% of the time on average  so which one of these two scenarios are more likely? i would say the first one in most cases but only you can determine what your threat model is  a mathematical example  let s do some math to see how many passwords you would actually drop let s say that characters are picked from three groups upper case lower case numbers + special characters and let s say that there are an equal number of character in each group furthermore let s say that you require passwords to have at least one character from each group what fraction of passwords would you drop?  if i get my math right it is   where   is the length of the password for   you get 5% for   you get 0.1%  unless you are completely sure that the only threat is an attacker that knows how the password was generated i find it hard to imagine how you could be that i would say that reducing the search space by 0.1% is worth it  while @anders answer is accurate i want to extend his case for dropping low entropy passwords and i couldn t fit it in a comment  firstly i wanted to introduce a parallel many ciphers e.g des have  weak keys  which make encryption behave suboptimally this implies that there is no flat keyspace one where all keys have the same strength if there are enough known weak keys these are often added to a  reject list  and discarded the only time this is not done is if the number of weak keys is infinitesimal i agree that there is a big difference in encryption keys v/s passwords but the principle of discarding weak keys/passwords should still apply   secondly let s take the math further based on anders  perfectly valid imho math 2% of 12 character-long passwords can be characterized as weak this is not a trivial number say users with a weak password have a 5% chance of being brute-forced whereas a strong password has a 0.1% chance of being brute-forced   originally 0.2% of the accounts that you generate passwords for will be broken    =   ==>    but half of these broken accounts have weak passwords if you discard those 2% from your generator s output the number of successful attacks will be only 0.1%    =   ==>    thus by removing 2% of passwords the number of successful attacks will go down by 50,2016-06-21 14:18:05,680,127707,i have created a random password generator function which can be found  here  if anyone wants a look which will churn out passwords with a random mix of letters numbers and other characters  this question  and  it s answer  suggest that i should select the first chosen password because otherwise my passwords become more predictable based on what i like but theoretically what if one came out with only letters or only letters/numbers or only one special character should i then generate another password or am i losing entropy? i feel as if the first password would be vulnerable to wordlists but not using it technically makes my passwords more predictable is there is good place to draw the line?  p.s unlike that other question i do not care how hard these passwords are to remember,0.025,17,sse,entropy|password-policy|passwords,is it better to choose less-vulnerable random password,5,attacks|weakness|protection|vulnerability|weak password,0.4347323477268219
30618,here is a c# implementation using recursion    demo   if you want to retrieve the values later store the strings in a global array before returning from function,2015-08-15 17:31:22.807 UTC,134,32027367,i am looking for solution in c# to generate combinations on given list of characters or word to perform dictionary attack on zip files because we lost passwords file for those zip advantage is that we know possible words on it  dictionary should contain all the combos of words that i choose and all characters/words are small case only  example let say we have a set of chars    from the list of words a single word/character may repeat maximum of 4 times if any such alogrithm/logic available please suggest,0.014925373134328358,2,so,algorithm|c#|combinations|dictionary-attack|statistics,algorithm to generate all combinations of a specific size from a single set,2,attacks|dictionary attack,0.43305861949920654
67844,,2019-04-16 19:58:15,76,207560,if there are these blackboxes both are using algorithms which are considered secure yet   blackbox will sign any message no limit for messages to be signed  blackbox will generate 8‐digit hotp code for any counter value   are they easier to crack? how much if yes?  the only allowed interaction way is using user interfaces writing/copying a message setting counter value so no side‐channel or other physical attacks,0.02631578947368421,2,sse,black-box|digital-signature|hotp|penetration-test,cracking signing and hotp blackboxes,2,attacks|penetration test,0.4329220950603485
1369,those passwords are based on offline attack -- that is the attacker has the hash and is trying to crack it  if that s your concern use something like scrypt bcrypt or pbkdf2 to harden your password hashes and make them harder to attack offline  if you re concerned about online attack just limit the rate of account logins with some sort of exponential backoff or block the ip after some number of bad login attempts  either way you ll make the online attack so slow that the attacker won t get past  p4ssw0rd   there are 3 different kinds of brute-force attacks     common dictionary attack  say to 100 most common passwords or top 30000 or whatever typically a shallow attack run against a large set of targets looking to pick off the weak ones     exhaustive dictionary attack  with varying degrees of exhaustiveness all common passwords all dictionary words and keyboard patterns replace letters with numbers append numbers and symbols insert symbols.. etc typically a very targeted attack     full exhaustive attack  start at   and work your way up 100% success rate if you keep at it long enough very targeted not particularly practical    so the question is which dictionary are you guarding against? a simple solution is to just check to make sure your password isn t in the corresponding attack dictionary   checking for  1  is something that s already done on more advanced systems such as linux but unfortunately the password list for  2  could be many gigabytes or terabytes long it s generated on-the-fly when used so checking against it would have to be at least partially programmatic rather than a straight lookup as long as you re using roughly the same list for defense that is commonly used for offense you should catch all the easy ones  3  is of course impossible to defend against except to create very long passwords which you should do by today s standards 12 to 16 characters minimum depending on the selection of possible characters you use uppercase numbers symbols etc,2013-07-16 22:14:11,609,39051,suggestions to make a good password all focus on creating a string that a computer or more likely a network of computers with multiple gpus can not guess putting a password meter for when users create passwords can help against this a little e.g by enforcing a password at least 8 characters long with a mix of cases letters numbers and symbols but password meters and rules can t protect against the following passwords which all  look  pretty good   all of these passwords were cracked quickly for  this article  on ars technica it seems impossible to compete with the complexity creativity speed and comprehensiveness of cracking algorithms  so here is my question if in theory a few hundred crackers used their hardware and cracking programs to generate a massive list of all of their guesses--and they continued adding to it so that the list would include any new guesses likely to be folded into cracking programs e.g say another site gets hacked all the leaked passwords would go into the blacklist would this be feasibly useful as a secure password generation tool? or would it be easily circumventable or does it have some massive problem i m not seeing?  since this massive blacklist would probably have to be crowdsourced and therefore available to crackers and potentially even something that crackers could add to are there ways it could be used to a cracker s advantage? or would using this list make their job harder no matter what?  any other reasons why it wouldn t be useful,0.026272577996715927,16,sse,passphrase|password-cracking|password-management|password-policy|passwords,would a massive blacklist of guessable passwords be useful,4,leak|attacks|weakness|protection,0.432900071144104
14055,as you wrote the salt prevents rainbow table attacks and doesn t help against brute-forcing it s bcrypt s slowness which mitigates brute-forcing bcrypt offers a cost factor which controls the time necessary to calculate a single hash  the additional protection you want can be achieved better by encrypting the calculated hash with a server side key any algorithm like aes-256 the key doesn t become part of the hash then and can be exchanged whenever this is necessary the advantage is the same as with your fixed salt actually called pepper only an attacker with privileges on the server can start cracking the password hashes i tried to explain this at the end of my  tutorial  about safely storing passwords  so let the salt do its job and do not mix it up with other tasks instead encrypt the password-hashes afterwards,2018-01-23 11:18:09.46 UTC,278,48400412,brypt  generate a random salt for every password this is ok to prevent rainbow table attack but is this ok to prevent brute force attack ? i mean if the user choose a weak common know password brute force attack can be made on well know password list so my idea will be to concatenate the user password with a fixed salt and eventually also with a user id salt ie user pseudo if the attacker don t have access to the code of the software if he hack only the database then he will not be able to find real password using brute force attack with well know password list   so what the good way to do ?   or   or,0.03237410071942446,9,so,bcrypt|hash|passwords|salt|security,do we need to use a fixed salt with bcrypt,3,attacks|weakness|protection,0.4321061670780182
16881,your thinking is correct md5 and sha1 should never be used for password hashing  i would recommend the following in order of preference   argon2  bcrypt  scrypt  pbkdf2   if you tag your question with the language/framework you are using i can recommend specific libraries or methods  also be aware that  encryption  is not the right word to use here  these are password  hashing  algorithms not encryption algorithms  as per  owasp password storage cheat sheet  the recommendation is        argon2 is the winner of the password hashing competition and should be considered as your first choice for new applications    pbkdf2 when fips certification or enterprise support on many platforms is required    scrypt where resisting any/all hardware accelerated attacks is necessary but support isn’t    bcrypt where pbkdf2 or scrypt support is not available      md5 and sha1 are not secured for most security related use cases because it is possible to find collisions with these algorithms in other words given an input and its hash value it is possible to derive another input with the same hash value  sha-2 group of hashing algorithms are secured for many security use cases but not for password hashing because they are blazingly fast when compared with the above algorithms and performance is something we don t want for password hashing because that would make it easier for an attacker to perform a brute-force attack by trying a wide range of passwords in a short period of time  the above 4 algorithms are therefore meant to be expensive in terms of memory computation power and time these values are usually parameterized so that they can be tuned to a high value as new technologies improve the computation power with passsing time therefore while using these algorithms it is important to choose the work factor values correctly setting a very low valur may defeat the purpose  in addition to that a salt should also be used  again from the same owasp source         generate a unique salt upon creation of each stored credential not just per user or system wide      use cryptographically-strong random data     as storage permits use a 32 byte or 64 byte salt actual size dependent on protection function    scheme security does not depend on hiding splitting or otherwise obscuring the salt         salts serve two purposes          prevent the protected form from revealing two identical credentials and     augment entropy fed to protecting function without relying on credential complexity          the second aims to make pre-computed lookup attacks on an individual credential and time-based attacks on a population intractable,2019-01-29 23:11:26.533 UTC,514,54431028,just now i m working in a financial project here the team is thinking to use   for  .but today is easy copy a   or   password to decrypt inclusive if they are complex password like:  you might use a online page to decrypt it and there is it.small and mid-range developers including me uses those   but i think is not enough to provide security over the database.excluding       etc.  can someone give me a clue about what is the better approach to solve this vulnerability,0.021400778210116732,11,so,cryptographic-hash-function|cryptography|md5|password-hash|sha1,is still valid password hashing using md5 or sha1,4,owasp|attacks|protection|vulnerability,0.4320923388004303
60314,recursive calls of   cause stackoverflowerror    you can calculate in a loop not in recursion     you can set limit not to 10000 according to accuracy you really need      is a bad way of calculation use,2016-09-26 12:31:42.33 UTC,154,39702903,as a school project i am trying to calculate the value of pi using the taylor expansion of   as   i made this program to calculate the value of pi but i need very large values of count to calculate it and whenever i put the limit to value of more than  10000  say  20000  i get the error   is repeated a multiple times  i do not want to handle the exception as that would increase the time taken please tell me why this is happening and what other possible solutions could i use?   extra information    the code i wrote is as follows,0.01948051948051948,3,so,java|math|pi|stack-overflow,why is java.lang.stackoverflowerror given in the computing of pi using the taylor series expansion,2,overflowerror|stack overflow,0.4317905902862549
55426,try this udf   bignum udf   example,2018-01-22 14:41:21.997 UTC,73,48384076,using autoit when i multiply   by   i get   but in separate steps such as multiplying   by   seven times i get the overflow value of    it appears autoit cannot handle numbers with more than eighteen digits is there a way around this?  for example can i multiply   by   and have the result displayed as  ,0.0136986301369863,1,so,autoit|integer-overflow|numbers|numeric,how do i display a large number in scientific notation,1,integer overflow,0.42952314019203186
15481,key stretching  functions like  pbkdf2  allow you to control the time it takes to hash the password by adjusting the iteration count or other equivalent parameters for other functions  you re correct in observing that there s a tradeoff here between making passwords hard to crack by brute force and avoiding dos attacks  you ll need to choose an iteration count to balance these two considerations  do keep in mind that modern cpus can calculate millions of hash iterations per second  even if you use an iteration count of say only 1000 that still makes cracking the passwords 1000 times harder than not iterating the hash at all even though hashing a single password with 1000 iterations only takes about a millisecond  there are also other ways in which you can avoid or mitigate login dos attacks  for example you could implement ip-based throttling so that each ip address can only make some limited number of login attempts per minute  of course distributed dos attacks can get around that but it s still a hurdle for any potential attacker to overcome  also if you implement  login anti-csrf tokens  and you should they ll also protect you against certain simple types of login ddos attacks by requiring an extra round trip to fetch the token before any login attempt can be made   captcha s can be another useful way to deal with login dos attempts  finally you may want to restrict all login requests to a separate set of servers and/or processes with appropriate resource limits so that even a successful dos attack on the login form won t take down the  rest  of your site  if you really wanted to get fancy you could even require the client to submit a cryptographic  proof of work  more or less equivalent to the effort of hashing the password  for example you could send the client a random string and require it to send back a suffix to that string such that the random string plus the suffix hash to a value ending with  n  zero bits where 2  n   is approximately your iteration count  there exist  javascript crypto libraries  that you could use to implement the client side of such a system,2012-12-12 10:46:13.12 UTC,518,13838019,in registration operation there is a password crypt which as i understand can t parallelized since if it makes an operation of a key derivation pbkdf2 for example then it needs the previous value then i guess it is a linear operation?  making this process slow is benefic for a user since his password will be stronger for bruteforce for example but there is another problem dos attack! if each operation of registration takes 1 second then using a load balancer like nginx will handle 8 simultanuous registration operation every second which blocks the application? and if we add other operations say 2 seconds then the application is not good?  so any suggestions? using tornado will not solve the problem since tornado -as i understand- makes asynchronous operations in i/o operations.,0.03088803088803089,16,so,crypt|python|registration|tornado,what is the best time i can get for registration operation in python,5,ddos|attacks|protection|denial of service|cross site request forgery,0.4289431869983673
23801,random uses 48-bit key so it will repeat approx every 2^48 values it means not every possible   will be generated that may or may not be random enough for you if in doubt use securerandom you can always change it later  the biggest problem with using   to generate your iv is not that it is likely to repeat but that an attacker can predict future ivs and this can be used to attack cbc  related  https://crypto.stackexchange.com/q/3515/2805   yes the iv should be fully random if you don t use a full random you will likely leak information about the plain text don t forget that the random is xor ed with the plain text so if the iv has a predictable structure you may find repeating cipher texts and thus leak information the same way that ecb does this will be even more pronounced if the attacker can influence the plain text to be encrypted  an iv i not normally required to be unpredictable but is needed to be of one time use what this means is that a simple random number generator that uses a weak seed or has a short period should not be used to generate an iv  the strongest random number generators in use only generate a few tens of bit of entropy per second most algorithms treat the iv as being secondary to the key so the slow and strong rng should be reserved to the key and to seeding a fast long period rng for the iv,2012-09-03 14:03:41.96 UTC,350,12249392,for a stream cipher to be secure against repeated key attacks the iv s should not repeat themselves but does securerandom have a benefit over a simple non-secure random in that respect or is it just for generating an unpredictable sequence?  assuming i m using fixed sized messages with aes cbc mode and i generate a new random for each iv using the current nano time as seed does this increase the probability of repeating iv compared to a securerandom,0.02,7,so,cryptography|encryption|initialization-vector|java|random,is a securerandom really needed for generating initialization vectors or is random enough,3,leak|attacks|weakness,0.4284234046936035
17618,i recommened you to use sha-2 family the sha-2 family of hash functions are the current replacement of sha-1 the members of sha-2 are individually referred to as sha-224 sha-256 sha-384 and sha-512  for a hash you basically just want to choose the largest block size possible and use a salt value to avoid rainbow attacks  if you need to replace md5 in an application where using a hash was a bad design choice in the first place which include many uses in conjunction with password protection of login information or generation of a key from a password then you do not want just to replace md5 you want to change the design  google pbkdf2  there are implementations in php out there and it s a one-stop-shop for complete password protection  if you re just hashing the password and then done with it md5 is a little worse than sha1 which is a little worse than sha256 etc but they re all vulnerable to rainbow tables  make sure you re using proper salting techniques at minimum and to really do it right you should use an algorithm like pbkdf2 the object of which is to make it computationally intensive to brute force with a dash of obscurity in there as well since it requires the attacker to know the number of iterations you re using not just the algorithm,2012-03-16 11:43:49.9 UTC,258,9736617,i hear md5 is long dated and obselete what are my alternative options?  many thanks,0.01937984496124031,5,so,cryptography|md5|php,what is a current functional replacement for md5 cryptography in php,3,attacks|protection|vulnerability,0.4279502034187317
26507,attacker could very possibly guess that the first part is a salt   really?  how?  how do they know where the salt ends and the user-supplied password begins?  what s the rule for parsing salt out of a password?  are you saying it s obvious because the salt is numeric?  then use base64 salt  now how obvious is it?  the real value in a salt is not only in protecting a single record against attack but rather making it so that if multiple users have the same password they will appear different in their hashed form for that to be effective you have to use a per-record salt  if your security encryption/hashing mechanism results in users with the same password having the same representation in your database then you ve provided the attacker with an easy method of cracking many accounts at once  salting does make a big difference which hashing algorithm are you using though? remember salt will always be added to whatever is put in the field so even if a collision is found if he types it into that field on the form the salt gets added and the collision no longer matches  personally i hashhashsalt1+hashpass+hashsalt2 however salt only protects when one can get the hashes from the database if one truly can t get to the hashes then the best one can so is type random stuff into the form  if your salt is big enough it s not practical to create said rainbow table   http://en.wikipedia.org/wiki/salt_cryptography   it s also about having mutiple factors being used to encrypt your passwords while someone might steal your database they might leave out the salt and thus be sol  what you re saying doesn t make any sense  let s go with a simplified example to demonstrate  assume you re using md5 and the attacker has a table  the table includes  john1970->d3 88 99 bc 0c b5 dc 0e  d1 7f bb f7 eb f4 ea b2  if they then steal your unsalted database and see a hash d3...b2 they ll know ignoring collisions the password was john1970 and they can then use that password on your site and possibly others  if however you use the salt 123456 obviously a random one would be better they will see a hash  2a ca 76 59 03 23 35 61 e0 20 49 11 8f fe f3 0e  instead  even if they are able to compromise the salt when they get the hashes you should try to prevent this by storing them separately they will be left with  md5x+123456 = 2a...0e  and no way to easily determine x  salts are a very powerful technique and should be used whenever possible  on the other hand what you call non-standard methods appear to be half-cocked attempts at inventing your own unproven hashing algorithm  forget that  it makes a difference for the very reason you mention it makes things harder for an attacker using a dictionary attack  have a look at  you want salt with that?  for a good explanation  a salt is designed to be a defense against rainbow tables but the beauty of it is knowing that it is a salt in  no way  weakens it as a defensive measure  this is because it is not that a salt has some magical properties or anything -- it s because  it s an extra piece of information added to the password the attacker enters and is specific to the password he is attacking -- it s not something that can be reused for multiple accounts -- not even multiple accounts on the same server  sure the attacker can just add the salt to his rainbow tables but you ve just made his rainbow tables have to be bigger by a factor of whatever data you use as the salt  if you add two random bytes you ve make the attackers rainbow tables have to be 65536 times as large  that is not insignificant  add four random bytes and the factor is above 4 billion  a salt is used in conjunction with a cryptographic hash not merely tacked on the front of the password you don t store the salted password or the password but rather the hash of the salted password the purpose of a salt is to protect users from bad password choices not obscure the password you never use just a salt  to clarify:a salt will not protect against a dictionary attack.it can protect against a rainbow table style attack where a large table of hashed and a corresponding input text is generated good passwords protect against guesses dictionary attacks salts help protect against someone getting a hold of your hashed passwords by making it expensive to use precomputed tables when i said bad i meant passwords that were likely to be in the extant tables  security axiom never ever store plain-text passwords in the database  with this in mind salt suddenly makes a big difference because of this salt the attacker’s pre-calculated hashes the aforementioned rainbow tables are of no value   salts aren t added to make guessing a password harder on the front end they re added to make sure that the storage of the password doesn t leak extra information   in fact the salt is generated randomly for each password and stored without any form of obfuscation alongside the password  passwords are never stored in a database only the hash is stored md5 sha1 sha2* etc  the md5 of  password  is always 286755fad04869ca523320acce0dc6a4 if you just stored the md5 in the password database you can just look for that string and know that the password there is  password   by adding a salt of  84824  the sum becomes 2ca20e59df3a62e5dc4a3a0037372124 but if you got another database or another user uses the same password they may have a random salt of  8999  giving 4e7a210a07958cfe24138a644cbb7f84  the point is that if an attacker were to get a copy of the password database the password hashes would be meaningless you wouldn t even be able to tell if 2 or more users were using the same password  edit  in comparison - the mathematical formula you apply can be reversed if you choose a salt then xor the salt with the password hash then the attacker can just undo the xor operation and get the original hash at which point rainbow tables are extremely useful  if you think the mathematical formula can t be reversed there s a chance that you re actually losing data and that you re mapping multiple passwords to the same final hash this actually multiplies the chance that the attacker will find a password for the hash since any of the appropriate passwords will work  if you re xor ing and keeping the xor value secure then it s just an extra secret that needs to be kept somewhere and divulging that secret effectively loses all your passwords again due to rainbow tables with salts there are no additional secrets the operation cannot be reversed but can be repeated and each password needs to be attacked individually  edit of course this is now thoroughly relevant  i just logged in as you   the purpose of the salt is not just to obfuscate the password but to add to its length it doesn t help much that the hacker guesses that there is a salt if he doesn t know what it is if the salt doubles the length of the password that raises the number of possible words to the second power your xor method doesn t do that  1-2-3-4-5? that s the stupidest combination i ve ever heard of in my life! that s the kinda thing an idiot would have on his luggage!  ahh spaceballs..  anyhow yeah if you re using 123456 as your salt 007 security may not be as good as it can be still you re making it harder you re eliminating dictionary attacks as a means to crack it unless they know the salt.. sure it could still be brute forced but the honest truth is that nothing is unhackable everything you do is all about making it harder to hack because impossible to hack is impossible to do  i can t speak to the math behind the question but i ll use a phyical metaphor  just because i know that the lock on the door knob to my house can be defeated with a bump tool or a blowtorch i still lock my door  and i m in an apartment building with yet another door to enter the building and i lock that one too although you could say that door is a waste of time because many people have keys to it and the often prop it open  security is a set of concentric defenses some more interesting that others  it is a judgement call about which defenses are more work than benefit e.g hashing + salt + rot13 would probably add more work than benefit,2009-04-28 12:26:43.243 UTC,1639,797626,i don t claim to be an expert in security but it seems to me that adding a salt doesn t really make a huge difference  for example if the password of the user is john1970 and the salt is 123456 this means that the password is 123456john1970 while this makes things harder for an attacker if using a dictionary attack e.g rainbow tables the attacker could very possibly guess that the first part is a salt i find using non-standard methods like xoring with some key or applying a few simple mathematical operations to the codes of the characters far more effective i know most of you won t probably agree with me but but this seems to make more sense to me  your opinion?   duplicate     need some help understanding password salt    non-random salt for password hashes,0.018913971934106162,31,so,hash|passwords|security,is using a  salt  all that good,4,leak|attacks|weakness|protection,0.4271799623966217
10103,you did a lot of research before posting the question i cannot answer much aside the resources-question which is i use applied cryptography be menezes/oorschot for almost everything i ever wanted to know on topics of cryptography including hashes  maybe you ll find a copy at your universities library good luck  it is about an attack scenario the difference lies in the choice of input in multi-collision there is  free choice of both inputs  2nd preimage is about ﬁnding any second input which has the same output as  any speciﬁed input   when a function doesn t have multi-collision resistance it may be possible to find collision for some kind of messages - not all of them so this doesn t imply 2nd preimage weakness,2009-07-29 08:59:12.01 UTC,541,1198900,what is the difference between a multi-collision in a hash function and a first or second preimage     first preimage attacks  given a hash h find a message m such that  hashm = h     second preimage attacks  given a fixed message m1 find a different message m2 such that   hashm2 = hashm1     multi-collision attacks  generate a series of messages m1 m2 .. mn such that  hashm1 = hashm2 = .. = hashmn    wikipedia tells us that a  preimage attack  differs from a collision attack in that there is a fixed hash or message that is being attacked  i am confused by papers with which make statements like      the techniques are  not only efficient to search for  collisions but also applicable to  explore the second- preimage of md4.  about the second-preimage attack they  showed that a random message was a  weak message with probability 2^–122  and it only needed a one-time md4  computation to find the  second-preimage corresponding to the  weak message     the second-preimage attack on md4    if i understand what the authors seem to be saying is that they have developed a multi-collision attack which encompasses a large enough set of messages that given a random message there is a significant though extremely small chance it will overlap with one of their multi-collisions  i seen similar arguments in many papers my question when does an attack stop being a multi-collision attack and become a second preimage attack.    if a multi-collision collides with 2^300 other messages does that count as a second preimage since the multi-collision could be used to calculate the pre-image of one of the messages it collides with? where is the dividing line 2^60 2^100 2^1000?    what if you can generate a preimage of all hash digests that begin with 23? certainly it doesn t meet the strict definition of a preimage but it is also very certainly a serious flaw in the cryptographic hash function    if someone has a large multi-collision then they could always recover the image of the any message which hash collided with the multi-collision for instance  hashm1 = hashm2 = hashm3 = h  someone requests the preimage of h and they respond with m2 when does this stop being silly and becomes a real attack?    rules of thumb? know of any good resources on evaluating hash function attacks?   related links     hash collision q&amp;a     cryptographic hashes    the ehash main page,0.036968576709796676,20,so,cryptanalysis|cryptographic-hash-function|cryptography|hash-collision,what is the difference between a multi-collision and a first or second pre-image attack on a hash function,3,flaws|attacks|weakness,0.42711740732192993
7439,secure password storage alone does not help against brute forcing trivial passwords if all the attacker has to do is try 100 passwords to get to the users account then slow password verification is not a real problem apart from that password verification cannot be too slow because otherwise a dos on the system can be created by just trying to log in  regarding your example if a password verification takes 0.05 seconds then the system can only verify the login of 20 users within a single second for many use cases this is too slow on the other hand the attacker only needs 5 seconds to brute force 100 passwords and at the same time makes the system unusable for others i.e dos  if instead the password verification takes only 0.01 second but there is a limit of only 3 attempts within a 30 seconds for a single user account than the attacker would need 1000 seconds to try 100 passwords and at the same time the system can handle other users without problem  apart from that the common password hashing is not intended to deal with brute force attacks against a single account the salting and hashing complexity are instead intended to make it impossible to crack passwords en masse  hashing is reasonably fast so it s perfectly feasible to hash a few thousands of passwords per second of course there are hashes that are designed to be seriously slow and hashes that are designed to be secure and efficient however if you choose a hash that is too slow and you have a lot of users then you might stall legit login attempts because your server can t handle the load      simply adding a 200ms delay to each password verification achieves a similar protection in the login form   yes but a delay is usually a sleep which means the executing process or fibre gets blocked for 200ms and if if you make a thousand brute-force requests per second then you ll either have thousand processes hanging or a thousand fibres hanging - none of which is ideal also a delay does actually pretty much nothing against brute-force requests since requests occur in parallel if i can test 1000 passwords then i don t care about that 200ms delay because it s just going to delay the first response if you throw a thousand rocks down a cliff and they take 200ms to hit the ground that 200ms does not limit the amount of rocks that you can throw - it just increases the delay until the first rock hits the ground,2018-11-09 12:12:00,760,197325,for authentication controls owasp gives advice to prevent brute-force attacks by  locking out account  and i see this kind of advice in several places blocking by source ip after failed logins blocking by account ...  why ?  i mean two cases here     either you securely store your passwords in database with costly verification bcrypt argon2 ...  in this case your passwords are supposed to resist to offline attack when the attacker has direct read access to the database and i think they will  if password verification takes on server side ~ 0.05s with a decent hardware and you impose a password length of 7 character and forbid common password included in a potentially big list it will take on average   to decrypt each password assuming users chose passwords of 7 characters in   unless your threat model imply attacker having a really big computing power i find it enough and online attack brute-force using the login form is slower than offline attack  is there still a need to block the attacker if its bruteforce attempts are bound to fail for quite a long time ? and yes the brute-force attempt will be a kind of ddos but this has to be mitigated by general anti-ddos techniques which are not specific to login forms    either you do not store your password securely fast-to-compute hashes no salt no hashes at all ...  this is a problem and you should first consider securely storage of passwords and if you can t software legacies ... simply adding a 200ms delay to each password verification achieves a similar protection in the login form but not in the case of database leak / offline attack and is far more simple to implement    in none of these 2 alternatives i see blocking brute-force attempts a good solution it adds complexity and potentially creates dos vulnerabilities,0.030263157894736843,23,sse,authentication|brute-force|passwords,why is it still necessary to block brute-force attacks when passwords hash verification requires significant work,7,ddos|leak|owasp|attacks|protection|vulnerability|denial of service,0.426704466342926
7390,over the years the recommended password hashing algorithms have changed several time as vulnerabilities and unexpected technological advances have occurred  there s no 100% guarantee that a hash which is safe today will still be safe in 10 year  by protecting the hashes from being accessed by an attacker you mitigate your risk of compromise in the case where a hash is captured and it is cracked in the future  this is also a reason why multiple organizations require passwords to be changed on a regular basis  there s been many large attacks where password databases get dumped and eventually cracked  rockyou is one of the most notable ones it s wordlist is included in all kali instances now  hashes simply slow down an attacker  while some people will look at the math and say oh it s totally safe it would take x million years to crack this that s not always realistic when we see new vulnerabilities and shifts in computing paradigms all the time  safe password storage algorithms have two main features they re one-way and they re slow   being one-way means that given the output you cannot efficiently compute the input efficient means anything faster than trying all inputs until you get a matching output  being slow means that computing an output the stored value from the input takes a while because they re one-way you need to try inputs and if trying an input takes ten billion calculations on your cpu then it s going to take a while before you checked all possible passwords   but what if your password is in the top 10 of worst passwords like admin or 123456? then an attacker is going to guess that within a few attempts no matter how strong your hashing algorithm is  therefore  people with weak passwords will always be at risk if their hash is known to an attacker that s why we keep them secret   when we log in to a system we don t want to wait 5 minutes for the server to complete the password algorithm so those algorithms are still quite fast something like 0.1 seconds because computers get faster over time we also need to increase the number of computations done so that it still takes an attacker with modern hardware a long time or maybe we want to upgrade to a different algorithm that protects against a certain kind of asic or gpu cracking program in those cases if the old hash is known to an attacker they can crack the user s password more easily so  additionally keeping the hashes secret as long as possible we also have the most up-to-date protection at the moment the hashes get hacked   some people include pepper in their password hashes see  this question  or  wikipedia  in which case it would be perfectly safe to publish the password hashes  so long as the pepper value remains a secret  but then the question is why would you? there is usually no advantage to publishing the hashes when the pepper value leaks attackers can start cracking the oldest hashes you ever published if you protect the hashes instead of posting them publicly attackers first have to obtain both the pepper and the user database before they can start cracking anything  omitting this precaution of protecting password hashes should never be done without the user being warned of the risk one example of this is a  tripcode  where the hash gets published as proof of identity without needing a database this is the only case that i can think of where publishing hashes of memorized secrets is a good idea other hashes such as signatures of software are of course a different topic because those do not function as passwords  i agree all the answers written before me but would like to answer this question briefly       we have to protect the hashes because that s what all hackers or pentesters want to get as to crack the hashes and find the passwords through brute-forcing dictionary attacks and rainbow tables if you make easy to access them to your password hashes you will make them very happy for sure   if you protect the hashes you protect your  weak  passwords  added to the answers above some weak protocols are vulnerable to pass the hash attack where all the attacker needs to login is username and the hash of the password the attacker doesn t need to brute-force the hash so better not to take the risk of publishing hashes,2018-10-21 17:42:05,782,196114,why should we protect the access to the password hashes? and under what conditions can this precaution be omitted,0.03964194373401535,31,sse,hash|passwords,why should we protect access to password hashes,7,leak|attacks|pentest|weakness|protection|vulnerability|weak password,0.42521804571151733
36515,,2019-04-23 15:39:59.787 UTC,79,55814837,we have been studying deadlocks in class and we learned with a general graph reduction is omn! if we find any reduction that fully reduces the graph we know that no deadlock exists  can we gather any information about coupling based on the ratio of deadlock reductions versus successful reductions?  it seems to me like if 1 of out 10000 reduction completed it would indicate a tightly coupled system,0.05063291139240506,4,so,deadlock|operating-system,what else can we know from rag reduction,1,deadlock,0.42488786578178406
24454,online brute-force attacks against a live system are not viable as it’s simply too slow limited bandwidth latency throttling perhaps captchas etc one could try a dictionary attack but probably only with a very short list of passwords  but for offline password attacks where the attacker is in possession of the password hash the only limiting factor is the hardware and software of his own system however often brute-force attacks are still viable only against high-value targets due to cost-benefit ratio  not only is it achievable with modern approaches using gpus and fpgas it s an incredibly viable approach note this doesn t necessarily apply to client-server models however if you have encrypted data protected by something such as pbkdf2 it is possible with enough computing power that the password and therefore key could be recovered  the appropriate approach depends on what it is you re trying to attack attacking websites is much more difficult as without a distributed attack the site can simple throttle its responses slowing your attack time,2014-03-18 04:40:52.923 UTC,318,22470537,"i am trying to penetration test one of my websites.username and password is unknown to me.i am trying to break username and password using brute force attack  although,this strategy should be straight as i need to keep generating a new alphanumeric combination of variable length each time and posting it using some self written program  but this strategy needs a lots of processing time and power my simple doubt is that is this strategy good enough for username and password breaking or some other task is expected to be done  i have heard a lot about dictionary attack,but that too requires a predefined and pre-expected lists of usernames and passwords  should i go for brutusbut it didn t worked for me or a self written program?what is the right way of enlisting the username and passwords",0.04716981132075472,15,so,brute-force|dictionary-attack|passwords|security,is bruteforce attack really an achievable thing,4,attacks|protection|penetration test|dictionary attack,0.42461156845092773
11038,"for md5 specifically? yes  several years ago an article was published on an exploit of the md5 hash that allowed easy generation of data which when hashed gave a desired md5 hash well what they actually discovered was an algorithm to find sets of data with the same hash but you get how that can be used the other way around you can read an overview of the principle  here  no similar algorithm has been found for sha-2 although that may change in the future  yes what you re talking about is called a collision a collision in any hashing mechanism is when two different plaintexts create the same hash after being run through a hashing algorithm  in one sense this is possible if you have strings that are longer than the hash itself then you will have collisions so such a string will  exist   however finding such a string would be equivalent to reversing the hash as you would be finding a value that hashes to a particular hash so it would not be any more feasible than reversing a hash any other way  finding a message that matches a given md5 hash can happen in three ways   you guess the original message for passwords and other low entropy messages this is often relatively easy that s why we use use key-stretching in such situations for sufficiently complex messages this becomes infeasible  you guess about 2^127 times and get a new message fitting that hash this is currently infeasible  you exploit a pre-image attack against that specific hash function obtained by cryptoanalyzing it for md5  there is one with a workfactor of 2^123  but that s still infeasible   there is no efficient attack on md5 s pre-image resistance at the moment  there are efficient collision attacks against md5 but they only allow an attacker to construct two different messages with the same hash but it doesn t allow him to construct a message for a given hash  yes it is possible to come up with a collision since you map from a larger space to a smaller this is something that you can assume to happen eventually actually   is already considered as broken in this respect from  wiki        however it has since been shown that md5  is not  collision  resistant;[3] as such md5 is not suitable for applications like ssl  certificates or digital signatures that rely on this property in  1996 a flaw was found with the design of md5 and while it was not a  clearly fatal weakness cryptographers began recommending the use of  other algorithms such as sha-1—which has since been found also to be  vulnerable in 2004 more serious flaws were discovered in md5 making  further use of the algorithm for security purposes  questionable—specifically a group of researchers described how to  create a pair of files that share the same md5 checksum.[4][5] further  advances were made in breaking md5 in 2005 2006 and 2007.[6] in  december 2008 a group of researchers used this technique to fake ssl  certificate validity,[7][8] and us-cert now says that md5 should be  considered cryptographically broken and unsuitable for further  use.[9] and most u.s government applications now require the sha-2  family of hash functions.[10",2012-07-31 20:54:08.767 UTC,574,11748848,i know it s not possible to reverse an md5 hash back to its original value but what about generating a set of random characters which would give the exact same value when hashed? is that possible,0.017421602787456445,10,so,md5|security,md5 hash reversing,5,flaws|exploit|attacks|weakness|vulnerability,0.42399290204048157
1543,no  the whole principle of modern cryptography is that you should  automatically assume  that the attacker knows your crypto scheme you re treating your scheme as secret which is a bad idea  stick to normal bcrypt / pbkdf2  applying a transform on passwords before hashing them makes no direct harm to security as long as it is  injective  two distinct passwords before apply the transform shall still be distinct once both are transformed your character shifting is fine for that if you do it properly i.e beware of transforming a byte of value 255 into a byte of value 0 which will truncate the password   indirectly  there is a bit of harm in that your extra transform is extra code thus extra complexity and complexity is always bad for security moreover if the transform is computationally expensive then it comes at odds with the iteration count used in pbkdf2/bcrypt i.e you have less cpu available so you must use a lower iteration count  a transform like the one you envision makes any good to the security only insofar as the attacker is not aware of it i.e the attacker did not do his homework basic low-power attackers who just got lucky with a sql injection attack may indeed lack the knowledge but these attackers are not the scary ones your extra transform will not deter strong attackers and that s strong attackers you should worry about  transforms applied  after  hashing have no influence whatsoever on security except if you are using something like extra hashing in which case you are just trying to build a custom hash function which as usual is a bad idea so don t do it the proper way to use your cpu is not to waste it on voodoo character shifts and other rituals instead use your cpu to have a bcrypt/pbkdf2 iteration count as high as is tolerable for your overall application performance  salting passwords does obscure these patterns you basically suggest salting two times once with your homemade salt and afterwards within bcrypt  this would only produce additional security in a scenario where you attaker doesn t know your homemade salt but does know the unique salt that you use with bcrypt   such a scenario shouldn t happen the attacker should only know your unique salt for bcrypt if he has access to your whole system and then he also knows your homemade salt   in general it s best practice to trust the encryption function instead of trying to implement your own one in this case it means trusting bcrypt to salt properly instead of trying to get salting right yourself   what you are doing with shifting the characters is adding a secret to the hashing process an attacker has to know or has to find out what you did before you calculated the hash otherwise he cannot use a dictionary to find the original password  there are better ways to add such a server-side secret you can encrypt the already hashed password with a block cypher two-way encryption to get your key an attacker not only needs read access to your database sql-injection additionally he must gain privileges on the server to read the key  ➽ with encrypting the password-hash you can prevent dictionary attacks as long as the key remains secret the key remains secret as long as the attacker has no privileges on the server  this is the same advantage a pepper can give you but in contrast to the pepper encrypting the hash allows to exchange the key should this be necessary  remember that clever brute force + dictionary attacks rely on being able to try millions of guesses a second properly implemented bcrypt / pbkdf2 / scrypt will require several tenths of a second per guess thus making brute force impractical on a large scale  the ars technica article makes it clear in the first page that these attacks are being performed against unsalted md5 hashes  so if you want to protect against these attacks choosing a high iteration count for your key derivation function is more important than any clever changes to the password or hash,2013-10-17 15:38:25,793,44018,assume that i m using bcrypt with a unique salt or some other  best practice  to hash user passwords before storing them in my database   is there any security advantage to be gained by character-shifting the password before encrypting?  i.e        is there any security advantage to be gained by character-shifting the  hash  after encrypting excluding the  ? i.e    if there s no advantage to be gained is there any  loss  of security by doing this?  or is it a wash?   in light of the rise in  attacks based on standard password patterns  does obscuring these patterns help at all,0.025220680958385876,20,sse,passwords,altering passwords before storing,3,attacks|protection|sql injection,0.42391353845596313
21472,most of the worry about using something like md4 for a password is related less to currently known attacks than to the fact that once it has been analyzed to the point that collision generation is easy it is generally presumed to be considerably more likely that somebody will be able to use that knowledge to create a preimage attack -- and when/if that happens essentially all possible uses of that hash function become vulnerable  the answer entirely depends on what you re using it for if you need to prevent somebody producing a collision with a few milliseconds i d be less worried than if you need to prevent somebody producing a collision within a few decades  what problem are you actually trying to solve?  when you don t care whether it s safe or not  seriously it doesn t take any extra effort to use a secure hash function in pretty much every language and performance impact is negligible so i don t see why you wouldn t   [edit after actually reading your question]      according to schneier a hash function vulnerable to a collsion attack can still be used as an hmac i believe this is because the security of an hmac is dependant on its secret key and a collision cannot be found until this key is obtained   actually it s essentially because being able to generate a collision for a hash does not necessarily help you generate a collision for the  hash-of-a-hash  combined with the xoring used by hmacs     does it then become safe to use a very weak message digest like md4 for passwords if a salt is perpended to the password?   no not if the hash has a  preimage attack  which allows you to prepend data to the input  for instance if the hash was   we d need a preimage attack which allows us to find pass2 such that    there have been append attacks in the past so i m sure prepend attacks are possible  download sites use md5 hash as a checksum to determine if the file was corrupted during download and i would say a broken hash is good enough for that purpose  lets say that a mitm decides to modify the file say a zip archive or an exe now the attacker has to do two things -    find a hash collision and create a modified file out of it  ensure that the newly created file  is also  a valid exe or a zip archive   with a broken hash 1 is a bit easier but ensuring that the collision simultaneously meets other known properties of the file is too expensive computationally    this is totally my own answer and i could be terribly wrong   the only time it is safe to use a broken hash function is when the consequences of a collision are harmless or trivial e.g when assigning files to a bucket on a filesystem  actually collisions are easier than what you list on both md5 and sha-1 md5 collisions can be found in time equivalent to  2 26.5   operation where one operation is the computation of md5 over a short message see  this page  for some details and an implementation of the attack i wrote that code it finds a collision within an average of 14 seconds on a 2.4 ghz core2 x86 in 64-bit mode  similarly the best known attack on sha-1 is in about  2 61   operations not  2 69   it is still theoretical no actual collision was produced yet but it is within the realm of the feasible  as for implications on security hash functions are usually said to have three properties   no preimage given  y  it should not be feasible to find  x  such that  hx = y   no second preimage given  x 1   it should not be feasible to find  x 2   distinct from  x 1   such that  hx 1  = hx 2    no collision it should not be feasible to find any  x 1   and  x 2   distinct from each other such that  hx 1  = hx 2     for a hash function with a  n -bit output there are generic attacks which work regardless of the details of the hash function in  2 n   operations for the two first properties and  2 n/2   operations for the third if for a given hash function an attack is found which by exploiting special details of how the hash function operates finds a preimage a second preimage or a collision faster than the corresponding generic attack then the hash function is said to be broken  however not all usages of hash functions rely on all three properties for instance digital signatures begin by hashing the data which is to be signed and then the hash value is used in the rest of the algorithm this relies on the resistance to preimages and second preimages but digital signatures are not per se impacted by collisions collisions may be a problem in some specific signature scenarios where the attacker gets to choose the data that is to be signed by the victim basically the attacker computes a collision has one message signed by the victim and the signature becomes valid for the other message as well this can be counteracted by prepending some random bytes to the signed message before computing the signature the attack and the solution where demonstrated in the context of x.509 certificates  hmac security relies on an  other  property that the hash function must fulfill namely that the compression function the elementary brick on which the hash function is built acts as a  pseudo-random function  prf details on what a prf is are quite technical but roughly speaking a prf should be indistinguishable from a  random oracle  a random oracle is modeled as a black box which contains a gnome some dice and a big book on some input data the gnome select a random output with the dice and writes down in the book the input message and the output which was randomly selected the gnome uses the book to check whether he already saw the same input message if so then the gnome returns the same output than previously by construction you can know nothing about the output of a random oracle on a given message until you try it  the random oracle model allows the hmac security proof to be quantified in invocations of the prf basically the proof states that hmac cannot be broken without invoking the prf a huge number of times and by huge i mean computationally infeasible  unfortunately we do not have random oracles so in practice we must use hash functions there is no proof that hash functions really exist with the prf property right now we only have candidates i.e functions for which we cannot prove yet that their compression functions are not prf   if  the compression function is a prf  then  the hash function is automatically resistant to collisions that s part of the magic of prf  therefore  if we can find collisions for a hash function then we know that the internal compression function is not a prf this does not turn the collisions into an attack on hmac being able to generate collisions at will does not help in breaking hmac however those collisions demonstrate that the security proof associated with hmac does not apply the guarantee is void that s just the same than a laptop computer opening the case does not necessarily break the machine but afterwards you are on your own  in the  kim-biryukov-preneel-hong  article some attacks on hmac are presented in particular a forgery attack on hmac-md4 the attack exploits the shortcomings of md4 its weaknesses which make it a non-prf variants of the same weaknesses were used to generate collisions on md4 md4 is thoroughly broken some attacks generate collisions faster than the computation of the hash function itself ! so the collisions do not imply the hmac attack but both attacks feed on the same source note though that the forgery attack has cost  2 58   which is quite high no actual forgery was produced the result is still theoretical but substantially lower than the resistance level expected from hmac with a robust hash function with an  n -bit output hmac should resist up to  2 n   work factor  n = 128  for md4  so while collisions do not  per se  imply weaknesses on hmac they are bad news in practice collisions are a problem for very few setups but knowing whether collisions impact a given usage of hash functions is tricky enough that it is quite unwise to keep on using a hash function for which collisions were demonstrated  for sha-1 the attack is still theoretical and sha-1 is widely deployed the situation has been described like this the alarm is on but there is no visible fire or smoke it is time to walk towards the exits -- but not to run  for more information on the subject begin by reading the chapter 9 of the  handbook of applied cryptography  by menezes van oorschot and vanstone a must-read for the apprentice cryptographer not to be confused with applied cryptography by b schneier which is a well-written introduction but nowhere as thorough as the handbook,2010-05-22 19:27:49.43 UTC,1901,2889473,it is trivial to use a secure hash function like sha-256 and continuing to use md5 for security is reckless behavior however there are some complexities to hash function vulnerabilities that i would like to better understand   collisions have been  generated for md4 and md5   according to nist md5 is not a secure hash function it only takes  2 39  operations to generate a collision  and should never be  used for passwords   however sha-1 is vulnerable to a  similar collision attack  in which a collision can be found in 2 69  operations whereas brute force is 2 80   no one has generated a sha-1 collision and  nist still lists sha-1 as a secure message digest function    so when is it safe to use a broken hash function? even though a function is broken it can still be big enough  according to schneier  a hash function vulnerable to a collision attack can still be used as an  hmac  i believe this is because the security of an hmac is dependent on its secret key and a collision cannot be found until this key is obtained once you have the key used in an hmac it s already broken so it s a moot point what hash function vulnerabilities would undermine the security of an hmac?    let s take this property a bit further does it then become safe to use a very weak message digest like md4 for passwords if a salt is prepended to the password? keep in mind the md4 and md5 attacks are prefixing attacks  and if a salt is prepended then an attacker cannot control the prefix of the message if the salt is truly a secret and isn t known to the attacker then does it matter if it s appended to the password?  is it safe to assume that an attacker cannot generate a collision until the entire message has been obtained?   do you know of other cases where a broken hash function can be used in a security context without introducing a vulnerability?   please post supporting evidence because it is awesome,0.026301946344029457,50,so,cryptography|hash|message-digest|security,when is it safe to use a broken hash function,6,attacks|exploit|forgery|weakness|vulnerability|man in the middle,0.4219851791858673
4665,really you want to ensure that the comparison function takes the same length of time regardless of the success or failure of the comparison  functions such as strcmp etc may shortcut this and return failure on the first byte that does not match  likewise with higher level languages with string objects where == and != can be used we are reliant on the compilers implementation  so to ensure constant time a good solution is to always perform a byte by byte comparison of all the bytes of the hashed value and then return true/false based on that comparison  for example,2016-06-30 12:24:10,292,128774,assuming we want to protect against  timing attacks on our hashed passwords  because even  to know the hash may give to the attacker a significative advantage  then we may want to perform a constant-time or fixed-time string comparison for our hashes  however  constant-time comparison is not a simple task  as we may expect because compiler may be  against  us and  simple random delays will just stop the most naive attacker   with all these premises we may mitigate this issue does we need to? passwords are often stored in databases/durable medium and in any real-world system this will add  noise  using this pseudo-code   note that string comparison on hashed password is still vulnerable to a timing attack but chances to leak useful information to an attacker should be lower and this algorithm is easily applicable in most languages - even those without a built-in constant-time password/hashes comparison  same reasoning is also applicable if we don t have an hashed password but directly plain-text !!! like this   do we still want/need to use a better constant-time comparison function in any real-world non-academic scenario,0.03424657534246575,10,sse,password-cracking|timing-attack,short integer hashes to mitigate timing attacks on passwords,5,leak|attacks|protection|timing attack|vulnerability,0.4218747317790985
12604,when preventing timing attacks is it safe to exit on different lengths?   generally no but it s really dependent on the situation   the function itself   this function will leak information with a timing attack regardless of the length comparison because it s running time is always dependent on the length of it s input  with the length compare the running time will change when both inputs are the same length  without the length compare the running time will change based on the length of the shorter input beause of   once the attacker controlled input exceeds the length of the other input running time will remain constant  the running time of this function is so short though unscientific testing shows less than 0.1ms for 32 bytes of input that in a real life situation it would fairly difficult for an attacker to take advantage of this because of other factors such as variance in network latency the attacker would probably need to already be on the machine where the code is executing to really make use of this weakness   concerning flask-bcrypt   in the context of flask-bcrypt though this function is only used for comparing hashes not direct user input because the hash length that bcrypt outputs is fixed the   should never actually execute hence no timing attack exists for this function when used with bcrypt  flask-bcrypt uses this function for checking equality because the running time for normal string comparison in python == will change based on the content of the strings consider two nearly identical strings of the same length if the first character of the two strings are different == comparison will complete faster than if the difference occurs at the last character of the strings  i would argue though that constant time string comparison is really unnecessary in this case the goal of the attacker is to deduce the stored hash value based on processing time the attacker needs to know what hash value is produced by their input to achieve this the only way to know what hash is being produced though is for the attacker to know the workfactor and salt and if they have this information then they already have the hash as well because they re all stored together in which case there s no reason to perform the attack to begin with,2013-08-29 10:07:50.937 UTC,446,18508026,in some libraries for example  flask-bcrypt  we can see that the code exits early if the two strings are different lengths     is this really safe? surely this reveals to an attacker that the two strings were different lengths early and leaks information,0.033632286995515695,15,so,bcrypt|hash|security,when preventing timing attacks is it safe to exit on different lengths,3,leak|attacks|weakness,0.4201875627040863
23327,when creating the hashed password you should use double salt  create a salt random md5 or sha1 then use format something like sha1--$password--$salt-- and then store hashed password and salt in database   then when authenticating you recreate the hash from --$pass--$salt-- string and compare it to the pass stored in db  the salt prevents someone from getting a copy of your encrypted password database and mounting an offline attack against  all  of the passwords at the same time it doesn t prevent attacks against a single password   you might enjoy reading the original unix password security article it does a very good job explaining what a salt is and why we have them  http://portal.acm.org/citation.cfm?id=359172   salt is randomly generated for each user but it s saved somewhere in the database you look up the salt for the particular user and use it to authenticate the user  the point is since salt is different for each user you cannot use a prebuilt dictionary of hashes to map the hashed passwords to clear text  rainbow attack   it doesn t defeat the purpose of the unique salt to store it the point of a unique salt is to protect your  entire  users repository from attack not a given individual user if an attacker compromises your database and is determined enough to crack a particular user s account they will there s nothing we can do about this but they would have to spend an inordinate amount of computer time doing so - enough that it would not be feasible to spend that much time on  each  user - thus protecting all your users contrast this with using the same salt for all users - once the attacker has the salt the same tables/processes can be re-run against every user in a relatively short time  you do not need a separate salt for every password  the purpose of salting is to resist rainbow tables -- you convert a candidate password into a new string that has your salt in it since the salt is some private string only you possess knowing the hash of a salted password will not help an attacker who has a run of the mill rainbow table    a clever attacker can try to build a custom rainbow table just for your service by creating an account and changing his password to observe what the resulting hash is if the salt is the same for every user then when he sees that the hash xyz123 corresponds to apple and notices that another user s hash is also xyz123 he can conclude that that user s password is apple  this is the point where most people decide to store a unique salt for each user   however   this is unnecessary    you already have a unique string for each user -- the username  it s not secret so it is not a good salt however the concatenation of the username and a global secret salt is both secret and unique if you store the hash of username+salt+password you only need to know the single global salt value at lookup time  it s true that this poses a greater risk if someone leaks the single global salt but it s a technique worth considering,2009-09-12 20:28:02.813 UTC,682,1416060,i ve been reading up on the benefits of salting and hashing passwords but one thing still eludes me..   when i provide a random salt for each user how do i then know what the salt was when i try to authenticate them to login?  so if i do.   hashpw = pw.randomnumber  i could store the random number in the database but that seems to kill the entire point of adding the salt. doesn t it? i could also use a non random number for each salt but then that also kills the point of the salt because if they figure it out they have all my users passwords..  i just started learning php and mysql and abstract things like this have been confusing me  thanks,0.017595307917888565,12,so,database|hash|password-protection|passwords,if i make the salt random for each user how do i authenticate them,3,leak|attacks|protection,0.4191300570964813
24714,the primary function of salts is to defend against dictionary attacks versus a list of password hashes and against pre-computed rainbow table attacks    salt cryptography   basically adding a little bit of unknown data into the hash prevents an attacker from precomputing all hashes for a given dictionary and then just looking up in the table to find the unhashed value  usually to encrypt sensitive data a salt is used  what this means is your sensitive data say password is concatenated with a stringsalt encrypted and then stored  this protects it against table attacks in which an attacker has most dictionary words and their popular algorithm encryption md5 sha1 etc strings in a table so if he were to have access to the db he would be able to decipher all of your sensitive data  using a salt makes it harder for the attacker since - the attacker needs to know the algorithm used with which the salt was added and would need a specific dictionary for that specific salt making his life harder,2014-08-12 19:39:34.397 UTC,223,25272722,i read the man crypt and didn t understand what the phrase below means salt is a two-character string chosen from the set [a-za-z0-9./] this string is used to perturb the algorithm in one of 4096 different ways,0.04932735426008968,11,so,c|crypt|salt,how does salt works on crypt function in c,3,attacks|protection|sensitive data,0.4184456169605255
12992,an attacker is  allowed  to know the salt - your security must be designed in a way that even with the knowledge of the salt it is still secure   what does the salt do ?   salt aids in defending against brute-force attacks using pre-computed rainbow-tables salt makes brute-force much more expensive in time/memory terms for the attacker calculating such a table is expensive and usually only done when it can be used for more than one attack/password if you use the same salt for all password an attacker could pre-compute such a table and then brute-force your passwords into cleartext..  as long as you generate a new best cryptogrpahically strong random salt for every password you want to store the hash of there is no problem    depending on how you use md5 it is a weakness since md5 is no longer seen as cryptogtaphically secure    if you want to strengthen the security further  you could calculate the hash several times over hash the hash etc. - this doesn t cost you much but it makes a brute-force attack / calculating rainbow-tables even more expensive.. please don t invent yourself - there are proven standard methods to do so see for example  http://en.wikipedia.org/wiki/pbkdf2  and  http://www.itnewb.com/tutorial/encrypting-passwords-with-php-for-storage-using-the-rsa-pbkdf2-standard  and  http://msdn.microsoft.com/en-us/library/system.security.cryptography.rfc2898derivebytes.aspx   using such a mechanism is these days  mandatory  since cpu/gpu time usable for attacks like rainbow tables/brute force etc. is getting more and more widely available see for example the fact that amazon s cloud service is among the top 50 of fastest supercomuters worldwide and can be used by anyone for a comparatively small amount!   splitting the salt into 2 pieces one hardcoded/constant and one unique part is not recommended!   depending on the algorithm used this can be a weakness helping an attacker to break your security i would highly recommend not doing this unless you can prove mathematically that it does not weaken your security which is very hard to do imho  there is no problem at all for an attacker to know your complete salt if you implement the security correctly..   update as per comment    rfc 2898 is an important reference on pbkdf2  in section 4.1 it talks about adding a part to the salt     the salt should contain data that explicitly distinguishes between  different operations and different key lengths   this has imho no relation to what the op is asking the rfc talks about information on key lengths etc being incorporated as an additional part along with the random salt this is very similar to having a file format containing a header section describing specific aspects of the data it contains this does not weaken security and is possibly very usefel in a scenario where interoperability between different systems is needed  in contrast what the op is asking is basically storing only a part of the salt in the db while keeping the other smaller part hardcoded in the application which means that the salt in the db is not complete.. doing this means losing entropy of the salt i.e a 64-bit salt with 8 bits being constant is actually only as secure as a 56-bit salt which in turn leads to weakening the security the salt provides at least for any algorithm i can think off right now.. this contradicts what the op intends increasing security   update 2 as per discussion with owlstead   you can add some secret which can be hardcoded in your application to the plaintext before hashing with pbkdf2 using a unique and random salt which is completely stored in the db.. that could help a bit although it accounts for  security by obscurity  imo,2012-03-07 16:34:10.33 UTC,804,9605463,hi i have a question regarding md5 hash / salting  i read a lot about hashing and salting recently and i understand that i should use a different salt for every single password and that i should not use something like username because the salt should ideally be completely unique but what i don t understand is why it is recommended to store this salt alongside the password in the database?  isn t it bad if the attacker gets hash and salt? i understand that its harder for the attacker because the unique salts ensure that he can t check a single calculated-hash against all passwords but still wouldn t it be better to keep a part of the salt hidden?  i was thinkin about splitting the salt in 2 parts the first part is stored in the database as usual the second smaller part is hardcoded in my application it would be nearly impossible for an attacker to get the complete salt as he would have to infiltrate both applicationserver and database  is this a good solution? the salt would still be unique  but all salts would end with the same sequence,0.026119402985074626,21,so,cryptography|hash|md5|security,md5 hash salting - where to store salt,3,attacks|weakness|hardcoded,0.41780292987823486
44271,from  http://www.strw.leidenuniv.nl/docs/intel/f_ug/ieee_ovw.htm      the underflow exception occurs if the rounded result has an exponent  that is too small to be represented using the floating-point format of  the result   this implies that the error occurrs when the ratio of the dividend and divisor is small enough to exceed the precision of the floating point format rather than any dependence on a specific value such as epsilon   as the denominator approaches zero assuming a non-zero numerator the result of a division will approach infinity as the numerator approaches zero assuming a non-zero denominator the result approaches zero when this value gets small enough an will occurr   if the numerator and denominator are very close in value even if they are very small you can get a useful result so a very small numerator does not necessarily cause an underflow   example   in c# epsilon is 1.401298e-45   epsilon/epsilon == 1.0f  even though the numerator is very very small the result is still a valid float   now if you were to try something like this     will have an order of 1e-83 since 83 far exceeds the maximum single-precision float exponent the value will be clamped to zero this is where the underflow occurrs    this generates a divide-by-zero instead of infinity because the intermediate result stored in   is first clamped to 0 before being used in the second operation  whether you get an underflow or a divide-by zero can depend on the compiler your use and order of parenthisis etc   for instance again in c#   as well as    gives 20971522.0f   however the mathematically equivelant expression   gives infinity,2012-01-13 16:58:06.687 UTC,300,8854285,why is a divide underflow only caused when the divisor is much smaller than the dividend  shouldn t it occur anytime the denominator is close enough to zero regardless of the size of the dividend,0.023333333333333334,7,so,computer-science|divide-by-zero|underflow,divide underflow,1,underflow,0.4164171516895294
2454,"if you re hashing arbitrary data there s no easy way to reconstruct it since it doesn t have a predictable pre-hashing size the reason password hashes that aren t salted are dangerous when leaked is because you re dealing with a very constrained set of possibilities without a salt the password of password will always result in the same hash this allows an attacker to start with common password and see if there are any matches which there often are if you take data of an arbitrary size and hash it you can t reasonably guess what its original content was imagine taking a 1 megabyte file and reducing it to a 16 byte md5 hash without any other information about the data there is no realistic way to turn that hash back into the original information   plain hash is not secure if the attacker plans to retrieve some passwords using a rainbow-table attack or other brute force techniques appending the salt should make the hash reasonably secure even if released to the public  unbreakable hash doesn t make sense if you use a weak password e.g 123456 and don t use salting any strong hash function become weak  you are correct that brute force attacks are feasible especially if the data being hashed comes from a relatively small search space  here is a recent example  where new york cab details were inadequately disguised using a hash from the article     it turns out there s a significant flaw in the approach because both  the medallion and hack numbers are structured in predictable patterns,  it was trivial to run all possible iterations through the same md5  algorithm and then compare the output to the data contained in the  20gb file software developer vijay pandurangan did just that and in  less than two hours he had completely de-anonymized all 173 million  entries   note that it is not the use of md5 that is the problem here any hash algorithm could be brute forced in exactly the same way  it may be possible to render the hashing irreversible by adding salt before hashing however at that point you would do just as well to create a lookup table mapping sensitive data to entirely random values—then there will be no hash algorithm to break  of course this all assumes that there are associations in the data that you wish to preserve after anonymising otherwise the safest approach is to entirely omit or mask any sensitive data",2014-09-19 09:45:55,521,67727,i m having a friendly debate with someone who thinks that a website can safely make public sensitive data about it s users  as long as that data is hashed  don t ask why it s a long and hypothetical story my position is that this opens the data up to brute-force attacks at least and that no hash is truly unbreakable given enough time and resource therefore even hashed data should be protected and kept private who s more right? can private data safely be made visible in public  as long as it is hashed  or not,0.02495201535508637,13,sse,hash|privacy,is it ever safe to publish hashed data,7,leak|flaws|attacks|weakness|protection|weak password|sensitive data,0.4161055088043213
14573,there are 2 recognized vulnerabilities relating to the use of salts   the first is  cwe-759   which states that a salt  must  be used for passwords  the 2nd vulenrablity is much more important  it is  cwe-760  use of a one-way hash with a predictable salt   the salting mechanism that you are purposing is a vulnerability according to cwe-760  a salt should be generated using a large  cryptographically secure pseudo-random number   this number should be base256   a good size would be the same number of bits that the message digest produces  for instance sha256 should have a 256bit salt  both should have the same amount of entropy because they are both susceptible to brute force    in order to break a salt of this size you ll need a rainbow table so large we don t even have a word for it      why not use the hash of the username  as the salt?   a password hash cannot be broken until the salt is retrieved  salts make precomputed attacks more resource intensive  but never impossible  the problem with using a hash of the user name is 2 fold   first of all you are computing 2 message digests which is a waste of resources  from a security perspective this salting mechanism is unsuitable becuase the username for web applications is often  public knowledge    a salt must be a secret  ideally this secret is stored separately from the password hashes   if the salt is stored in the database along side the password hash  then sql injeciton can be used to obtain both values and then a simple dictionary attack can be used to break the hash   to improve the secuirty provided by a salt it should be stored separately from the password hashes such that both must be compromised before the any hash can be broken  this can be accomplished by storing the salts in a separate database  or in a local flat file  keep in mind that mysql s file_priv s could be use do to read this flat file  so make sure this is disabled   if you use a random salt you re pulling from a very large random pool of possibilities  when you re pulling from usernames you re pulling from nowhere near as large of a pool as usernames are usually all lowercase dictionary-word ish and are a subject of constraints that the os/authentication system puts on the usernames must start with a letter no special characters some os s still require up  to 8 char usernames  lots of usernames are standardized or at least popularized root administrator bob mary.. you get the idea.another problem is that usernames aren t usually protected you can see them through apache s user directories anonymous ftp often allows the public directory to group things up by username etc  an attacker could just start by harvesting the usernames and building themselves a very nice list of salts  all this stuff adds up to one problem  higher probability of coming up with a list of salts that works  this gives attackers an ability to do offline pre-calculation of usernames and their possible hashes setting it up for a bruteforce attack  you might want to create a challenge-response mechanism to thwart that  the point of the salt is to prevent the attacker from performing parallel attacks that parallelism must be understood both space- and time-wise roughly speaking this means sharing the attack cost between two or more attacks  for instance consider a non-salted hashed password setting the attacker can hash all words in a dictionary for a cost proportional to the size of the dictionary and check those hashed words with regards to  several  hashed passwords this can be simultaneous the attacker has a list of hashed passwords and wants to crack one or iterative the attacker precomputes his hashed dictionary then uses it as a tool against several passwords in distinct systems either way this is cost sharing  the  salt  is some data which should be somewhat unique to each hashed password instance salting prevents such cost sharing to the extent of the uniqueness of the salt  using the user name or hash thereof as a salt leverages user name uniqueness usually  on a given system at a given time  user names are unique this prevents locally space-wise sharing if the attacker gets a snapshot of all hashed passwords he cannot attack them in parallel with cost sharing he will have to incur the hashed dictionary cost for every attacked password however this does not prevent time-wise sharing the attacker precomputes a hashed dictionary with the salt corresponding to user bob and will regularly try to guess bob s password assuming that bob changes his password on a regular basis e.g because this is mandated by his system administrator this does not prevent either some global sharing there are several -- many -- systems out there with a user going under the name of bob  so using the user name as salt is not bad this is better than using no salt at all but a random salt is still better because it will change even in situations where the the user name is kept unchanged a user changing his password two users on distinct systems with the same name  i m going to take a slightly contrary view  a pure hash isn t the best idea for the reasons given but better than nothing but i think it s an entirely different story if you do a hash of an application-wide salt + username  it s still per-user but not something an attacker can easily guess  obviously you ll want to make sure that the app-wide salt isn t visible to an attacker e.g by reading it from a file that s outside of the app server directory tree  you can extend this a bit further with multiple hashes  that is don t just use   use something like   where h is the hash and    is simple concatenation.  the extra time won t have a noticeable impact on your application but makes the cost to an attacker much higher  i think multiple hashes are a good idea even if you store random hashes in the user authentication table  again it won t have a noticeable impact on your application but will make it much harder for an attacker  the salt helps protect against an attacker using a precomputation or dictionary attack when a salt is used the attacker needs to create a separate dictionary for every salt value however if the salt isn t random you give the attacker an advantage because they can create dictionaries that are more likely than others for example they could create a dictionary using a salt of jsmith or hash of jsmith for this reason it is generally a good idea for the salt to be random   comparing a precomputation attack against a hashusername salt and a random salt let s say for example the attacker decides to create dictionaries for the most common 1000 usernames and say half a dozen different hash algs that s 6000 salts and so 6000 dictionaries if a random 32 bit salt is used that s 2^32 or circa 4.2 billion dictionaries so when the salt space is dramatically reduced like it is by using hashusername precomputed attacks become much more feasible  the reason for salts was to prevent cryptanalysis attacks  unique salt per user means you can t tell if two users have the same password  nondeterministic salt per user means you can t tell if the same username:password is used on two systems  don t try to outclever the salt  if you begrudge the space then don t use them and put your effort into protection your data and the backups! directly,2010-04-29 10:23:07.73 UTC,1612,2736505,in the process of building what i d like to hope is a properly-architected authentication mechanism i ve come across a lot of materials that specify that   user passwords must be salted  the salt used should be sufficiently random and generated per-user  ...therefore the salt must be stored with the user record in order to support verification of the user password   i wholeheartedly agree with the first and second points but it seems like there s an easy workaround for the latter  instead of doing the equivalent of pseudocode here   why not use the hash of the username as the salt?  this yields a domain of salts that is well-distributed roughly random and each individual salt is as complex as your salt function provides for  even better you don t have to store the salt in the database -- just regenerate it at authentication-time  more pseudocode   of course   in the examples above should be something reasonable like sha-512 or some other strong hash.  this seems reasonable to me given what little i know of crypto but the fact that it s a simplification over widely-recommended practice makes me wonder whether there s some obvious reason i ve gone astray that i m not aware of   edit  some appear to not grok what the question is  i no way am i suggesting that no salt be used  referring to therook s edited answer i m familiar with the references noted in those cwe s  the core question i have is why is hashusername a predictable salt?   edit 2  thanks to all those that provided answers biffabacon directly addressed my core question in his 2nd paragraph basically anything you can do to maximize the domain of the salts being used and therefore the hashed passwords being generated is good but there s lots of tasty info in various comments on this question,0.024193548387096774,39,so,authentication|cryptography|encryption|security,can per-user randomized salts be replaced with iterative hashing,5,cwe|attacks|protection|vulnerability|predictable salt,0.4142812490463257
14937,"it s about as strong as an unsalted sha1 hash with two iterations i.e pretty weak  the lack of salt allows an attack to create rainbow tables or simply attack all password hashes in your database at the same time  the low iteration count makes the attack fast since the attacker can simply try more password candidates  you should add a salt and use a slower hashing method such as pbkdf2 and bcrypt the .net class  rfc2898derivebytes  implements pbkdf2 so i recommend using that one  i wouldn t recommend hmacsha1 for database password storage but setting the key to be the same as the password does weaken the usefulness of the key in this purpose the key is supposed to be secret and used to determine if the underlying hashed data has changed  for passwords you should be using a salt+password combination to increase the security of hash algorithms i usually use a salt that is unique to the user but not the same as the password such as the user number or initial registration ip address  also keep in mind that sha1 is no longer recommended as a hashing algorithm  you can reference  msdn  for a clearer understanding     this property is the key for the keyed hash algorithm      a hash-based message authentication code hmac can be used to  determine whether a message sent over an insecure channel has been  tampered with provided that the sender and receiver share a secret  key the sender computes the hash value for the original data and  sends both the original data and the hmac as a single message the  receiver recomputes the hash value on the received message and checks  that the computed hash value matches the transmitted hash value      hmac can be used with any iterative cryptographic hash function such  as md5 or sha-1 in combination with a secret shared key the  cryptographic strength of hmac depends on the properties of the  underlying hash function      any change to the data or the hash value results in a mismatch,  because knowledge of the secret key is required to change the message  and reproduce the correct hash value therefore if the original and  computed hash values match the message is authenticated",2012-01-19 22:35:06.28 UTC,468,8934389,i m working on some c# security code and was about to replace it when i saw that it was using the hmacsha1 class the code is used to hash a password for storing in the database the thing that caught my eye was that it uses the password as the hmac key which is exactly what is computing the hash for so is using the data for both the key and the thing your hashing ok? does this make the security stronger or weaker?  psuedo code,0.017094017094017096,8,so,.net|c#|hmacsha1|security,hmac sha1 using the same value for key and message,3,attacks|weakness|secure coding,0.41412052512168884
14600,whilst in practice the longer one is probably stronger i think there may be potential weaknesses when you get into the nitty gritty of how encryption and ciphers work.. possibly..  other than that i d echo the other responses that the strength-checker you re using isn t taking all aspects into account very accurately  just a thought..  it might be a flaw by the password trength checker - it recognises a pattern.. a pattern is not good for a password but in this case it is a pattern on a complex string.. another reason can be the one pointed out by answer from  wael dalloul    someone can see the repeated text when you type it  any  spies  have two chances of seeing what you type..  the best reason that i could think of comes from the  electronic authentication guide  published by nist it gives a general thumb rule on how to estimate entropy in a password   length is just one criteria for entropy there is the password character set that is also involved but these are not the only criteria if you read shannon s research on user selected passwords closely you ll notice that higher entropy is assigned to initial bits and and lesser entropy to the latter since it is quite possible to infer the next bits of the password from the previous  this is not to say that longer passwords are bad just that long passwords with a poor selection of characters are just as likely to be weak as shorter passwords  i guess because you need to type your password two times by using the keyboard so for that maybe if some one is in front of you can notice it   the algorithm is broken  either uses a doublet detection and immediately writes it off as bad or calculates a strength that is in some way relative to the string length and the repeated string is weaker than the comparable totally random string of equal length  sounds like the password strength checker is flawed  it s not a big issue i suppose but a repeated strong password is not weaker than the original password  my guess is it can generate a more obvious hash  for example abba -> a737y4gs but abbaabba -> 1y3k1y3k  granted this is a silly example but the idea is that repeating patterns in key would make hash appear less random  my guess is that it s simply trivial to check for someone attacking your password trying each password doubled and tripled too is only double or triple the work however including more possibly characters in a password such as punctuation marks raises the complexity of brute-forcing your password much more  however in practice nearly every non-obvious read impervious to dictionary attacks [yes that includes 1337ifying a dictionary word] password with 8 or more characters can be considered reasonably secure it s usually much less work to social engineer it from you in some way or just use a keylogger,2009-08-25 11:42:23.463 UTC,575,1327742,many websites have  password strength checking tool  which tells you how strong your password is  lets say i have   which is always considered super strong but when i do    it is suddenly super weak i ve also heard that when password have just small repeating sequence it is much weaker  but how possibly can the second one be weaker than the first one when it is twice as long,0.020869565217391306,12,so,passwords|security,why are passwords with repeating substrings weak,3,flaws|attacks|weakness,0.41350817680358887
24252,approach 4 use  pkcs#5  pbkdf2 for deriving a key from a pass + a cipher of your choice for encryption using that key preferably somebody else s implementation  and don t forget the salt  you store it together with the encrypted data  it only needs to be 8 bytes or so,2011-06-03 15:28:46.283 UTC,427,6229136,i start with a weak password 8 lower case characters for ex and a file i need to encrypt that file using that password result has to be secure against known attacks  approach 1 i could hash the password using sha-256 and then use the resulting hash and file as inputs to aes-256 giving me an encrypted file i understand that both sha-256 and aes-256 are very fast wouldn t this make the file vulnerable to a brute force attack?   for example could one grab a rainbow table of pre-computed sha-256 hashes and assuming its a really small file and a really weak password try to aes-256 decrypt using each hash from that table in a reasonable time a few months with specialized hardware  approach 2 use bcrypt if i understand correctly bcrypt is better suited for encrypting files than sha-256 + aes-256 since it s key generation scheme has a work factor resulting in a stronger key or am i wrong?  the ruby and python implementations wrappers? that i ve seen focus on using bcrypt as a hashing scheme for passwords not a cipher per se  can i even use bcrypt to hash a weak pass and encrypt the file in one step?  approach 3 use bcrypt to hash the pass use that hash and file as inputs into aes-256 giving me the encrypted file this takes care of the key is too fast to generate problem assuming its a problem. however bcrypt hashes are 448-bits long and aes-256 wants a 256-bit key naive solution is to simply drop the trailing bits of the hash and use that as the key for aes-256 i would not go this route because i don t know enough about cryptography to know what the consequences are  edit  i can t salt the pass since this is for an offline application ie there is no reasonable place to store the salt  i can salt the pass and store the salt unencrypted along with the encrypted file salts are almost inherently public/visible if say a database is compromised purpose of a salt is to prevent a rainbow table attack thanks to nemo bellow,0.01873536299765808,8,so,aes|bcrypt|cryptography|encryption|sha,encrypting a file with a weak password bcrypt or sha-256 + aes-256,4,attacks|weakness|weak password|vulnerability,0.4130462110042572
18693,captchas are also not perfect there are ocr algorithms to programmatically solve them there are also systems which outsource the problem i.e dodgy download sites can give you a popup to solve a captcha but their real goal is not to find out whether you are a human or not but to solve that particular captcha i think there are even factories at places where the human labor is very cheap where people solve captchas 10 hours a day as their normal jobs,2016-04-17 16:11:38.347 UTC,149,36678946,as we all know there was a recent vulnerability on  facebook that was exploited by an indian developer as stated here   brute force in 2016 is very weird facebook applies rate limiting while entering the code for phone  why they are not using captchas ?   isn t the problem be avoided by adding captcha ?  thanks,0.020134228187919462,3,so,brute-force|burp|captcha|facebook|penetration-testing,captcha solution to brute force,3,exploit|vulnerability|penetration test,0.4128582775592804
11070,"yes md5 is severly compromised i would advice you to use a pbkdf2 functionality which would provide a much better security   md5 security    pbkdf2 wikipedia article   i would recommend using a stronger password than a 4 digit one though  think about it why are you hashing passwords? because even if your database is stolen the intruder won t be able to find a password based on the hash but if the space of your passwords is 4 digits 10000 combinations how long will it take to find a password that matches given md5 hash? one millisecond? you will hit the same security vulnerability with any modern hash function md5 isn t considered secure nowadays   what you need is salting with a very long salt for each user create some random data called  salt  and compute hashpassword + salt you aren t obviously storing passwords but you will store hashes and salts per each user  second thought 4-digit password with salt still isn t secure - all you get is that the intruder will have to brute-force password per each user but with 10k key space this is still trivial i don t know any other method that will protect such a short password  i see a number of problems here  first if a four-digit passcode is all that is preventing access to your uber-sensitive data you re in trouble  i hope that there are other security measures in place since brute-forcing 10,000 combinations by hand is trivial much less with some kind of script  second i m not sure you understand the point of hashing the passwords  i doubt you will get a hash collision just from 10,000 possibilities but that is basically irrelevant when the passwords can be brute-forced  all you are accomplishing is a little bit of obfuscation from someone with read access to the database  third the needs of a password hashing algorithm are different than the needs of other hashing algorithms  you need the algorithm to be slow which usually means having to run it repeatedly and you need for there to be salt so that the password cannot be derived from a lookup table  supposedly blowfish isn t bad  i find the  pgcrypto  docs from postgresql have a pretty good explanation  there is an easy method to figure this out based on risk  by definition risk is hazard times probability of the undesired event  in this case you re concerned about the probability of an md5 hash being cracked which is certainly significant  but with a pin that s only 4 digits long the probability of a straight out brute force attack succeeding in one try is only 10 -4  so depending on the value of the data you pretty quickly get to an undesirable risk  in any case the probability of cracking an md5 in one trial is very likely much smaller than 10 -4  so he s probably correct  while md5 is broken these vulnerabilities don t affect password hashing so md5 instead of better hash functions isn t the problem here still i generally recommend using a better hash function  normally use some method to hash slower and add a salt check  wikipedia on key deriviation functions .pbkdf2 and bcrypt are popular choices for kdfs  but i can think of no way to protect a 4 digit password there are only 10 000 different passwords it s trivial to bruteforce even salts and kdfs won t help you  systems using low entropy pins rely on the checking server/hardware never getting compromised so they can lock an attacker out after a few wrong attempts but you can t do that if the attacker gains access to the password hash",2011-06-15 21:34:29 UTC,796,6364680,i ve been in discussion with this security guy he s probably the most i can afford for my new project anyways it is a service that saves sensitive data password pins that can be requested by the user via phone the user has a password 4 digits which he uses to access the sensitive data the security guy told me he would use md5 to hash the password that is used to access the sensitive data here the discussion started as i thought and am quite sure that md5 is too vulnerable since it has been cracked/collisions have been caused    what hashing method should be used to hash passwords that protect sensitive information? i have a feeling that this service might become a high value target for hackers so i m really worried about it i m starting to worry about the overal quality and especcialy security of services the security guy is going to deliver but have no idea where to find others,0.020100502512562814,16,so,hash|md5|security,is it safe to hash passwords that protect sensitive data with md5,5,attacks|protection|vulnerability|sensitive data|sensitive information,0.41244280338287354
19794,sounds like a homework question that is expecting a formula as an answer  way too many things unspecified  in particular a dictionary could be precomputed with all possible salts 2^12 = 4096 not that big for one password  if all 2^24 users used that same one password then every password would be in the dictionary and the question is what is the expected time to do 2^24 lookups into a table of 4096  on the other hand if none of the users passwords are in the dictionary then you will never find the password using a dictionary attack unless you stumble upon a hash collision    probably better asked on   security.stackexchange.com,2012-04-25 15:55:45.977 UTC,270,10319259,i was reading that the designers of unix password algorithm used a 12 bit salt to modify the e-table of the unix hashing function the des supposing i have a system with 2^24 users?   is that ever possible to user dictinary attack? and if so how long would it take? years??  i am really new on computer security  sorry editing i am not sure what unit time i guess i have to assuming bytes per minute depending on my code?   the reason i am asking is for a project where one of the questions states:consider a system with 2^24 users assume that each user is asssigned a salt from a uniform random distribution and that anyone can read the password hashes and salt for users what is expected time to find all users  passwords using dictionary attack?   thanks very much,0.022222222222222223,6,so,cryptography|dictionary-attack|password-protection|salt,what would be the expected time to find all users passwords in unix using dicitionary attack,3,attacks|protection|dictionary attack,0.4121643602848053
179,the would be called a  second preimage  for a hash function  h  with an output of  n  bits there are three kind of attacks that we consider for each there exists a generic algorithm with a high cost and the function is deemed secure if we cannot find any method which is faster than the generic algorithm the attacks are    preimage given  x  a  n -bit string find  m  such that  hm = x  generic attack has average cost  2 n   expressed in evaluations of the function  h  over small inputs the generic attack works by trying random messages  m  until hitting  x  the luck and pray attack    second preimage given  m  a given string find  m   distinct from  m  and such that  hm = hm   this is the case you envision here generic attack is again of cost  2 n   and is similar to the preimage attack    collisions find  m  and  m   distinct from each other such that  hm = hm   generic attack has cost  2 n/2   known as the  birthday attack     md5 has a 128-bit output  2 128   evaluations is a very high and should provide adequate security it is one billion billions times higher than is technologically doable right now even with a google/facebook-like budget on the other hand  2 64   while still very expensive months of computation with thousands of computers has already been demonstrated once see  distributed.net   moreover a number of weaknesses have been found in md5 allowing for a very efficient algorithm for generating collisions with my pc i can generate one md5 collision in 14 seconds on average -- using a single core for that reason md5 is  not  considered secure anymore but no shortcut for second preimages is currently known the existence of weaknesses leading to easy collisions shows that the internal structure of md5 is not garbled enough so we have reason to worry about preimage attacks which  might  be found in the near future but right now july 2011 no such attack is publicly known  so the answer to your question is that it would be overwhelmingly difficult to send you an altered iso which would end up with the same md5 hash than the original one but the ubuntu distributors would be well inspired to begin publishing sha-256 hashes too just in case  it would be difficult to the point where to seriously suggest it even remotely possible is verging on lunacy  there have been some demonstrations of theoretical attacks against md5 wherein the attacker could create message data intended to yield a predetermined md5 hash but this is miles and miles away from adding a non-jibberish file to an iso and having it give the same hash  a much more likely attack scenario would be the mitm altering the page that lists the md5sums before it gets to you so that you see the attacker s hash rather than the real one however unlikely this may be here are the hashes for your comparison         ubuntu-11.04-desktop-amd64.iso 7de611b50c283c1755b4007a4feb0379      ubuntu-11.04-desktop-i386.iso  8b1085bed498b82ef1485ef19074c281,2011-07-15 06:01:20,630,5310,if i want to download the ubuntu11.04.iso then  ubuntumirrorsrv -> isp -> isp -> etc -> mypc  i just want to ask that how difficult is to spoof the original md5 sum e.g the md5sum would be reachable through https!  so we have - on the ubuntumirrorsrv xy md5hash and xz ubuntu iso - on mypc the downloaded iso from ubuntumirrorsrv xy md5hash and xy! ubuntu iso   so could the md5hash be the same as the original one on the ubuntumirrorsrv if there were a mitm attack that modified put a trojan in the ubuntu iso e.g one of my isp? +- a few mbytes - how difficult could that be,0.03492063492063492,22,sse,cryptography|hash,how difficult is it to modify an iso image and still have the old md5 check sum,5,trojan|attacks|spoofing|weakness|man in the middle,0.410278856754303
67320,note that you can t compute the bcrypt hash in the application and pass it to the database unless you store the salts separately from the hashes  if you do that you ll incur the overhead of performing a select first then the bcrypt and then the password verify step  doing as the presentation suggests having the db calculate the hash might make the most sense  postgres 9 s crypt module does support bcrypt as a built-in hash type  if you want to do the hashing in the application anyway comparing with a hash is probably the simplest least likely to have implementation issues  since bcrypt is based on blowfish it only considers up to 448 bits of input so you can safely hash the output with sha-512 and mathematically will not lose any strength  in reality sha-256 is fine as well as users passwords essentially never contain more than 256 bits of entropy  even 256 bits of perfectly random printable characters would take 56 character long passwords  comparing using a hash is likely to have the most straightforward implementation and since you already have the overhead of bcrypt the cost of 2 additional hashes is trivial  this part of the answer is based on a misinterpretation of the question as being concerned about timing attacks against the application not the database but i m leaving it here for reference.  if you re using bcrypt you ve already hashed the password so you don t need to be concerned about the timing results  even  if  someone could deduce the hash of the password they d still need to find a plaintext to provide to the application which should be computationally infeasible or bcrypt is broken  timing attacks are only useful if the attacker can iterate over the values being compared to find a useful comparison  i.e a b c...  with a hashed value this is not possible as they cannot iterate over the output of the hash for cryptographically strong hashes small changes in the input to bcrypt result in large changes to the output  a user providing the input to bcrypt has no way to try out prefixes and then extend the prefix  an attacker would not know anything about either side of the comparison and thus timing attacks do not leak any information about the value of the hash     rather than giving the application direct access to the hashes so that it check the password i d like to delegate this to the database system itself   why do you want to delegate it?  it s simpler to just fetch the hash for the username given and do a constant side string comparison using pseudo-code above with the fetched hash  every successful login will give the web application running on your server both the entered password as well as the computed hash of the password  granted what you want is fairly straightforward to do within for a given database using its procedural langauge pl  for example in postgres you could write   granted be very careful to give your constant time string comparison several test cases to make sure it is behaving properly,2014-06-10 22:34:54,764,60750,i m looking for a reliable solution to compare two strings without leaking their content through time differences the length of the strings is  not  secret  the background is this i m implementing password-based authentication for a web application the passwords are hashed with bcrypt and stored in a database rather than giving the application direct access to the hashes so that it check the password i d like to delegate this to the database system itself the application only hashes the password and then passes the hash to a database procedure this procedure has special privileges to access the stored hashes and compare them with the provided hash the goal is to protect the hashes against sql injection attacks a similar idea is described in  this presentation   obviously this scheme is only effective if the procedure does not leak any information about the stored hashes so the string comparison must be timing-safe  i m aware of the following method pseudo-code   however this is rather cumbersome and i m not sure if it works as expected in high-level languages like sql  another common suggestion is to  hash the strings and then compare the hashes  this would be much simpler than the code above  which method is preferable? if i were to use the hash solution which algorithm would i choose to not degrade the strength of bcrypt not even theoretically? sha-256? sha-384? sha-512,0.015706806282722512,12,sse,passwords|timing-attack,timing-safe string comparison in high-level languages,5,leak|attacks|protection|timing attack|sql injection,0.40955451130867004
12312,use the     absolute value function   note that     if x is the integer -9223372036854775808 then   throws an integer overflow error since there is no equivalent positive 64-bit two s complement value    for generating  positiveincluding zero  random number with only upper bound use   for generating  positive  and  non-zero  random number use    high  - represents upper bound   low  - represents lower bound   and   both  tend  to give the desired result  to my eye the second is more readable first we pseudo randomly select an integer then we apply abs insure it s non-negative then we modulo this by n to select the appropriate range  however the first method is more reliable the second method has a very low but non zero risk of throwing an integer overflow as noted by nulluserexception  your method   for getting a random number within a certain range is flawed as it will give an uneven distribution the number zero will come up twice as often as any other number  incidentally using   to make the numbers positive is flawed in the exact opposite way making the number zero turn up half as often as any other number so they cancel each other out,2011-11-29 00:15:38.937 UTC,241,8304204,in sqlite      the   function returns a pseudo-random integer between -9223372036854775808 and +9223372036854775807   you can get an random with maximum value n with    but that still generates negative numbers how can i generate only positive random numbers in sqlite ,0.02074688796680498,5,so,random|sqlite,generating only positive random numbers in sqlite,3,flaws|overflow error|integer overflow,0.4095349609851837
2295,,2020-03-17 00:56:41,63,60714973,while inferenc with differ batch size each iter of the loop is lead to rapid consumpt of memori thi is onli observ when run on the cpu and not on gpu here is a snippet for reproduc the issu could somebodi help in solv the issue? thank,0.031746031746031744,2,so,memory-leaks|pytorch,why is memori leak on cpunot on gpu with vari batch size in pytorch,1,memory leaks,0.409391313791275
5469,first of all you  should not encrypt passwords  encryption is reversible instead you should  hash the passwords  a hash is kinda like one-way encryption  when the user signs in instead of decrypting you  encrypt  hash the password they provided and see if it matches the hash that is already stored in the database if it s the same password and salt then the resulting hash will be consistent and you can let them in  when hashing passwords you have to consider a few things    it s important to use salt to prevent side-channel attacks in relation to duplicate detection      you should use a well-vetted routine      you should use a high enough work factor so it is expensive to brute force    i ve explained all the considerations here  why is md5 considered a vulnerable algorithm?       in summary you should use bcrypt which has had more time to be vetted than argon2    run a quick sha-256 on the password before feeding it to bcrypt to prevent truncation    the bcrypt work factor should take 50-500ms on your target machine    limit login attempts to prevent online brute force as well as dos via hash computations    bcrypt already has salt but it s best to also include some pepper ~72 bits of entropy which is a quick sort of extra password added to the user s password the pepper is the same for all users but is unique to your application it is stored either in your source code or better yet in a separate text file but never stored in an sql database    include reasonable password strength requirements in your application because no amount of hashing will protect a weak          would an unencrypted password database in a system be considered a vulnerability? think yes   if you don t hash or encrypt it s a vulnerability for sure     would an unsalted hashed password database in a system be considered a vulnerability? unsure   yes primarily because of rainbow tables however the bigger concern would be how strong of a hash function you are using most of the good algorithms already have salt so if you don t have salt there s a good chance you re using a weak algorithm     would a salted and hashed password database be considered a vulnerability? tend to think no   you re right this is ok it s common to store both salt and hash in an sql database but it s best to also include pepper which is kept outside of the sql database     and would an encrypted password database but with a same secret key be considered a vulnerability?    yes reversibly encrypting passwords is bad if the key is stolen the passwords are easily retrieved without brute force hashing solves this problem,2017-01-29 02:10:02,519,149723,few considerations regarding what to call a vulnerability    an unencrypted password database think yes   an unsalted hashed password database unsure   a salted and hashed password database tend to think no   an encrypted password database but with a same secret key   which of the above scenarios would a security researcher be able to report as a vulnerability,0.028901734104046242,15,sse,passwords|vulnerability,unencrypted passwords in a system considered a vulnerability,5,attacks|weakness|protection|vulnerability|denial of service,0.4089130461215973
10099,use a long text for opt like 6-10 chars long which will provide a lot of combinations factorialn which will be a very big number that no ordinary system can guess that otp in 5 minutes  use not only numbers but also characters which can make your otp more strong,2019-02-11 06:10:02.683 UTC,118,54624825,we have a feature in our application that ask for a six digit otp before doing certain functions it is sent via sms and expiration is 5 mins there has been an internal penetration test that exposed that this is vulnerable to brute-force attacks what can we do programmatically to prevent this,0.05084745762711865,6,so,brute-force|one-time-pad|one-time-password|owasp,how can otp one time password be protected against brute force attacks,5,owasp|attacks|protection|vulnerability|penetration test,0.4066130220890045
12463,salting a password is a counter measure against rainbow table attacks  consider a database with the following entries   and consider the following rainbow table that stores common passwords which a hacker may use to try and find password hashes in a db   a hacker can easily find commonly used passwords without having to compute or crack any hashes themselves jim jack and jill have been compromised while jane who uses a more obscure password has been saved from compromise  without a salt the hashing function  always  looks like   however with a salt   you can see that the rainbow table looses its efficacy and is no longer usable none of     or   will match   of password1     is security more about the protection of the location of the code than it is hashing the stored information?   security is about building mitigations for attack vectors  there are many types of attacks and many types of mitigations  you ve stumbled across a few of them  salt a random string stored along with the hash protects against a specific attack vector-- a  rainbow attack   launched by a malicious user with access to the entire table  pepper a cryptographically random secret salt value stored in a different location protects against something else-- a  dictionary  or  brute force  attack launched by a malicious user with access to the hashed value and salt for a particular user but without access to other systems especially the system that stores the pepper  if you are reading up on this you will find that a lot of experts think that pepper is more or less useless and amounts to security by obscurity  the reason they feel this way is they believe at least in principle that if a cracker has access to your db then he almost certainly has access to the storage location holding the pepper       why does generating a random salt for each in this case password matter particularly if the database has been exposed i.e does salt randomness really add anything to the security of information?   in order to understand why this is important you must understand how a  rainbow table  can be used to speed up an attack  with a rainbow table a cracker can compute all the known hash values for common passwords and scan your entire database table to find them  this is much faster than a brute force attack on each individual entry  if a salt is combined with the passwords it renders the rainbow table useless   in the end it is all about slowing the cracker down by making the problem computationally infeasible     the security of the salt seems to lie in the attacker not having it not its randomness   this is absolutely incorrect  it is all about changing the input of the hash from something predictable e.g a word from a dictionary to something unpredictable a word combined with a random string which again is to thwart a rainbow table     why store the salt in a database at all?   when the user signs on he provides his password and not the salt he doesn t even know the salt  the entered value must then be combined with the salt and then hashed  the system compares this hash to the hash stored in the database  if there is an exact match the system can infer that the hashed value was created with the same password entered by the user and therefore validate the authentication attempt  since the user doesn t know the salt the database has to store it  the purpose of the salt is to provide additional protection if the database is compromised if you assume the database cannot be compromised you can just store everything in plaintext and a password-based authentication system will work fine the problem is that password databases are compromised all the time   john wu and michael burr have already covered why you want a salt to prevent passwords that are the same from appearing as the same hash in the database   if you use a salt it must be stored in the database otherwise a legitimate user cannot be authenticated without hashing the given password with all possible salts so if the attacker gets access to a database entry the attacker gets access to the salt too   if an attacker gets access to database entries we want to maximize the amount of work and time it takes for the attacker to discover the password by brute force if there is no salt the attacker can precompute before getting access to the database and store a rainbow table which is basically a list of potential passwords that are run through the hash function looking up a database entry in the rainbow table is very fast so if the attacker can store one and has access to the database passwords are discovered immediately when using a salt the size of the required rainbow table increases exponentially with the salt length so using a sufficiently large salt will effectively preclude the use of the rainbow table   the attacker can still brute force the database entry with the known salt and hash algorithm the attacker will check a dictionary this is slow because the attacker cannot precompute anything the attacker must use the salt discovered in the database if the password is not in the dictionary the attacker must resort to guessing passwords eventually they will discover the password this is also why password expiration is useful if the expected time to brute force the hash is greater than the password expiration period you will have changed the password before the attacker brute forces it,2014-10-17 20:00:03.96 UTC,1765,26432597,introduction  i have done a lot of research today and have learned a lot about hashes and encryption and the important difference between the two one thing i have learned is this people disagree on the efficacy of salting   in the responses to this  question  and particularly in the comments to the accepted answer people conclude - it seems - that storing a randomly generated salt next to the hashed password in a database  is  the best way to secure passwords there are several resources that state this and this will be the basis for my question   in this  question  the accepted answer states that with access to the all the possible hashed data values the salt doesn t do much - conecptually - to make things anymore secure though that would be an unfeasibly large amount of stored data salts and hashing algorithms serve the purpose of increasing the number of possibilities for a single password entry making the use of a dictionary-based attack against a database exponentially less efficient here i use the term dictionary-based more liberally refering - really - to a table whether that be a rainbow table a table utilized for brute-force attacks re-computing each time or a simple lookup table   the bits and pieces   from my understanding a well encrypted password needs three 3 things   to be hashed   to be hashed in conjunction with a salt   to be hashed in conjunction with a random salt one salt for each password entry   in order for  a cracker to understand the resulting stored hash he or she needs four 4 things    knowledge of the hashing algorithm  knowledge of the salt  the correct plaintext password value perhaps supplied as input from a dictionary-based attack running a dictionary through the algorithm in conjunction with the salt   the security bit   the disagreement seems to surround what to do with the salt i.e where to store the salt the phrase i keep seeing passes around stack overflow is security by obscurity - which appears to be the premise of a salt let s make this bit of information less  seeable  however from my reading i have come to understand that security by obscurity doesn t really work the premise of which seems to be  if the attacker has access to the algorithm and - if provided - the salt he or she can figure it out    the premise then of security is not  solely  the mixing around and changing of the values but the means the database transactions or server use of stopping the types of attacks that would    yield the salt to the attacker  allow the attacker to brute-force the database   another user commented that it shouldn t matter if the code is exposed to the attacker as good code - or a good security algorithm - would prevent the attack from succeeding   so security then is a combination of three 3 things    preventing access to the server where everything is stored  creating an algorithm that prevents brute-force entry to the database   encrypting transactional information  hashing obscuring sensitive data    the concern over the efficacy salts seems to stem from the assumption that the attacker has access to the database itself   scenario i ve got a dictionary your salt and your hashing algorithm grabbing your password should be of no real trouble this is why we work to stop brute-force attacks   so my question as a result of these conclusions which are now on the stack overflow chopping block are   questions    does security also involve the protection of the location of the code e.g access to the hashing algorithm?  why does generating a random salt for each password matter particularly if the database has been exposed i.e does salt randomness  really  add anything to the security of information? is this simply a matter of the exponential increase in time for an attack as a result of the random salt e.g making an attack less appealing?    the security of the salt seems to lie in the attacker not having it not strictly its randomness     why store the salt in a database at all?   essentially is all of this security  stuff  about the efficacy of hashes etc. just myth? maybe i am just responding to the idea that nothing really will ever be entirely hacker proof we just do everything we can?    why i am motivated to ask   it seems to me that if your database is exposed the salt - random or not - doesn t matter especially if the database stores the salt   best practice then would be twofold salts only adding a bit more security 1 the hashing of sensitive information 2 the protection - in the context of this question - of the database from brute-force attacks  there seems to be a lot of misinformation floating around so  some of it probably resides in my derived understandings above   here i am assuming guards against cross-site scripting etc are already in place,0.026062322946175637,46,so,algorithm|encryption|hash|security,passwords salts and security concepts,6,attacks|protection|sensitive data|plaintext password|cross site scripting|sensitive information,0.40636295080184937
27041,different inputs may result in the same hash this is what is called a hash collision  check here   http://en.wikipedia.org/wiki/collision_%28computer_science%29   hash colisions may be used to increase chances of a successfull brute force attack see   http://en.wikipedia.org/wiki/birthday_attack   the whole point of a cryptographic hash is that similar passwords would absolutely not create similar hashes  more importantly you would most likely salt the password so that even the same passwords do not produce the same hash   do similar passwords have similar hashes?   no  any similarity even a complex correlation would be considered a weakness in the hash once discovered by the crypto community it would be published and enough discovered weaknesses in the hash eventually add up to advice not to use that hash any more  of course there s no way to know whether a hash has undiscovered weaknesses or weaknesses known to an attacker but not published in which case most likely the attacker is a well-funded government organization the nsa certainly is in possession of non-public theoretical attacks on some crypto components but whether those attacks are usable is another matter gchq probably is i d guess that a few other countries have secret crypto programs with enough mathematicians to have done original work china would be my first guess all you can do is act on the best available information and if the best available information says that a hash is good for crypto then one of the things that means is no usable similarities of this kind  finally some systems use weak hashes for passwords -- either due to ignorance by the implementer or legacy all bets are off for the properties of a hashing scheme that either hasn t had public review or else has been reviewed and found wanting or else is old enough that significant weaknesses have eventually been found md5 is broken for some purposes since there exist practical means to generate collisions but not for all purposes afaik it s ok for this in the sense that there is no practical pre-image attack and having a handful of hashes of related plaintexts is no better than having a handful of hashes of unrelated plaintexts but for unrelated reasons you shouldn t really use a single application of any hash for password storage anyway you should use multiple rounds   could a hacker increase their chances of brute-forcing the password if they have a list of hashes of similar passwords?   indirectly yes knowing that those are your old passwords not because of any property of the hash but suppose the attacker manages to very slowly brute-force one or more of your old passwords using those old hashes and sees that in the past it has been thisismypassword3 and thisismypassword4  your password has since changed to thisismypassword5 well done by changing it before the attacker cracked it you have successfully ensured that the attacker did not recover a valuable password! victory! except it does you no good since the attacker has the means to guess the new one quickly anyway using the old passwords  even if the attacker only has one old password and therefore cannot easily spot a trend password crackers work by trying passwords which are similar to dictionary words and other values to over-simplify a bit it will try the dictionary words first then strings consisting of a word with one extra character added removed or changed then strings with two changes and so on  by including your old password in the other values the attacker can ensure that strings very similar to it are checked early in the cracking process so if your new password is similar to old ones then having the old hashes does have some value to the attacker - reversing any one of them gives him a good seed to crack your current password  so incrementing your password regularly doesn t add much changing your password to something that s guessable from the old password puts your attacker in the same position as they d be in if they knew nothing at all but your password was guessable from nothing at all  the main practical attacks on password systems these days are eavesdropping via keyloggers and other malware and phishing trying to reverse password hashes isn t a good percentage attack although if an attacker has somehow got hold of an /etc/passwd file or equivalent they  will  break some weak passwords that way on the average system  short answer no!  the output of a hash function varies greatly even if one character is increased  but this is only if you want to break the hashfunction itself  of course it is bad practice since it makes bruteforcing easier  no if you check the password even slightly it produces completely new hash   as a general rule a good hash will not hash two similar but unequal strings to similar hashes md5 is good enough that this isn t a problem however there are rainbow tables essentially password:hash pairs for quite a few common passwords and for some password hashes the traditional des-based unix passwords for example full rainbow tables exist  with a good hash algorithm similar passwords will get distributed across the hashes so similar passwords will have very different hashes  you can try this with md5 and different strings   it depends on the hashing algorithm if it is any good similar passwords should not have similar hashes  it depends on the hash algorithm used a good one will distribute similiar inputs to disparate outputs  to expand on what others have said a quick test shows that you get vastly different hashes with small changes made to the input  i used the following code to run a quick test   and i got the following results,2010-04-21 14:09:48.757 UTC,1072,2683626,our computer system at work requires users to change their password every few weeks and you cannot have the same password as you had previously it remembers something like 20 of your last passwords i discovered most people simply increment a digit at the end of their password so thisismypassword1 becomes thisismypassword2 then 3 4 5 etc  since all of these passwords are stored somewhere i wondered if there was any weakness in the hashes themselves for standard hashing algorithms used to store passwords like md5 could a hacker increase their chances of brute-forcing the password if they have a list of hashes of similar passwords,0.027052238805970148,29,so,brute-force|hash|language-agnostic|passwords|security,do similar passwords have similar hashes,6,attacks|malware|weakness|eavesdropping|weak password|bad practices,0.40629076957702637
1223,the 1000 factors cancel out by making collisions much probable so that each password has on average 999 brother passwords which equally grant access to site a you make entering site a 1000 times easier due to the vast choice of possible passwords that easy-site-a password has only 1/1000 probability of being good for site b where the user reused his password but that extra 1000 difficulty factor is over the difficulty of entering site a -- which has just been made 1000 times easier so entry in site b has not been made harder  one other way to see it is to imagine the attacker trying to attack site b he then creates site a himself -- with an extremely collision-happy hash function so that every possible password grants access to a it suffices to have a hash function which always return the same value the attacker can certainly implement a site with no access restriction i.e that kind of site a does it change anything to the difficulty of entering site b ? since it is something that the attacker can build from scratch independently of site b and its users it cannot  in the situation you envision you can imagine site a as a filter for site b sure site a won t tell you which password is the right one for site b but it will reduce the list of candidates to 1000 out of a few billions this can only  help  attacking site b and will certainly not  protect  site b,2013-05-02 18:16:43,493,35261,website security breaches seem to be a common occurrence giving the attacker password hashes that he can conduct a brute force attack against often given him a list of passwords that will work on other sites since users commonly reuse passwords  would it improve overall security by making the hashing algorithm slightly weaker and more prone to collisions so even if the attacker does brute force a password that works on the hacked site it s not likely to work at any other site?  so if for example  some hash algorithm were 1000 times more likely to have a collision than some other hash algorithm it makes the attacker s work 1000 times easier on that site but any password he brute forces has only a 1 in 1000 chance of working on another site he could continue to brute force and come up with all 1000 passwords that collide with the hash but if he tried to cycle through all 1000 passwords on other sites hopefully that site would shut him down before he got through all passwords  would this lead to better overall security by reducing the ability of a hacker to exploit passwords shared with other sites or would it just weaken security by deliberately making it much easier to brute force a single site s password,0.02636916835699797,13,sse,brute-force|hash|passwords,could a password hash that s prone to more collisions provide better overall security,4,attacks|exploit|weakness|protection,0.4048781394958496
27278,why not add a random salt to the password and hash that combination next concatenate the hash and salt to a single byte[] and store that in the db?  the advantage of a random salt is that the user is free to change it s username the salt doesn t have to be secret since it s used to prevent dictionary attacks  i think you need to ask yourself what are you hoping to gain by making this more complicated than just generating a random salt value and storing it?   the more complicated you make your algorithm the more likely you are to introduce a weakness inadvertently  this will probably sound snarky no matter how i say it but it s meant helpfully - what is so special about your app that it needs a fancy new password hashing algorithm?  i think you are over-complicating the problem   start with the problem    are you trying to protect weak passwords?   are you trying to mitigate against rainbow attacks?    the mechanism you propose does protect against a simple rainbow attack cause even if user a and user b have the same password the hashed password will be different it does seem like a rather elaborate method to be salting a password which is overly complicated    what happens when you migrate the db to another server?   can you change the unique per db value if so then a global rainbow table can be generated if not then you can not restore your db      instead i would just add the extra column and store a proper random salt this would protect against any kind of rainbow attack across multiple deployments   however it will not protect you against a brute force attack so if you are trying to protect users that have crappy passwords you will need to look elsewhere for example if your users have 4 letter passwords it could probably be cracked in seconds even with a salt and the newest hash algorithm   the salt just needs to be random and unique it can be freely known as it doesn t help an attacker many systems will store the plain text salt in the database in the column right next to the hashed password  the salt helps to ensure that if two people user a and user b happen to share the same password it isn t obvious without the random and unique salt for each password the hash values would be the same and obviously if the password for user a is cracked then user b must have the same password  it also helps protect from attacks where a dictionary of hashes can be matched against known passwords e.g rainbow tables  also using an algorithm with a work factor built in also means that as computational power increases the work an algorithm has to go through to create the hash can also be increased for example  bcrypt  this means that the economics of brute force attacks become untenable presumably it becomes much more difficult to create tables of known hashes because they take longer to create the variations in work factor will mean more tables would have to be built,2009-07-27 23:01:43.617 UTC,925,1191112,suppose you were at liberty to decide how hashed passwords were to be stored in a dbms  are there obvious weaknesses in a scheme like this one?  to create the hash value stored in the dbms take   a value that is unique to the dbms server instance as part of the salt  and the username as a second part of the salt  and create the concatenation of the salt with the actual password  and hash the whole string using the sha-256 algorithm  and store the result in the dbms   this would mean that anyone wanting to come up with a collision should have to do the work separately for each user name and each dbms server instance separately  i d plan to keep the actual hash mechanism somewhat flexible to allow for the use of the new  nist  standard hash algorithm  sha-3  that is still being worked on  the  value that is unique to the dbms server instance  need not be secret - though it wouldn t be divulged casually  the intention is to ensure that if someone uses the same password in different dbms server instances the recorded hashes would be different likewise the user name would not be secret - just the password proper  would there be any advantage to having the password first and the user name and  unique value  second or any other permutation of the three sources of data?  or what about interleaving the strings?  do i need to add and record a random salt value per password as well as the information above?  advantage the user can re-use a password and still probably get a different hash recorded in the database  disadvantage the salt has to be recorded  i suspect the advantage considerably outweighs the disadvantage.  there are quite a lot of related so questions - this list is unlikely to be comprehensive    encrypting/hashing plain text passwords in database    secure hash and salt for php passwords    the necessity of hiding the salt for a hash    clients-side md5 hash with time salt    simple password encryption    salt generation and open source software    password hashes fixed-length binary fields or single string field?    i think that the answers to these questions support my algorithm though if you simply use a random salt then the  unique value per server  and username components are less important,0.018378378378378378,17,so,encryption|hash|salt|security,password hashing salt and storage of hashed values,4,attacks|weakness|protection|weak password,0.40425875782966614
2894,you shouldn t mess with the algorithm like this i can t think about what the impact of this method is but it does scream insecurity at the very least it would allow an attacker to move roughly 256 times as fast since crc is a relatively simple math function and then of course faster on the database part you re a few whiteboard coding exercises away from somebody dumping all your passwords if they get db access  instead issue a token using a cheap function that is rate-limited per client address or client address prefix and require that token with the username and password that will allow you to rate limit how often you perform the expensive password checking process  i upvoted jeff s answer  architecturally you are better off creating some independent gate keeping functions that are tied to client address and which run when the client side login input screen is called   keep this separate from your authentication routines  you can limit try rates or max tries before flagging the account in some fashion and that does not require you to muck around with the core of the authentication    crypto implementations are a function the weakest link and crc is not even really a true cryptography hashing primitive  its pretty bad in this regard  storing crc checksums creates problems because it allows an attacker who gets the database to utilize an easily paralleled process to reduce the work to attack each password in the database   putting the crc of the salt+pass phrase right there with the final scrypt hashes works for an attacker as a sort of a sieve   the attacker can run a fast routine that begins a brute force effort with no other object but to find strings that checksum to your stored crc  only strings that pass this sieve step have to run the gauntlet through the computationally and memory intensive scrypt hashing   its even faster if the attacker is running dictionaries,2015-02-02 20:42:09,462,80685,i m considering using scrypt for password storage i m open to pbkdf2 as well or bcrypt by itself  the issue is that i don t want this to become a potential point for a ddos attack given the overhead of the actual computation  i was thinking something  very  weak with a lot of collisions as a sanity check first like crc8 against the salt+passphrase might be a good idea then using a wait before returning the failure to guard against timing attack  this assumes a minimum length of 8 with a 3 of 4 requirement for   uppercase alpha  lowercase alpha  number  non alpha-numeric   how much would this would actually reduce the effectiveness of scrypt in a brute force attack should data be compromised,0.02813852813852814,13,sse,bcrypt|passwords|pbkdf2|scrypt,combining scrypt + a short crc,4,ddos|attacks|weakness|sanitization,0.40418002009391785
24288,salting hash with an unpredictable salt is used to prevent  collision attacks   a collision attack in one world is an attack targeting the hash table algorithm which normally operates in o1 but can be tricked to operate in on  tricking an hashtable algorithm is simple an hashtable implementation is usually slower when storing objects having a common hashed value it s named a collision it s slower because the two values are stored together for exemple in a linked list so if you can produce a large quantity of keys that all collide you re forcing the hashtable to store them all in a linked list which is damn slow and eats a lot of cpu  to exploit this you have to know the hash algorithm to be able to predict and generate conflicting keys if the hash is salted by a value you don t know you ll not be able to generate colliding keys,2014-07-13 18:20:45.173 UTC,335,24725811,while scrolling through all   options i found that python contains an option to [turns] on hash randomization so that the   values of str bytes and datetime objects are “salted” with an unpredictable random value although they remain constant within an individual python process they are not predictable between repeated invocations of python  source   the official documents refer this document  http://www.ocert.org/advisories/ocert-2011-003.html  that is supposed to provide more information however it doesn t provide any information what such a crafted http requests looks like all relevant links on the site are dead i know that this can be fixed by calling   however i m more interested in the details how can one http request make 100% of the server cpu for several hours and how can randomizing the hash value fix that? is it creating some kind of a dead lock? i know that a http request can take long if the script is broken infinity for/while loop  s or is doing a very expensive task but i assume that this isn t the case,0.020895522388059702,7,so,denial-of-service|python|security,python -r function,5,attacks|exploit|dead lock|predictable salt|denial of service,0.40376001596450806
7008,the size of the input is irrelevant in fact because of the  birthday paradox  you don t need any more than the size of the hash to make collisions guaranteed the best way to avoid collisions is to use a stronger hash which is not vulnerable to them such as sha-2 however you are describing a more difficult attack than a collision attack called a  preimage attack  which md5 is safe from  there are three types of attacks *  that result in having two files with the same digest     1st preimage  - find an input that resolves to a specific hash     2nd preimage  - modify an input without changing the resulting hash     collision  - find any two distinct inputs that have the same hash    these are called attacks when they can be carried out more efficiently than by brute force search collisions can still occur naturally and in fact they are guaranteed with any non-trivial amount of input due to the  pigeonhole principle  but hashes are designed to make it difficult to  intentionally  perform for a hash with an output the size of md5 s the chance of a random accidental collision is extremely low even if you hash 6 billion random files  per second  it would take 100 years before you get a 50% chance of two hashes colliding md5 is  great  for detecting accidental corruption  a strong  n -bit hash function is designed to have a security level of 2  n   against both 1st and 2nd preimage attacks and a security level of 2  n /2  against collision attacks for a 128-bit hash like md5 this means it was designed to have a security level of 2 128  against preimages and 2 64  against collisions as attacks improve the actual security level it can provide is slowly chipped away  md5 is  vulnerable to a collision attack  requiring the equivalent of only 2 18  hash invocations instead of the intended 2 64  to exploit unless the attacker generates  both  files it is not a collision attack  †   an attacker who has a file and wants to maliciously modify it without the hash changing would need to mount a 2nd preimage attack which is  completely infeasible  against md5 with modern technology the  best attack  has a complexity of 2 123.4  compared to md5 s theoretical maximum of 2 128  collision attacks are relevant in different situations for example if you are given an executable made by an attacker without a backdoor you may hash it and save the hash that executable could then later be replaced with a backdoored version yet the hash would be the same as the benign one! this is also a problem for  certificates  where someone could submit a certificate for a domain they do own but the certificate would intentionally collide with one for a domain they do not own  it is safe to use md5 to verify files as long as the stored hash is not subject to tampering and can be trusted to be correct and as long as the files being verified were not created or influenced! by an attacker it may still be a good idea to use a stronger hash however simply to prevent a potential practical preimage attack against md5 in the future from putting your data at risk if you want a modern hash that is very fast but still cryptographically secure you may want to look at  blake2    * while there are other attacks against md5 such as length extension attacks that affect all merkle–damgård hashes as mentioned by @lieryan these are not relevant for verifying the integrity of a file against a known-correct hash     †  a variant of the collision attack called a  chosen-prefix collision attack  is able to take two arbitrary messages prefixes and find two values that when appended to each message results in a colliding digest this attack is more difficult to pull off than a classic collision attack like the length extension attack this only applies to merkle–damgård hashes   the size of the file doesn t make a difference md5 is based on  merkle–damgård construction  which is vulnerable to the  length extension attack  15kb is plenty to do the length extension attack there are plenty of known collisions and methods to generate md5 collisions that are just a few hundreds bytes in length and once a base collision is found being vulnerable to length extension means that they can be used to generate an arbitrary number of further collisions  it depends on what you want to defend yourself against  security is never a one-size-fits-all game if it were then there would not be 12941 different hash algorithms instead you need to understand that every security measure defends you against a specific sort of attack you put a password in your computer to defend against random people accessing it not because it s so fun to type   whenever you log in  as for hash algorithms you can grossly classify them as cryptographic hashes and non-cryptographic hashes cryptographic hash algorithms are designed to withstand a number of attacks while non-cryptographic hashes are designed to be as fast as possible 1  md5 for example is considered a cryptographic hash but so broken that it s only usable as a non-cryptographic hash  when to use a non-cryptographic hash  if your goal is to detect bit-flips when copying a file from one location to another say a thumb drive to a laptop then md5 is absolutely the right choice i would even go as far as saying any  fast non-cryptographic hash  is good when you copy files you realistically do not need to fear attacker interference if you are paranoid about hackers being able to modify your kernel then adding hashes will not solve your problems  verifying file integrity with attacker interference  if you intend to sign and publish those files then an attacker might have the ability to craft a possibly legitimate file with the same hash - meaning that your signature is just as valid on the malicious file  an example  let s say your original message   looks like this     i hereby declare that the bunny rules!   you use your hash function   and gain the digest   afterwards you sign the digest   and get a signature    you then publish your message   your signature   and your hash function    i might be the attacker in the scenario and craft a message   that has the exact same hash in your chosen hash function     it is publicly known that dogs are better than bunnies in every regard..   since   the signature   is valid for both your original   and my malicious    in order to defend yourself against such attacks it is vital to choose a strong hash algorithm with high resistance to collisions this means that it becomes  very hard  for me to find an   where    good choices would include sha256 and sha512 as well as tons of others it seems everyone has some favourite non-mainstream hash functions but sha256 and sha512 have very widespread support and it will be hard for you to find a system that does not support these hashes and since your files are very small calculating the hash should be almost instant  for example on my 800mhz machine calculating the sha512 hash of a 16k random file took 3ms so even on a toaster it should be relatively quick    1   you can see the same thing with random number generators cryptographic prngs aim to deliver random numbers that are really hard to guess while non-crypto prngs aim to just give numbers that look random at first glance and do that fast   the size per-se isn t hugely relavent the actually collision data can be as small as a single block   however you are much safer with a collection of text files than with a collection of pdfs or similar  why? because the results of a collision attack generally result in both files of the pair containing some random looking garbage in a rich format this random-looking garbage can be hidden from sight so the attacker can trick the collection administrator into accepting one of their pair of colliding files   in a text file though the content is plain for everyone to see   short answer  no it is not secure to use md5 to verify the integrity of files short or long   the full answer depends on  how confident you are in the distribution of errors   is there an independent random chance of bit flips in each position in the file owing to transmission on a slightly lossy channel like a serial port?  if so you could use md5 but it s much cheaper to use a crc which is guaranteed to detect a single bit flip and can be guaranteed by standard choices of crc polynomial to detect all odd numbers of bit flips  but you asked about  secure  which suggests you are considering slightly more intelligent adversaries than a lossy serial port   if you re not  confident  that the errors are independent random bit flips then don t use md5 or a crc   it is very easy for intelligent adversaries to find pairs of distinct files that share a common md5 hash or crc checksum and in many scenarios this can enable an adversary to forge documents that your md5 system will not detect  the size of the file is not relevant it is easy to find md5 collisions in files as short as 64 bytes with no limit on how long they can be  there is a place to discuss the technical differences between collision attacks preimage attacks and second-preimage attacks  an answer to a  general question  about whether it is  secure to verify the integrity of files  is not such a place  when you have a  specific protocol  in mind where you can articulate the precise powers of the adversary and exactly how the legitimate users will behave in the protocol  and  you have implementation constraints that limit your choice of hash functions so that you  must  consider md5 then we can discuss perhaps on crypto.se whether it is  safe  to use md5 in that protocol to attain the security you hope to achieve against such an adversary  but it would be much simpler and safer for you to just use sha-2 or sha-3 or blake2,2018-05-29 03:21:04,1859,186657,i know that collision for md5 has been documented since the 90s and that digital certificates based off of md5 has been demonstrated to be completely compromised back in 2010 but how effective is md5 in ensuring that small amounts of data have not been tampered with?   i have some small text files that are a few pages in size let s say 15kb in size i ve been using sha-256 on them but it would be much more convenient to be able to use md5 instead  how secure would md5 be as a hash digest for these small 15kb text files? would a malicious party be able to produce collisions for such a small amount of data or does the small size make this a difficult endeavor,0.025282409897794515,47,sse,hash|md5|sha256,is it secure to use md5 to verify the integrity of small files less than 15kb,5,attacks|exploit|backdoor|vulnerability|malicious file,0.40372464060783386
64245,,2019-12-11 08:02:24,32,59281206,is there a maximum limit on how many mb or gb of memory a single process can use in linux,0.03125,1,so,linux|memory-leaks|memory-management|process,how much memory can process use ? linux,1,memory leaks,0.40349721908569336
14513,"no hashing algorithm is secure imo however there are a few out there that will slow hackers down our only hope is they get peep off with it..  increase the cost   example code in php : $hash = hello  $hash = password_hash$password password_default,array cost =>15  echo $hash  var_dumppassword_verify$password $hash  thus a cost of 15 takes lot of time to hash and crack it   above example shows the hashing process as well as way to verify your hash for the authentication purpose since   bcrypt creates new hash everytime it run !!!!  put this code in the .php file and then run it too check your code running   do not forget the &lt ?php ? >  well there are two types of attacks against passwords   the attacker can somehow read out your password hashes which are store in the database sql-injection or any other way to get access to your system the attacker now wants to get the original password since the hashed one doesn t get him far  the attacker uses your login-form repeatedly with different passwords brute force   in case 1 he has already circumvented your login-form he don t cares how much time is needed for one login on your system but he cares how much time it takes to brute-force the password on his systems so the work factor just makes it harder to brute-force the password by trying every combination and hashing it with the same algorithm as your system re-hashing the password to make the login-process longer won t help in this case   in case 2 however he  does  care how much time is needed for trying a password on your system because he must do it a lot to get to the right login plus he must probably guess the username if he can t find that out the attacker must be very dumb because he will leave traces and wastes time you can make his life more miserable by blocking his ip after a certain number of failed logins plus you could add a   after every invalid login a sleep does not need that much resources as a cpu intensive hashing-operation",2011-12-27 06:11:56.73 UTC,470,8641697,i am referencing this answer  https://stackoverflow.com/a/4766811/1114105   we re-hash the password but then we don t really do anything with the hash we use the post-submitted plaintext password in the checkpassword function to authenticate can t a hacker bypass the re-hashing part?    here s my pseudocode underneath  if a password/username is submitted by post and $row[ password ] is the hashed pword in sql   note i found that the work factor does not make a difference in the time it takes checkpassword to run it only increases the time for hashpassword,0.01702127659574468,8,so,bcrypt|php|security,i m using bcrypt for php and want it to take a long time for a potential hacker to login,4,bypass|attacks|sql injection|plaintext password,0.40090349316596985
6322,you re missing another downside  it tells the attacker the length of the plaintext   in some cases this is considered sensitive information if i m brute-forcing passwords for example you re telling me that i can skip trying all passwords except those of the specified length cool thanks      collision attacks usually require the attacker to modify the length of the data by appending more blocks to the existing data or something like that don t they?   i believe you re describing a  length extension attack  the real cryptographers would probably roast me for saying this but i naively have a possible solution of padding the massage prior to hashing at which point you can drop the whole length encoding and just use a traditional hash function _really don t go building software based on this i don t guarantee that it s secure   then when the attacker tries to do a length extension attack by concatenating msg2 to the end   they will have a different number of padding 0 s which changes the massage prefix and totally breaks the length extension attack   the more i think about this the less i want to stand behind the idea i should probably spend some time googling length extension attack mitigation rather than sitting here and speculating   a cryptographic hash function is designed to have large changes to the output even on small changes to output by adding some bytes describing the length of the input you change this assumption since these length bits will only create small changes to the output on small changes to the input  apart from that you actually leak important information about the input as  mike ounsworth  already pointed out essentially you tell the attacker not only how long the input will be but also that there is definitely some value that long which results in this hash this greatly reduces the space the attacker has to brute-force in order to get the same hash result - which in case of passwords is all which is needed since a collusion is not required  but even in the case where you care about a preimage it is not clear that the bits you add to encode the length would not be more useful instead in extending the strength of the hash itself in order to avoid a preimage attack while it might help in cases where the input must be shorter than the hash itself and thus preimages are unlikely i think these bits are better used for a longer real hash in case the input is longer then the hash output because same-length preimages are much more likely then     the attacker would have to find a collision with the exact same length which is obviously much harder than finding a general collision   is it really that obviously harder? i would agree that it is harder if the input is shorter than the hash since it is a problem of finding preimages at such a small size but if the input is much larger than the hash i.e like with certificates i doubt that requiring a specific length really adds additional complexity to finding a preimage unless there is some inherent weakness in how the hash is constructed,2017-10-12 19:58:07,884,171180,edit2 this discussion was reduced to are collisions actually harder to create when you add a length constraint? which is more relevant to the crypto exchange i will create a question there and link it when it exists thanks for everyone who participated in the discussion   edit the term collision attack in wikipedia seems to only talk about finding any random collision i meant to say  preimage attack   it would seem like a good idea to me because it would prevent all collision attacks that require the attacker to modify the length of the data the attacker would have to find a collision with the exact same length which is obviously much harder than finding a general collision  collision attacks usually require the attacker to modify the length of the data by appending more blocks to the existing data or something like that don t they?  the only downside i see to this is that it will require the hash results to be longer the way i see it only by 2-4 bytes being the length of the data modulo 2^16-2^32 should be about enough so not by much  this of course doesn t have to be part of the hash function itself it could just be a general security recommendation verify the expected length of the data not just that it matches the calculated hash to prevent many forms of collision attacks  though i ve never heard this recommendation said or mentioned before even though the way i see it it makes a lot of sense  so in general i m asking here why is this not a good idea? should it be encoded in hash functions so it s automatically applied by anyone using those functions or should it be a general security recommendation that is up to the user of the hash function to implement on his own separately from the hash function?  or maybe i m completely missing something that makes this idea ridiculous,0.026018099547511313,23,sse,hash,shouldn t popular cryptographic hash functions encode the length of the input data in the output,4,leak|attacks|weakness|sensitive information,0.40008509159088135
4549,"pixie works by exploiting weaknesses in the generation of the e-s1 and e-s2 nonces which are used to produce the enrollee hash as described in the  pixie dust attack   traditional attacks attack the two halves of the wps pin psk1 psk2 in an online attack essentially brute-forcing all possible options for the pin until it is found this has to be done online against the target so it takes a long time  the wps exchange involves computing two hashes e-hash1 e-hash2 which are derived from two nonces e-s1 e-s2 the public keys of the enrollee and registrar pke pkr the authkey a value derived from the key derivation key / kdk and two halves of the wps pin psk1 psk2 the values pke pkr e-hash1 and e-hash2 are known by an attacker since the router will give you them or you re providing them as a client and you need to find the correct combination of e-s1 e-s2 psk1 and psk2 in order to break the hash offline  cracking the psk1 and psk2 parts is relatively easy there are only 10,000 possible values for psk1 and 1,000 for psk2 the last number is a checksum and each hash is separate so you only need to compute 11,000 hashes this doesn t take a lot of time on a modern system  however the problem is that you need to know the two nonce values e-s1 and e-s2 this makes the offline bruteforce approach untenable as these are 128-bit values  the crux of the pixie dust attack is that the e-s1 and e-s2 nonces are not generated securely in many routers as an example mediatek routers are known to just use zero for both values in some broadcom routers the prng used is a weak one likely an lfsr or something like mersenne twister and it is also used to generate the pke by guessing the possilbe prng seed values until you find one which generates the same pke as the router gave you can then generate the e-s1 and e-s2 values trivially realtek routers use the unix timestamp in seconds to generate the pke e-s1 and e-s2 values but since the generation is fast it s usually the case that e-s1 = e-s2 = pke or e-s1 = e-s2 = pke+1 in the case that it did tick over a second boundary and since we know pke we now know e-s1 and e-s2  this is why the pixie attack is so fast instead of trying 11,000 possible pins against the live router which is slow and might result in the router blocking you it captures the wps hash values and cracks them offline utilising weaknesses in nonce generation so that it only has to try 11,000 pin values against each possible guessed e-s1 and e-s2 value pair",2016-06-01 09:59:14,578,124774,i have been studying about the brute-force attack on the wps pin and i understand that because the last digit is the check digit and  that the pin is divided into two halves in m4 and m6 messages one can brute-force the wps pin in 11.000 attempts   though i recently heard about pixie attack on wps and it said something like it takes advantage of the low entropy in some routers what exactly is pixie attack and how does it affect wps? one more question how can one find the wpa key from brute-forced wps pin,0.02768166089965398,16,sse,router|wifi|wireless|wps,what s the difference between pixie attack and other attacks on wps,3,attacks|exploit|weakness,0.3990008234977722
17735,"modern brute-force attacks using multiple gpus could crack this in short order  i recommend you follow the guidelines for password storage for this application  here are  the current password storage guidelines from owasp   currently they recommend a long salt value and pbkdf2 with 64,000 iterations which iteratively stretches the key and makes it computationally complex to brute force the input values  note that this will also make it computationally complex for you to generate your key values but the idea is that you will be generating keys far less frequently than an attacker would have to  that said your design requires many more key derivations than a typical password storage/challenge application so your design may be fatally flawed  also keep in mind that the iteration count should doubled every 18 months to make the computational complexity follow moore s law  this means that your system would need some way of allowing you to rehash these values possibly by combining hash techniques  over time you will find that old hmac functions are broken by cryptanalysts and you need to be ready to update your algorithms  for example a single iteration of md5 or sha-1 used to be sufficient but it is not anymore  there are other hmac functions that could also suit your needs  that wouldn t require pbkdf2 such as bcrypt or scrypt but pbkdf2 is  currently  the industry standard that has received the most scrutiny  one could argue that bcrypt or scrypt would also be suitable but this is yet another reason why a pluggable scheme should be used to allow you to upgrade hmac functions over time  yes it s possible but not in this lifetime   no a sha hash is not reversible at least not easily  when you hash something if you need to reverse it you need to reconstruct the hash  this is usually done with a private salt and public key  for example if i m trying to prevent access based off my user id  i would hash my user id and the salt  let say md5 for example  my user id is 12345 and the salt is abcde  so i will hash the string 12345_abcde which return a hash of 7b322f78afeeb81ad92873b776558368  now i will pass to the validating application the hash and the public key 12345 which is the public key and the has  the validating application knows the salt so it hashes the same values 12345_abcde which in turn would generate the exact same hash  i then compare the hash i validated with the one passed off and they match  if i had somehow modified the public key without modifying the hash a different has would have been generated resulting in a mismatch  there are no published first pre-image attacks against sha-256 without such an attack to open a shortcut it is impossible for an attacker to the recover a secret value from its sha-256 hash  however the mention of a secret key might indicate some confusion about hashes hash algorithms don t use a key so if an attacker were able to attack one secret-value&ndash;hash-value pair he wouldn t learn a key that would enable him to easily invert the rest of the hash values  when a hash is attacked successfully it is usually because the original message was from a small space for example most passwords are chosen from a relatively short list of real words perhaps with some simple permutations so rather than systematically testing every possible password the attacker starts with an ordered list of the few billion most common passwords to avoid this it s important to choose the secret value randomly from a large space  there are message authentication algorithms that hash a secret key together with some data these algorithms are used to protect the integrity of the message against tampering but they don t help thwart pre-image attacks  in short  yes",2010-02-19 16:46:42.98 UTC,773,2298018,we would like to cryptographically sha-256 hash a secret value in our database  since we want to use this as a way to lookup individual records in our database we cannot use a different random salt for each encrypted value   my question is given unlimited access to our database and given that the attacker knows at least one secret value and hashed value pair is it possible for the attacker to reverse engineer the cryptographic key?  ie would the attacker then be able to reverse all hashes and determine all secret values?    it seems like this defeats the entire purpose of a cryptographic hash if it is the case so perhaps i m missing something,0.02069857697283312,16,so,encryption|security,is it possible to reconstruct a cryptographic hash s key,4,owasp|flaws|attacks|protection,0.39845767617225647
18216,the only way to protect against bruteforce attacks is to use a slow hashing algorithm right now  bcrypt  or  pbkdf2  is the way to go as it can be configured to require a lot of cpu time and ridiculous amounts of ram as cpus get faster and memory cheaper you just need to increase the settings and slow down the hashing  it really makes a difference if you can guess millions of hashes per second or only a few dozends  codinghorror has a great article about this topic  http://www.codinghorror.com/blog/2012/04/speed-hashing.html   the point would be that you would salt the password in the same way when you are comparing it while logging a user in and while storing the password having a good salt would prevent someone doing a standard lookup and it would also strengthen the hash against dictionary attacks if the salt and the password are combined within a hash it would greatly reduce the use of offline attacks and you wouldn t need to take any special precaution of protecting it   use passwords that won t show up in a dictionary  use  key strengthening    or just encrypt the password database itself  if you have no way to transport the decryption key you can encrypt the decryption key using public key encryption  encrypt the password file with something strong then it doesn t matter how the passwords are stored inside the file  ie use something like pgp for transport  i noticed that this answer has been downvoted once before someone else upvoted it and just thought i would clarify it since i suspect the person that downvoted my answer didn t actually read the question on the other hand perhaps full encryption of the entire file is not possible for the person asking the question this would be a handy clarification in the question if that is the case  anyway the question specifically states that this is about  transporting  the database it also specifically states that this is not about  online storage and usage   as such using a safe and secure encryption method for the entire file is really the best way to go as then you re not limited to having to manipulate the file contents to try to mask each individual password  instead the whole file would be meaningless and if more than just the password file is to be transported it would be lumped together with and thus encrypted together with a lot more data making the process of identifying the passwords nearly impossible as well  the attacks on such encrypted files are a lot less likely to succeed if you pick a reputable encryption algorithm and implementation pgp gpg etc. than inventing something yourself or at the very least making it easy for the attacker to find the individual passwords in small chunks  if you control the function that try to decrypt the database put a timer of 1 second between retry?  that would surely slowdown a bit the brute force   all the other responses are good but i d add  force password changes on a regular basis  this is the sure way to disrupt offline brute force attacks it s harder to hit a moving target,2009-04-14 12:50:47.063 UTC,683,747394,i was wondering if it was a common practice to salt and hash a password like    epadding || hashsalt || password    where || is concatenation and e uses rsa for example  i m primarily asking this question for transport of a password database not online storage where the server would have the private key to decrypt the encrypted password at all times    i know that a cryptographic hash should be irreversible but an  off-line  brute force attack on weak passwords would easily reveal a password  to eventually decrypt a record in this database the server will know the length of padding and simply take away padding to reveal hashsalt || password    this is not a typical problem but i couldn t find a reference as to someone having to properly transport a password database and defend against an  offline  attack,0.017569546120058566,12,so,passwords,how should one defend against an off-line brute force password attack,3,attacks|protection|weak password,0.3975779116153717
65656,"des especially single des cannot be considered secure a brute force attack in 1998 broke a des key in 56 hours with a $250,000 specialized machine more modern estimates are around $10,000 and 24-26 hours to break a des key using off-the-shelf tools and without quantum computing that s not what anyone would consider secure  although blowfish is not as insecure some attacks can crack messages if few gbs of data have been encrypted using the same key   mandatory backdoors could break any algorithm   although some people suspect that one or more such backdoor was inserted into des there s no proof in fact there is some evidence to the contrary as some modifications of des imposed by the nsa actually hardened it against attacks that weren t publicly known   as for blowfish i m not aware of any claim let alone evidence about the existence of a backdoor  with quantum computing one could brute-force a key with a time complexity that is the square root of the one required by a classical computer an m-qubit quantum computer where m is olog22^56 could break a des key in 2^28 attempts rather than 2^56   currently aes-256 is considered secure even with quantum computing while aes-128 is not des is not due to its very limited 56 bit key size as for blowfish the current default implementation uses 128 bits which wouldn t be enough to withstand quantum attacks   so blowfish and des are in current implementation are insecure  see a related article here  without a doubt this would only get worse as quantum computing emerges i would suggest going with aes and sha-512 however even this may not be a challenge for quantum computers due to their insane increase of clock cycles",2018-04-21 02:03:36,343,184223,what is the strength of algorithms such as des rijndael and blowfishused in password manager safe password with the current development 2018 of quantum computing the former parallel processing the mandatingbackdoor in the face of a brute-force attack,0.03206997084548105,11,sse,backdoor|blowfish|des,strength blowfish and des with current quantic computing and mandating backdoors,2,attacks|backdoor,0.3970256447792053
23225,a bit late but  yes  the salt must remain the same to be able to decrypt the stored values again  basically salting means randomizing a passphrase to make dictionary attacks a lot harder  how does a salt protect against a dictionary attack?    update one year later : by the way use a securerandom generator for the bytes in stead of a random generator - it s  better  i could go into detail but you can find that elsewhere as well  http://docs.oracle.com/javase/7/docs/api/java/security/securerandom.html,2011-10-22 09:03:04.427 UTC,158,7858454,i m implementing licensing in my android application and there is an array of 20 bytes that need to be passed into the aesobfuscator that is passed to the servermanagedpolicy object can this array be generated randomly every time the code is ran or does it have to be hardcoded?  right now i m randomly generating the salt like this,0.02531645569620253,4,so,android|obfuscation|salt,android do the random salt bytes passed to aesobfuscator need to stay the same,3,attacks|hardcoded|protection,0.39677897095680237
8081,you are correct this implementation of constant-time string comparison will  leak information about the length of some string that is being compared against an attacker controlled string    however if this is checking strings for authentication purposes you never should be comparing raw strings  you would first hash preferably with a salted key-stretched hashing function like bcrypt or pbkdf2 the entered password with the salt taken from the database and compare against the hashed password from the database    in such a scenario where a password login could potentially work the lengths of the compared hashes will always be identical regardless of what string the attacker enters  now there may be scenarios where the lengths aren t identical e.g maybe you used to use  sha256crypt  but decided to migrate user hashes to bcrypt  so you explicitly check whether the user-entered password works against either hash which have different lengths and there s no vulnerability if different lengthed hashes fail quickly  obviously it makes more sense to use an identifier to track the hashing algorithm you used  or you use client side code to hash first and then transmit the hash and the attacker tweaks the sent hash length  if they send an incorrect length there s no problem to fail quickly it s not like the correct hash length is a secret  you do have to be careful about writing code like the following python code   which  seems  like it should work until you recognize that   returns   because of how  zip  works stopping at the length of the shortest string  you could fix this with say    but if you think about it it is still vulnerable to timing attacks leaking the length of the string  finally you should recognize that constant time string comparison in principle leaks information about the length of the two strings being compared -- the running time will be proportional to the number of comparisons run  you could try to mask that e.g add random wait time but in principle any secure system will not be weakened in any practical way by knowing the length of the strings being compared,2019-07-03 01:43:12,437,212812,in node you can use   to check if two strings are equal in a timing-attack safe way but they must have the same length so you have to do something like that   is this approach safe? in my opinion it s not because it can leak the length of the string! surprisingly i discovered that   of php works in a similar way   do you consider this approach safe,0.036613272311212815,16,sse,node.js|php|timing-attack|web-application,timing attack - is safe to check if strings have the same length,6,leak|attacks|weakness|timing attack|vulnerability|incorrect length,0.3965291976928711
27146,"a salt prevents reverse checks against rainbow tables that are meant to hack passwords easily the salt converts something easily hackable into something more difficult for a hacker to decrypt  i would highly reccomend that you toy around with this api; http://www.openwall.com/phpass/   it does all that nitty gritty password generation for you without you needing to be a security expert additionally it has fallbacks built into it to work with older/weaker systems  the service salt described as a sitewide static salt in  the question you cite  and also sometimes called  pepper  in crypto literature is simply a secret string which is fed to the password hashing algorithm along with the password and the per-user unique salt  the point of having a service salt like that is that unlike the per-user salt values the service salt is  not  stored in the database but somewhere else typically in a configuration file or hard-coded into the application  thus it protects the passwords against attacks that  only  compromise the database but don t allow the attacker to access the app configuration  with modern web apps such an attack scenario is not as unlikely as it might seem for example a simple  sql injection  attack would often fit this scenario  one detail to keep in mind is that unlike the per-user salt which just needs to be unique and not too easily predictable the pepper actually has to contain a substantial amount of  entropy  say 128 bits and must be kept secret for it to be of any use  in any case including such a secret constant in the password hash calculation is pretty easy so there s very little reason  not  to do it even if your password hashing algorithm doesn t explicitly support it you can just say append the pepper to each password     the benefit provided by using a salted password is making a lookup  table assisted dictionary attack against the stored values  impractical provided the salt is large enough that is an attacker  would not be able to create a precomputed lookup table i.e a rainbow  table of hashed values password + salt because it would take too  much space a simple dictionary attack is still very possible,  although much slower since it cannot be precomputed   source  http://en.wikipedia.org/wiki/salt_cryptography",2012-07-30 04:50:37.847 UTC,459,11715548,i want to know how service salt works i know how to bcrypt password and give each user an unique salt  but i heard there is also another layer you can add by having a service salt  i am just curious how that works what is the difference between that and an unique salt generated for the user?  here s where i saw the term service salt  web application - storing a password,0.023965141612200435,11,so,bcrypt|passwords|php|salt,how does service salt work,5,attacks|weakness|hard coded|protection|sql injection,0.39601653814315796
27319,random salts have a tremendous benefit if all accounts in the system use the same salt an attacker can brute-force calculate hashes for that salt and break into all accounts with just one computational run if they use different salts per account brute-force only gets you into one account  a salt is be random by definition there is no such thing as a  static salt  if it is not random it s not a salt but a key   the point of the salt is to make sure the attacker has to mount a separate attack for each password he/she wants to crack in other words the point of salting a hash is to prevent precomputation attacks  rainbow tables   the easy solution for getting it right is to use a  standard library  instead of cutting corners  while best practice for password storage dictates that they should be stored in a hashed format with a unique salt the original question actually raises a reasonably good point if you store the salt in a different location to the hashes the impact of those hashes being disclosed is lowered  1 if the passwords were only hashed and stored in a database and the site suffered from sql injection then an attacker could crack the hashes  2 if the passwords were hashed with a salt and the both hash and salt were in the database and the site had sql injection then an attacker could crack the hashes but would require more computational effort as there is no performance boost from pre-computed tables  3 if the passwords were hashes with a salt and the salt was stored elsewhere then sql injection would afford an attacker little leverage to ascertain the actual password  scenario 1 is obviously weakest but the difference in security between 2 and 3 is less clear-cut and depends on the relative probabilities of sql injection vs server-side code disclosure and associated classes of vulnerability  what do you trust more - your ability to protect against sql injection or the ability of apache/php/whatever to protect your server-side content  things are never simple and i actually think the idea in the op makes more sense than other answers give credit for  you could use both a salt stored in database and a key if you like stored in the web app source when generating passwords  always use random salt for each password  if you don t then the benefit of having the salt is lost if you use the same salt  then in the case if website gets compromised  the hacker can use same hash table for hacking all the passwords in your userlist if salt is random  then he has to have new hashtable for each user  i  m not sure if you are salting correctly -- the purpose of a salt is to foil precomputed dictionary attacks if your database is compromised therefore you are using a database to begin with so what does your no need to use the db comment mean?  if you are not using a random salt then you don t make it more difficult for the attacker to attack your hashes if they get their hand on the salt you will be better off using a random salt -- you won t need to keep it hidden for your security to work  the salt also does not need to be long or unusual rk is a good salt 1q is also good its purpose is simply to vary the output of the hash function,2011-05-08 18:05:55.2 UTC,680,5929251,is there any working difference between   and   if i use random salt i need to store the random salt in the database on the other side if i use a fixed salt then no need to use db ! and if the code can be hacked to see the salt static then the hacker will be able to see the database also with the hash and random salt :d so does it worth it ? what if i use a salt like   ,0.027941176470588237,19,so,encryption|php|security,static salt vs random salt - security php,5,attacks|weakness|protection|sql injection|vulnerability,0.39599427580833435
67326,one tangible example is timing attack on string comparison it works by measuring how much it takes for the application to compare 2 strings    the default string comparison implementations in nearly all programming languages are optimized and they work by comparing 2 strings character by character and they alert a mismatch as soon as they see a difference between the 2 characters currently being compared    so if we have this code in our application     what happens is that 1 and 2 are going to take different amount of time to run 1 fails faster than 2 so if we change our input second string one character at a time by measuring the time it takes for the function to return we can guess the first string  to mitigate this timing attack we have to change our code so that it takes a fixed amount of time regardless of what the inputs are     i find this reference helpful  coding rules,2016-02-06 21:10:52,205,113005,i understand that timing side-channel attack is a  measurement  of time taken to execute cryptographic algorithms i d like to see a simple example that explains how timing attack takes a place on any cryptographic algorithm  thank you,0.02926829268292683,6,sse,cryptography|encryption|side-channel|timing-attack,timing side-channel attack example,2,attacks|timing attack,0.395933598279953
32723,"use       to get word from min length to max length    returns   this is a naïve implementation   i don t have enough reputation to comment yet so to make a full list based on the itertools sample above   this way you have all words from length 1 up to n in your list  def word_genstart= 3,end= 3 elements = 1:        hud seidu daannaa wordlist gen    msc infosec ceh",2014-02-04 17:15:20.677 UTC,146,21559039,i want to perform a dictionary attack and for that i need word lists how to generate word list from given characters of specific length  or word length from min length to max length ? i have tried   and   but it does not help they does not have all the word lists that it should return any help will be greatly appreciated thank you,0.0136986301369863,2,so,dictionary-attack|python|python-2.7,python - how to generate wordlist from given characters of specific length,2,attacks|dictionary attack,0.3953350782394409
23595,md5 is known to be vulnerable to collision attacks.http digest does not require collision resistance from the hash function it uses the hash to verify both parties poses the same secret plaintext without exposing it en route  if in doubt just add https :-     md5 hashes are now considered broken because collision might happen   wrong  the probability of accidental collisions was known when md5 was written what has changed is that techniques are now available to reduce the amount of effort required to generate a specific hash  if http digest auth is currently adequate for your purposes then continue to use it there are other far more serious / exploitable vulnerabilities in digest authentication  this is all described on  wikipedia,2011-07-03 10:26:43.757 UTC,146,6562333,md5 hashes are now considered broken because collision might happen is this problematic for http digest authentication,0.0273972602739726,4,so,authentication|digest|http|md5,http digest authentication md5 collision,3,attacks|exploit|vulnerability,0.39409318566322327
29319,,2018-08-05 11:19:52.533 UTC,88,51693839,i recently started to exploit alloca quite a lot since for caching it is ideal but my application is based on processing medium chunks of data say 1024 floats while assuming the size of stack is say 1mb the chance of overflow is minimal it would still be handy to be able to determine if there s enough space and sort of optimize   so is there a crossplatform way to determine the remaining size of stack,0.022727272727272728,2,so,c++|macos|stack-overflow|windows,how to calculate remaining size of the stack,2,exploit|stack overflow,0.3936794698238373
8092,,2019-07-06 04:06:30,57,213033,how can i configure sql-map to try payloads at random time delay?as an example time between first and second request can be 0.10s and second and third might 0.25 and so on  i know there is --delay option but it takes static value,0.07017543859649122,4,sse,black-hat|cyber-warfare|penetration-test|sql-injection|sqlmap,how to create random request time delay in sqlmap,3,sqlmap|sql injection|penetration test,0.3916718363761902
44652,,2015-07-20 16:01:58.76 UTC,101,31521148,i am using cdf of a normal distribution in my project in java but i will very often encounter very small numbers when i calculate the cdf of a very small interval which are very far away from the mean e.g cdf-10000.02 -10000.01 for a normal distribution with mean 1000 std 0.1 that will cause underflow of double because the smallest number 64 bit double can handle is merely e-308 or so i googled but failed to find packages that can handle this   any ideas?thanks!  peng du,0.0297029702970297,3,so,double|java|normal-distribution|statistics|underflow,double underflow in statistical functions cdf of normal distribution,1,underflow,0.38996821641921997
4363,"the thing about brute forcing a totp token is that you have to guess right  at the right time   so if you don t want to lock a user out after x failed attempts which is common practice you can slow failed attempts down to the point where the odds of getting the totp right are statistically insignificant  for example 6 digit totp token has 1,000,000 possibilities  but the correct key changes every 30 seconds in the standard implementation of totp so window of success is not how long it takes to exhaust the entire token space but how many tokens the attacker can get through in a given period divided by the tokenspace and its still only a  chance  at success  make it impossible to get through the token space in 30 seconds and you reduce the attack to a gamble  example the attacker gets one guess per second  they can only get through 30 guesses of any given token so the odds of success are basically random  using the bernoulli process of random variables chance of failure times number of guesses 24 hours of attacking this way 86,400 guesses yields only an 8.2% probability of success at 10 days it rises to 57%  so simply convince your user to change their password or find some other way to block the attacker by ip etc before that percentage gets very high",2016-04-13 14:25:33,381,120371,if i implement two factor authentication using totp i obviously have to protect against simple brute force attacks of the totp value  if i ask for the totp value after successful password login the attacker already knows the valid password since the default length for totp value is 6 digits it s easy to brute force  possible solutions   block user account disadvantage of blocking a legitimate user with no way of unblocking  block ip might work but might be bypassed by leveraging a botnet should block more than a single ip for ipv6   solutions that do not work   block current session doesn t work because the attacker knows the password and can just open another session    how can a brute force protection be implemented so it doesn t lock out the legitimate user but still blocks attacks as good as possible,0.03674540682414698,14,sse,attack-prevention|brute-force|totp,two factor brute force protection,4,bypass|attacks|protection|attack prevention,0.38929498195648193
26156,,2019-03-22 15:04:08.413 UTC,106,55302524,it is obvious that if we compare two strings it is vulnerable to the time attack  i m now wondering whether the time leakage is still a vulnerability if the comparison is done on the hash of those two strings?  isn t it vulnerable to any other kind of attacks? suppose using a hash algorithm which is safe against the collision attack  or simply what will happen if someone could find out the hash of the original string? can it be abused?  here is just an example of the idea,0.09433962264150944,10,so,comparison|hash|security|timing-attack,is the time leakage by comparing the hashes of two strings vulnerable,4,leak|attacks|timing attack|vulnerability,0.3891634941101074
24565,short answer  any algorithm that would allow you to detect whether or not 2 users had the same password would also allow an attacker to detect whether or not 2 users had the same password  this is effectively a precomputation attack  therefore your problem is not securely solvable  example   assume i ve compromised your password database   assume i ve figured out how your hashes are calculated   if i can apply your password transformation algorithm to password and quickly tell which users use password as their password then the system is vulnerable to a form of precomputation attack  if i must do an expensive calculation to determine the password for each individual user and work spent to calculate user a s password does not make calculating user b s password easier then the system is secure against these type of attacks  further consideration  your idea of using a per-site salt with bcrypt and a high iteration count may seem attractive at first but it just can t scale  even at 10 seconds that s 6 password guesses per minute 360 per hour 8640 per day or 3m per year that s a lot  and that s just one machine  throw a botnet of machines at that problem or some gpu s and suddenly that number goes through the roof  just 300 machines/cores/gpu s could knock out 2.5m guesses in a day  because you would be using the same salt for each one you re allowing the attacker to crack all of your user s passwords at once  by sticking with a per-user salt only the attacker can effectively only attempt to crack a single user s password at a time  the short answer given above makes the assumption that the attacker has the same access  as the server at all times which is probably not reasonable if the server is compromised in a permanent manner owned by the attacker then no scheme can save you - the attacker can retrieve all passwords as they are set by the user the model is more normally that an attacker is able to access your server for a  limited  period of time some point after it has gone live this introduces an opportunity to perform the password matching that you ve asked about without providing information that is useful to an attacker  if at sign-up or password change your server has access to the password in plain text then the server could iterate through all the user accounts on the system hashing the new password with each user s individual salt and testing to see if they were the same  this doesn t introduce any weaknesses but it would only be useful to you if your algorithm for preventing multiple fake accounts can use this as a one-time input this password matches these accounts  storing that information for later analysis would obviously be a weakness for if an attacker can obtain your database of passwords they can probably also obtain this list of accounts with the same password a middle ground might be to store the information for daily review - reducing the total useful information available to an attacker who temporarily compromises your storage  all of this is moot if the salting and hashing occurs client-side - then the server can t carry out the test,2013-01-15 20:05:23.423 UTC,827,14345923,on our message board we use password matching to help detect members with multiple registrations and enforce our rules against malicious puppet accounts it worked well when we had sha256 hashes and a per-site salt but we recently had a humbling security breach in which a number of password hashes fell to a dictionary attack so we forced a password change and switched to bcrypt + per-user salts  of course now password matching doesn t work anymore i don t have a formal education in cryptography or computer science so i wanted to ask if there s a secure way to overcome this problem somebody i work with suggested a second password field using a loose hashing algorithm which intentionally has lots of collisions but it seems to me that this would either lead to tons of false positives or else reduce the search space too much to be secure my idea was to stick with bcrypt but store a second password hash which uses a per-site salt and an extremely high iteration count say 10+ seconds to generate on modern hardware that way users with the same password would have the same hash but it couldn t be easily deduced with a dictionary attack  i m just wondering if there s an obvious problem with this or if someone more knowledgeable than me has any suggestions for a better way to approach things? it seems to me like it would work but i ve learned that there can be a lot of hidden gotchas when it comes to security :p thanks,0.02176541717049577,18,so,bcrypt|cryptography|hash|security,how to securely detect accounts with matching passwords,3,attacks|weakness|vulnerability,0.3878563344478607
55674,,2018-06-25 08:55:12.787 UTC,61,51019606,computing the product of two  -bits integers requires a  -bits result to do not overflow  well how would be possible to compute the product of two  -bits integers as two   bits integers   and   where   contains the lower half of the result and   contains the higher one,0.01639344262295082,1,so,bit-manipulation|c++|integer|integer-overflow|multiplication,compute product of two integers as lower and higher half,1,integer overflow,0.386398047208786
23775,before php 7 php use  linear congruential generator  algorithm to generate a random number or in short lcg the algorithm works as follow   when you first make a random obviously there is no previous_random number that s why we provide  seed  so seed is just a first previous_random value  now we know the algorithm but we need to know what the value of     and   that php use i believe that each version of php use different value for those but let say we do not know those value how do we guess this value in my case i am using php 5.6.15 windows   so   since our seed is 0 so our   to get  value   we can use simple loop to guess     or you can get value   like this   now i am able to write the same random function as my current php version   however  i was able to predict my own php version because i have so much knowledge about my current environment i know a few previous random i know the seed if the attacker has almost zero knowledge it would not be easy to attack  mersenne twister  php 7.0+ by default use  mersenne twister  there are more parameters to be guessed than linear congruential generator so it requires more knowledge  is linear congruential generator bad?  depends on how much information you have exposed to the public if you generate only one random number and attacker has no knowledge of       and   it is impossible for attackers to predict the next random number,2017-07-08 11:04:11.107 UTC,357,44985462,does anybody know if there was a time or event where somebody used rand s weakness in order to predict exploit it? something like generating tokens or cheating in video games?  since prior to php 7 rand was very easy to crack in fact here is some c code credit to peter selinger that predicts the values given a seed   so once again was there a time when this weakness was used in order to predict the next random number and exploit something?  thanks,0.03081232492997199,11,so,events|exploit|php|random|weak,was there a a time when php s rand function was used as an exploit,3,exploit|attacks|weakness,0.38617271184921265
4773,1 in a sense yes since checking the password against a stretched hash will take more cpu time but you should have some rate-limiting in any case to prevent brute-force guessing passwords and while doing that you can take into account the dos potential also login attempts will not be the only way to try and dos something you might get excessive load from other requests too or just receive excessive amounts of network traffic preventing legitimate requests from reaching you   i think making it harder to crack the passwords in case of a leaked password database is worth the cost but of course it is a trade-off you have to take into account the expected amount of valid logins and that the system can handle them with millions of users the stretching might start to take its toll  2 yes if you rate-limit connection attempts and start denying them outright at some point it would be useless work to run the hash fully for a login that is going to be denied though returning a failure immediately will reveal via timing the fact that the password wasn t even checked,2016-07-28 09:52:59,395,131243,disclaimer i noticed several questions on this site but none really answer this two questions directly therefor i do not consider this duplicate    personally i really like the article called  some words on password use salting and stretching  from michael anders explaining why stretching is important      for the legitimate user it doesn t make a difference whether the program needs a microsecond to verify the password or a tenth of a second he will not notice the difference for the attacker it may be the difference between 1 day for an exhaustive search or the 100 thousand fold time of 300 years    i wonder if extensive stretching increases the risk of a denial-of-service dos caused by resource exhaustion it s likely easier to use more server resources when doing multiple login-attempts not for the purpose of logging in brute-forcing but for the purpose of causing a dos  secondly would a simple brute-force protection that prevents the password from reaching the password check function including hashing and stretching after an x amount of failed login attempts possibly prevent this kind of dos by resource exhaustion,0.03291139240506329,13,sse,brute-force|denial-of-service|hash|key-stretching|passwords,does stretching a hash increase a denial-of-service dos risk and does brute-force protection mitigates that,4,leak|attacks|protection|denial of service,0.3855474293231964
1296,online bruteforce attacks against a properly designed system is probably unfeasible against all but the weakest passwords  this is due to the fact that online systems can implement a wide array of rate limiting techniques that will limit the number of attempts the attacker has to guess the password of a single account   of course there are some techniques available to try and circumvent the rate limiting techniques such as trying a single password against multiple user accounts or distributing the login attempts among multiple ip addresses to try and get past ip bans however i m not sure about the effectiveness of such techniques  there is another factor that limits the effectiveness of online bruteforce attacks the limiting factor in such attacks usually isn t the processing power that can benefit from large gpu farms the limiting factor usually is the network each individual network interface can only send out that many packets per second even if you can afford to have thousands of network interfaces sending out the necessary packets for an online bruteforce attack the server your target is hosted on probably cannot handle the millions of packets bombarding it at this point your online bruteforce attack has turned into a denial-of-service attack  assuming that you are talking about a web application it is probably easier for attackers to exploit other flaws such as sql injections to get a database dump and perform offline attacks using clusters of dedicated hardware,2013-06-10 04:06:47,296,37183,i am familiar with how offline brute-force attacks work but for  online  accounts assuming no social engineering how feasible is it to brute-force attack a password? for example is this dependent upon password complexity or possibly some other vulnerability like eavesdropping on ssl/tls handshakes,0.06418918918918919,19,sse,authorization|brute-force|passwords,feasibility of brute-force on online password,9,bomb|flaws|attacks|exploit|weakness|vulnerability|eavesdropping|sql injection|denial of service,0.3823927044868469
17471,never store passwords just its hash  salt shall never leave server generate salt and keep it as part of generated hash  use seed for added protection seed can be number of seconds from some predefined date till the date user created his login with the application   c# code sample,2016-01-24 15:47:37.027 UTC,152,34977786,i try to implement a authentification algorithm my basic sequence for now is the following       right now i doubt that this is the correct way to do it with the salt i wantto prevent rainbow tables but requesting the salt from the server would let a man in the middle attack get the salt and after it cracking the password with a rainbow table using the salt is easy  is the only way solving this a https connection or is it somehow possible to get it more save,0.02631578947368421,4,so,password-protection|passwords|security,how to authenticate client at server using a hashed password with salt,3,attacks|protection|man in the middle,0.38051313161849976
11059,"in short aes is reversable a hash function is not  in response to the accepted answer.. sorry i m a new user can t post comments yet... salting only prevents rainbow table based attacks it does not protect weak passwords to protect the weaker passwords you will need to use a hash function that has been proven to be slow a properly configured bcrypt is the easiest way to do this md5 and sha1 are too fast to be secure the collisions found with md5 are unrelated to this problem i m describing  all 8-character passwords encrypted with md5 or sha1 even when properly salted can be cracked by  this dude in a single day  salting does not prevent this kind of attack optimizing the attack to consist of only the ~500k words in the english language.. and the 10,000 most common variations of them will crack a huge number of passwords  bcrypt is stronger against this kind of attack because it can be configured to be millions of times slower than md5 iteratively using md5 a million times will theoretically achieve the same thing but i suggest you stick with well tested libraries instead of rolling your own implementation bcrypt uses salting as well of course and is available in most programming languages so no reason to not use it  in theory scrypt is better but its too new and therefore implementations are probably still a little buggy  long story short  sha512 vs blowfish and bcrypt    md5 message-digest algorithm 5  is a cryptographic hash function while  advanced encryption standard aes  is a symmetric-key encryption algorithm so they are used for different purposes a hash like md5 or sha is used to verify passwords because it s hard to invert that is to obtain the password from the hash-string an aes encryption on the other hand is invertible the original message can be obtained if you know the key so if multiple messages are encrypted with the same key knowing it exposes all of them whereas if you manage to find a hash s original stringrainbow tables etc you ve only discovered the plain text for that particular instance and you ll have to redo the work to find a sollution for another hash-string  the main reason why using symmetric or asymmetric encryption is not advisable for protecting passwords is  key management  when using encryption you must protect the encryption key or the entropies from which the key is derived and protecting the key is a very difficult task to solve hashing with sha md5 or any other algorithm solves the problem of key protection because you don t need to keep any secret value other than salt but salt is significantly less sensitive than encryption key you can store salt in plain text so if you only keep passwords for authentication purposes performed by your app there is absolutely no reason to use encryption hashing would do just fine however there may be cases when you need to be able to decrypt passwords e.g you may need to  pass users credentials to third party apps  this is the only case in which the use of encryption would be justified for password storage  because aes encryption is symmetric given a password encrypted with aes and the key you can decrypt the password this is undesirable because you almost always want only the owner of the password to know it and don t want to have an easy way to derive the password the sha and md5 algorithms on the other hand perform a mostly one-way transformation of the password there is no piece of information key that allows you to return the transformed password back to its plaintext form  the use of aes as a symmetric cipher for passwords would be a volation of  cwe-257  and there for a vulnerability   it is possible to use a  symmetric cipher as a hash function   old unix passwords use des as a hash function and newer unix systems use blowfish as a hash function   but even though its a block cipher  its being used as a one-way function  which is a requirement for any password storage system   for php you should use  sha256   if you store a password encrypted it can be decrypted since many people reuse passwords across many different systems this is a bad thing so you use a one-way function a cryptographic hash function - this way a password can be verified without actually being revealed  as greg commented aes is an encryption/decryption algorithm md5 and the sha family are hash functions which are the more appropriate ones to use but steer clear of  md5 nowadays - it s not really seen as secure enough any more  xiaoyun wang published an effective collision attack against it in 2005 and its strength is now seen as considerably below its design strength - thus in cryptographic terms it is broken  for best results the standard is to salt and hash a password to store it - google these terms in tandem and you ll find numerous references",2010-06-29 20:02:47.94 UTC,882,3144283,everywhere i have seen people talking about storing passwords in a database they have almost always used md5  what is wrong with aes or sha1,0.017006802721088437,15,so,encryption|php|security,why not use aes for password encryption in php,6,cwe|attacks|weakness|protection|weak password|vulnerability,0.3797237277030945
3863,i m not aware of any publicly known attack using collision in sha-1 but md5 collisions were probably used already 2010 within attacks in 2012 it was discovered that a malware from the flame attack had a valid signature from microsoft which was possible due to a md5 collision attack see  http://blogs.technet.com/b/srd/archive/2012/06/06/more-information-about-the-digital-certificates-used-to-sign-the-flame-malware.aspx  for more details  as for using md5 or sha-1 with passwords simple hash some trivial password with md5 or sha-1 and then look up the hash with google example   the first hit on google for  the md5 hash  presents you with the password as does the first hit when searching for  the sha1 hash  thus typical passwords can easily be detected as long as the hash is not salted  apart from that even sha-256 is a bad choice for passwords these kind of hash algorithms are designed to be fast which only makes brute-forcing passwords easier for more details about this topic see  how secure are sha256 + salt hashes for password storage   sha-1 absolutely has been successfully exploited in known attacks take a look at this excellent blog post on whitehatsec s website exploiting the use of sha-1 for url signing requests  https://blog.whitehatsec.com/hash-length-extension-attacks/   as for password storage i can speak from personal experience that it is trivial to crack sha1 crypt linux hashes from ldap/ldif dumps using oclhashcat current benchmarks of a single amd 5970 gpu put bruteforcing nsldap {sha} at 3418.0 m/s and nsldaps {ssha} at 3401.1 m/s which i ve used to bruteforce hashes of 8 character passwords in hours using standard password masks  md5 in particular is now considered  cracked  for a  very good reason      the md5 hash collision attack that hijacked the windows update system back in 2012 was replicated with just 65 us cents worth of cloud computing fees   it took about 10 hour of cpu time to do the attack  this particular attack used a chosen prefix attack which means you could format your data to prevent the attack from working but the mere fact that this could be done declares that the security of your hashing is no longer found in the hashing algorithm but in the data format you are hashing which is a very poor place to put your security guarantees  the flame malware used a forged microsoft update signature that used md5 the server in question had been neglected when it came to security updates and patches and the creators of flame also known as skywiper malware exploited this to pose their command and control servers as legitimate microsoft update servers it then infected computers with its modules one of which was referenced as flame in the source code hence the name the  link  to the report by the crysys lab is here,2015-11-30 15:04:14,549,106843,pretty much every guide how-to and reference for dealing with passwords and hashing has a warning in big or bold letters stating something along the lines of     sha-1 and md5 are  not  secure and should not be used   fair enough it s not much trouble to use sha-256 or something else but have there been any examples of said weaknesses actually being successfully used in an attack? just how much weaker do these vulnerabilities make the algorithms,0.04553734061930783,25,sse,hash|sha,have weaknesses in sha-1 and md5 ever actually been successfully used in an attack,6,hijack|attacks|malware|exploit|weakness|vulnerability,0.3796788454055786
27231,it s a little unclear to me what your actual question is but if it is how does a salt help protect me against brute force attacks? the answer is that technically it does not there is nothing about a salt that makes brute force attacks more difficult salts instead make it difficult to brute force multiple accounts simultaneously essentially salts artificially inflate search space required to do a brute force attack making it computationally difficult to pre-calculate every possible password and then check them against the entire database salts can be stored in the clear so long as they are unique to each password  if you want to make brute forcing passwords more difficult what you want is an adaptive hashing scheme these schemes allow you to dictate how long hashing should take because an honest client should only have to authenticate on the order of tens of times but an attacker will need to do it on the order of millions or billions of times slower hashes make the task near impossible for the attacker while introducing little overhead in the system   what this all boils down to is that you should use bcrypt if you are hashing passwords it is designed to incorporate a salt and is an adaptive hashing system for more info see  this article on security.stackexchange.com   without salt an attacker can use an offline attack to precalculate the hash of common passwords secret qwerty etc  no salt allows an attacker to tell when different users are using the same password as they will have the same hashes  salt prevents precalculation and avoids the matching hash problem  an attacker with access to the datbase will also have access to the salts  she will need to attack each password separately because of the different salts  using stretching repeated hashing can also slow down an attacker  rather than storing   you store   where   is large enough for the overall calculation to take at least 0.1 second   that limits the attacker to around ten trials a second while having no discernible impact on the user  the purpose of a salt is not to prevent dictionary attacks it is to prevent precomputation attacks such as rainbow tables having a salt requires the attacker to attack each password individually after they gain access to the database they can t precompute hashes for passwords in the dictionary or reuse this effort across users  password stretching is a way to make dictionary attacks more difficult by increasing the amount of work the attacker has to do to test each candidate password  about salt  if you search the md5 encrypted password using search engine like google here you may find the original plain password but if you mix the salt in your plain password and then apply md5 encryption you wont be able to find it if any hacker anyhow hacks your database and if you are using just md5 encryption then he may use above method to hack passwords for e.g search this string on google  5f4dcc3b5aa765d61d8327deb882cf99 you ll get original password string salt is mainly added to protect against such attacks  check out  here  look at just content and concept here to understand this is from spring security docs,2012-03-19 07:34:21.05 UTC,735,9766496,please help me with my understanding also i am not talking about ssl or dh key exchange.as the salt is stored in db and is a secret to the attacker to just protect the user original password rainbow tables in case attacker gets their hand on the actual db itself then how will how you protect against brute/dictionary based attacks once again logging the wrong requests and denying ip of many bad request is known i am talking about cryptography here as the password is same for user1 attacker got it from other websites how does salt protects here i guess not then what are the best solutions available to stop such attacks assume data is really important like credit card numbers + cvvi know don t store cvv but that is not the question   edit by the way i came up with some stupid idea and it looks like a known method for stopping dictionary attacks read more this question  high cost encryption but less cost decryption   may be we can discuss some other methods here to protect against brute/dictionary/social engineering password attack,0.047619047619047616,35,so,cryptography|dictionary-attack|hash|security,storing salt+password hash in db and protecting against password attack,3,attacks|protection|dictionary attack,0.3782467842102051
3729,add a random amount of random characters into the request body before gzipping it   this will slow down the attack but not prevent it  the idea behind breach and compression oracles in general is that the plaintext and therefore the ciphertext will be  slightly shorter  if the attacker guesses the next byte of the secret token correctly  if there is some randomness in the plaintext length the attacker can make the same guess many times and average out the results  requests made using the correct guess will be  slightly shorter on average   a real attacker would use a more sophisticated statistical technique to reduce the number of requests required but you get the idea.  if you have always the same amount of random data the attack will probably still work in the same way if you instead add a random amount of random data the original attack might not work any longer but i think it can be modified by simply trying again and again in the hope that the same amount of random data is added often enough which means it will slow down the attack only and how much depends on the amount of random data you add for example if you add between 1 and 10 random characters at the top of the file it will probably only slow down the attack by a factor of 10 if you add 1..100 characters the slowdown will be factor 100   a better approach would be to create some random value and modify the csrf token so that it will consist of this random value and an xor or with this value with the original csrf token this way the csrf token will change all the time inside the html so that breach will no longer work  and the application can easily get the original csrf token back,2015-10-22 06:03:17,389,103384,django supplies gzip middleware but the  docs  issue a stark warning about  breach  which i had hitherto forgotten about the first thought i had was that i should be able modify the middleware to first add a random amount of random characters into the request body before gzipping it and render breach useless is this correct or am i completely misunderstanding breach,0.03598971722365039,14,sse,compression|exploit|known-vulnerabilities,can breach be thwarted by simply adding a sort of salt into the page being compressed,4,exploit|attacks|known vulnerabilities|cross site request forgery,0.3764917552471161
31711,have you considered working in log-likelihood?    becomes  which will be much more resistant to numerical precision problems  and then you can directly use   in your,2017-04-21 00:21:44.257 UTC,153,43531953,i have a large vector of likelihoods all in the range 0 to 1 but all numbers are quite a bit less than 1 i need to compute the maximum likelihood of the product of these vectors   how can i avoid underflow my fitter is failing in all my attempts the first step i took was to divide my array by the max value in my array.i am maximizing the product of the sum of two probabilities sampled n times  ultimately i need to minimize as per the bic   anyways   is an array of really small numbers of the form   here is an example with two parameters   and   that i vary and the array has size   and each   is,0.013071895424836602,2,so,data-fitting|numpy|python|underflow,likelihood involving very small numbers,1,underflow,0.37626057863235474
67122,when checking strings for equality you need to check that every character matches most programming languages will take a short-cut and return false - or not equal - as soon as they find a single character that doesn t match for example   you know that they are not equal as soon as you hit that 0 why would you keep checking?  this is super exploitable imagine that i m trying to crack a password or a hash or a mac tag or anything that needs to be compared for equality knowing that the server will do a lazy string comparison somewhere inside i might get the following timing results completely made up   cool now i know that the first letter is  c  because it took longer now i can do the same trick with caaaaaaa cbbbbbbb etc cracking one letter at a time is hugely faster than cracking the whole password at a time  this lazy string comparison is generally a good thing for programmers because it makes things run faster but it s bad for security secure string comparisons will check all the way to the end even if it found a difference so that there s never a timing difference,2015-07-21 22:21:30,265,94577,could someone please tell me what a string comparison timing attack is in simple terms? i have googled this but all the explanations are very technical also is this attack an better than a brute force attack? please correct me if i am wrong but i believe this attack is used to crack passwords,0.026415094339622643,7,sse,attacks|password-cracking,string comparison timing attack in plain english,2,attacks|exploit,0.3756236135959625
1154,thi function is translat from the intern function in the r sourc code here not necessarili the most effici but you should be abl to do thi for >2 element by use quick test use valu that are larg enough not to underflow so that we can compar the result of the regular and logspac calcul as far as i know thi is more or less the standard approach to logspac addit the advantag of use someth other than thi home-grown implement would be 1 code matur and test and 2 speed at least for the version i doubt there would be much advantag of a native-c or whatev implement of the brobdingnag packag use similar represent in python numpi has logaddexp but thi onli work pairwis you can use to gener it as abov thi is probabl a littl faster than in r,2020-01-14 20:17:18,263,59741181,for some evil reason i need to calcul the log of the sum of 500 super small probabl each term comput by sometim the code abov return 0 due to underflow but use logarithm will be fine i know i can handl two term mathemat but can we gener thi approach to more terms? anyon with more experi on this? btw i wrote a recurs function to comput thi mathemat it work but i don t think thi is practic becaus some p are super small and i can onli use we have and for two and three term,0.011406844106463879,3,so,logarithm|probability|r|underflow,how to deal with the log of a sum of more than two super small probabl,1,underflow,0.37532007694244385
11922,when the virus database is downloaded    i putted an   before the   so you can make a non-destructiv test.if the output is ok remove it,2016-10-22 12:53:58.583 UTC,100,40192417,so i have done my research on md5 and found  this which piesub quoted md5 generation code   which when i try to generate md5 code for the files in a certain directory this batch md5.bat was placed in the result looks like this   so basically i want a script that compares the md5 hash with  this  md5 database and deletes any malicious file,0.05,5,so,batch-file|hash|malware|md5|virus,how to make a md5 batch virus scanner,3,virus|malware|malicious file,0.37461262941360474
60665,you could try doing a sum of,2018-04-30 20:36:41.733 UTC,37,50107852,i m selecting   from a big period of time logs and it is getting negative values how do i handle it,0.05405405405405406,2,so,integer-overflow|logparser,how to handle integer overflow in logparser,1,integer overflow,0.374533474445343
11832,using bcrypt to handle passwords is the only step or rather encompasses the following   take password provide it to bcrypt library  store resulting hash  compare password to hash   you also forgot this link  http://codahale.com/how-to-safely-store-a-password/  which is what you reference with the quote  nice work! looks very complete to me  only suggestions i would have are   rotate the service salt   design a method to periodically rotate the service-wide salt and exercise it regularly  for example after generating a new service salt use it for all new accounts &amp any password changes when an existing user tries to log in authenticate them with the old service salt if successful update their hash with the new service salt and optionally a new user-specific salt for users who don t log in for  some time  randomly generate a new password on their behalf this will  keep up  security for users who ve abandoned your site forcing those that return to use the password reset facilities  some time  = whatever period you re comfortable with   don t hard-code your service salt   don t allow a lfi attack to compromise your service salt feed the service-salt to your application at start up and keep it in memory to compromise the service salt an attacker would need to be able to execute code to read the salt from memory if an attacker can do that you re pretty well hosed anyway =   don t reuse a users salt   look for opportunities to give users new salts user changes his password? generate a new random salt this further hampers brute forcing your server-wide salt should an attacker be able to obtain his hash whenever he feels like it couple this with regularly rotating your service-salt and i d wager you ve got a strong deterrent against brute-forcing  marking this as a community wiki should others have additional ideas,2011-06-24 07:21:46.207 UTC,715,6464662,"have i missed anything? are there any additional steps storing passwords to the db?   storing the password  &nbsp;&nbsp;&nbsp;&nbsp;after as much research on the subject as possible i ve come to the conclusion that the best way to store user passwords in a web application db in my case mysql+php is as follows   assign a sitewide static salt 16 rand chars incl 0-9,a-z,a-z,[]/*-   assign a per user random salt stored in the db  store the result hash_function$userpassword + $sitewidesalt + $randomsalt  store the $randomsalt alongside the resulting hash  use bcrypt adjustable workloadhashing       attack #1   attacker dumps the db via sql injection  &nbsp;&nbsp;&nbsp;&nbsp;db results ofour hash_function and the random peruser salt    after the dump the attacker couldobtain  $userpassword  and $randomsalt  by looking up his own account then by guessing the hashfunction such as md5 he could start arainbow attack on the $sitewidesalt  but that could take up to 1.41 hundred millioncenturies[1]  by using this type of security   does not allow a dump of the db to compromise stored passwords   the user still has to find the  $sitewidesalt  through another method       attack #2   attacker finds a local file inclusion lfi vector  &nbsp;&nbsp;&nbsp;&nbsp;attacker could obtain the raw code for our web application        after exploiting the web application    through a possible lfior rfi[2] the    attacker reads thesource code for    our webapplication and obtains our    simplealgorithm and the stored  $sitewidesalt       where to next?  &nbsp;&nbsp;&nbsp;&nbsp;now the attacker has both of the salts he can begin to rainbow to obtain the actual passwords except he must make 1 rainbow table  for each  user as each user has a different random user specific salt $randomsalt      a modern server can calculate the md5  hash of about 330mb every second if  your users have passwords which are  lowercase alphanumeric and 6  characters long you can try every  single possible password of that size  in around 40 seconds  ...cuda you can put together your own little supercomputer cluster which will let you try around 700,000,000 passwords a second...[3]   &nbsp;&nbsp;&nbsp;&nbsp;what we need to do now is extend the hashing function by using a time consuming algorithm such as bcrypt bcrypt s work load factor can be 5-6 orders of magnitude that of the simpler hashing functions cracking just one password can take years instead of minutes and as a bonus bcrypt already generates a random salt for each hash and stores it in the resulting hash    http://www.grc.com/haystack.htm     http://www.wildcardsecurity.com/security101/index.php?title=local_file_inclusion",0.022377622377622378,16,so,hash|passwords|salt|security,web application - storing a password,4,attacks|exploit|hard coded|sql injection,0.37349405884742737
25030,"yes it can be used to break anything except the password used to open a file  the reason it works is that there is a vulnerability in the way excel hashes passwords so there are actually only 194k different hash values   detailed discussion  the reason is that the passwords you enter i.e with tools/protect/protect worksheet or /protect workbook are not used directly in protection instead they are hashed mathematically transformed into a much less secure code effectively any password of any length is transformed into a string of 12 characters the first 11 of which have one of only two possible values the remaining character can have up to 95 possible values leading to only   potential passwords this may seem like a lot but it only takes a few seconds for a modern computer to try them all as a comparison a 4-character password containing just the 26 lower case alphabet characters has 456,976 combinations and a 3-character password consisting of lower case upper case and the digits 0-9 will have 238,328 combinations  again it doesn t matter what your original password is one of those 194k strings will unlock your sheet or workbook",2015-12-10 15:42:00.357 UTC,264,34206015,i have had to use this code to remove passwords from protected sheets that no one knows the password to i am interested to find out how it actually works and whether or not it can be tweaked to remove passwords from other excel bits eg workbook structure or workbook passwords,0.030303030303030304,8,so,excel|excel-vba|password-encryption|vba,how does this password breaker work? can it also be adapted to suit other things,4,weakness|protection|vulnerability|secure coding,0.3729746639728546
24830,"iterate over an hmac with a random salt for about a 100ms duration the salt needs to be saved with the hash use functions such as password_hash pbkdf2 bcrypt and similar functions the point is to make the attacker spend a lot of time finding passwords by brute force  see owasp open web application security project  password storage cheat sheet  see  how to securely hash passwords the theory  on security stackexchange  just hashing even using an hmac with a random salt is not enough on my laptop i can run a 20 byte password through sha-512 and compare in under 1us so with just a sha-512 hash i can try 1,000,000 passwords a second  for more information see password list at  seclists  infosec  password-cracking-tools  arstechnica  how i became a password cracker",2016-07-02 13:39:51.05 UTC,347,38160805,i have a database i am converting from storing plain text passwords to replacing them with password hashes is it recommended to take the extra step of using sql server column encryption to prevent the hashes being extracted by an attacker ?  the form of attack i am trying to prevent is where an attacker uses a well known password list combined with an copy of my database user table and just does a dictionary attack by hashing the well known password list and comparing the outcome with the stored hash they would probably get a certain percentage of success and with normal levels of password re-use amongst users they would be able to use the user email address/login with the cracked password to attempt an identity theft   initially i had considered hashing of passwords to be adequate but wonder if its best practise then to encrypt the hash column to prevent a physical copy of the table got through whatever devious means being able to be used for this kind of attack   my question is whether this is best or common practise amongst dbas this could apply to any hash storage of known data fields but in this circumstance its held within a sql server database,0.025936599423631124,9,so,sql-server,hashed passwords and identity attacks - best practise,3,owasp|attacks|open web application security project,0.3724614083766937
66845,in your example no most sites use password based key algorithms which are a form of hash function that takes as input the plaintext string the actual password the result from this functions is commonly a fixed length string output can vary with sha3 and so any attacker looking at the stored passwords sees the same length regardless of actual length of original string      can using very long complex passwords with online accounts be detrimental?    i m certain that the example password above would survive any dictionary attack out there as well as delay success of brute-forcing for quite some time however it is not very user friendly and would have to be written down somewhere in either electronic or physical form   both of which have weaknesses writing it down is prone to human error while electronic form on a device connected to the internet is prone to theft remotely of course both can also be stolen by hand..  so to answer your question yes it can be detrimental while it strengthens one security aspect it completely fails on the other  p.s agree with the answers above regarding hashing 1 char or 500 chars password would provide same length hash string  all passwords are hashed so the attacker sees the same length.bear in mind that if a password is brute forced the attacker will get the 1st password with the same hash which is almost certainly not the 500 character password if there is a 128bit hash there must exist a password with around on average 16 characters with that hash so these extra characters frankly provide no extra security dangerous - no from a pure computational perspective,2017-07-02 14:32:41,365,163254,in the case of data breach long complex passwords may draw more attention than simpler ones and the attacker might prioritize accounts with the most sophisticated passwords  given that  it is generally accepted that 128- bit of entropy is beyond anyone s capability to brute-force  very complex passphares most likely won t provide additional security but is there a reason to believe such passwords can even be dangerous to use?     a complex 500-character password,0.019178082191780823,7,sse,data-leakage|passwords,can using very long complex passwords with online accounts be detrimental,3,attacks|weakness|data leakage,0.37224167585372925
1194,a malware file can have different hash value at different intervals of time as the malware operates automatically  the hash value of the file may change consequent to any change in the file however if the data remains write protect no change would take place and the hash value of the malware file would also remain the same  if you run a given hash algorithm across a given file you will always get a given value as a result   the only way you can get multiple hash values for a given file is by using a different algorithm sha-2 instead of md5 for example  it is common for malware to attempt to avoid detection by adding arbitrary data to the end of copies making the hash of each copy unique but note that in this case each copy of the malware will still only have one hash  for a given hash algorithm and file a hash is unique it s the other way around a hash can be generated from an infinite number of file inputs but a single file gives a single hash  it has to do with how the hash works nothing to do with being malware a hash is  just a mathematical functionone input one output it is not smart enough to know that the input is malware  when you talk about malware most of the time there are multiple variants of the same malware. the working of the malware the exploit the payload/shell are all the same only a few instructions are changed for the purpose of either avoiding detection or communicating with a different command and control centre therefore a single variant of a family of malware will always have one single hash or signature if you are talking about different variants of a single family of malware then every single one will have a different hash value,2013-04-18 04:04:27,331,34519,can a malware file have multiple hash values,0.0513595166163142,17,sse,hash|malware,multiple hash values in a single malware file,3,malware|exploit|protection,0.37166649103164673
9607,i m not a cryptography expert however there are 3 things in particular that strike me as possibles issues with this suggestion   as mark points out the email may change however the salt needs to remain the same for a given password otherwise you won t be able to check the validity of the password  the size of email addresses is variable and i imagine that it is important that the salt be larger than a certain size  using an email address makes the salt  much  more predictable which is usually a bad thing in cryptography   i don t know if any of these is an issue or not however the thing about cryptography is that often  nobody  knows until someone has devised an exploit and by then its too late - so my advice would be to err on the side of caution and not use email addresses as salt  ophcrack which is what most attackers would probably use depending on your encryption function doesn t contain tables with special characters like    or  @  unless you get to the biggest extended tables so using an email would probably be better than many other salts  like all things security-related the answer depends on the question which didn t include information on  how secure  you want the system to be the most secure building is one with no windows or doors it s also the most useless building  from the highest level no it s not a bad idea it s not a great idea either however it may be good enough -- or more than good enough -- for your application  if someone has a rainbow table for a specific email address are you going to be able to stop them by hashing a password with a random salt? good hackers take the path of least resistance which may include getting root access to your system and downloading the salt and user tables are they separately secured? if so they have until a password change to match a hash regardless of the system-enforced consecutively failed login attempt limit or what you chose for salt  how much more complexity arises from random salt in your application? how determined a hacker are you trying to thwart? what other measures -- maximum consecutive failure lockout forced password expiration periods ingress and dos alerts firewalls etc -- do you have in place? the answer lies somewhere in the convergence between the answers to those questions and maybe others as well  as others already mentioned salt should best be random the purpose of a salt is to prevent rainbow table attacks using pre-computed hash dictionaries  assuming an attacker gets to know the hashed passwords and salts from your database if the salt is a74kd%$qau and the password is 12345 will he be able to crack it using a rainbow table? no even if the password is weak the attacker won t have a pre-computed hash dictionary at hand for your random salt  if you however use a non-random salt like the user id or email it is somewhat more likely that someone already created a rainbow table for that salt hoping to find a user with username john or the email john.doe@example.com 1    1 wpa security for wlans uses the ssid of the access point as a salt too bad someone already  pre-computed hashes  for the most frequent ssid names   edit  let me refer you to this  answer on security stackexchange  which explains a lot of details about password hashing and key derivation   bottom line  use a secure established password hashing scheme that is somehow resource-intensive to protect against brute-force attacks but limit the number of permitted invocations to prevent denial-of-service dos attacks  if your language library has a function for it verify on upgrades that it does what it is supposed to do especially if it s php   the answer below is left for historical reasons   you could use the user s login name as a salt which might be less likely to change than an e-mail address  edit   0xa3  correctly pointed out this is less secure than using the e-mail address because login names tend to be easier to guess and some are quite commonly used such that rainbow tables may already exist for them or could be reused for other sites  alternatively have a database column where you save the salt for the password but then you could as well use a random user-specific salt just as well which is harder to guess  for better security you could use two salts a user-specific one and a system-wide one concat them then hash the salts with the password    by the way simple concatenation of salt and passwords might be less secure than using  hmac  in php 5 there s the     function you can use for this    edit  rationale for a system-wide salt it can and should be stored outside the database but  back it up  you won t be able to authenticate your users if you lose it if an attacker somehow gets to read your database records he still cannot effectively crack your password hashes until he knows the system-wide salt   edit slightly off-topic  a further note on the security of password hashes you might also want to read  why do salts make dictionary attacks  impossible ?  on hashing multiple times for additional protection against brute-forcing and rainbow table attacks though i think that repeated hashing may introduce additional opportunities for denial-of-service attacks unless you limit the number of login attempts per time   note   considering the rise of multi-purpose multi-core systems graphics cards programmable micro-controllers etc. it may be worth using algorithms with high computation effort along with salts to counter brute-force cracking e.g using multiple hashing like pbkdf2 however you should limit the number of authentication attempts per time unit to prevent ddos attacks   one more thing  another main rationale for using a custom hashing built on widely-used standards rather than a widely-used pre-built function was php itself which has proven itself to be not trustworthy at all when it comes to implementing security-related stuff be it the not-so-random random number generators or a    function that does not work at all  under certain circumstances thereby  totally bypassing any  benefits that a compute- or memory-intensive password hashing function ought to bring due to their deterministic outcomes simple hash functions are more likely to be tested properly than the outputs of a key derivation function but your mileage may vary  to increase security it would be better to use a random salt email addresses can be found out quite easily thus reducing the effectiveness of the salt clarified in comments,2010-09-24 13:07:29.383 UTC,1131,3787346,is it a bad idea to use an email address as the  salt  for a password,0.01856763925729443,21,so,passwords|php|salt,email address as password salt,7,ddos|bypass|exploit|attacks|weakness|protection|denial of service,0.37153002619743347
55002,i ve left some comments inline as i read your function this isn t an analysis after reading the whole thing rather this is what i immediately think of as i read it   so i would highly recommend separating this out into two separate mechanisms one that calculates macs and the other that compares strings in constant time like php 5.6 s     does,2015-10-25 14:12:01.557 UTC,203,33330645,a way to prevent timing attacks for hash string comparison is to perform additional hmac signing in order to randomize the verification process see  https://www.nccgroup.trust/us/about-us/newsroom-and-events/blog/2011/february/double-hmac-verification/    in addition to the second hmac hashing for each hash a random salt of random length is added to both in order to make the hashing timing / process even less predictable  my implementation of this look like this   the function is called in this way after decrypting the encrypted text in order to verify the decryption result   will this successfully prevent a timing attack? am i doing anything unnecessary? could something be improved?  a second question is whether it is more advisable to perform the hmac verification on the plaintext the ciphertext or both as in my example and why,0.019704433497536946,4,so,hash|hmac|php|security|timing-attack,php double randomised hmac verification to prevent timing attack,2,attacks|timing attack,0.37140360474586487
7723,i think you misunderstand the purpose of salts   the purpose of a salt is not to make cracking an individual password using brute force any more difficult!    the actual purpose is simply that two users with the same password should not have the same hash   if you could look through a database seeing 3 users with the same hash it would be likely that this is a common/weak password it would not be possible to see that these users have the same passwords if salts are used pre-computed hash tables are also pretty much useless when passwords are salted   if an attacker has compromised your system you will not get additional security by having an extra database keep things clean obfuscation is not security     now attacker will be able to build a custom rainbow table for that salt to run attacks   i don t think you understand what a  rainbow table  really is and how much effort it is to create one basically a rainbow table is a trade-off where one invests lots of time and memory initially and then can crack lots of password hashes faster as long as the relevant hash is covered by the rainbow table to crack a single password it is way faster to just brute-force it instead of creating a new rainbow table with the specific salt the protection against attacks based on rainbow tables is thus not to hide the salt but to use unpredictable and sufficiently long salts so that the attacker does not have already a pre-built rainbow-table  in other words there is no need to protect the salt against the attack you imagine,2019-02-22 04:26:52,582,204056,i read many answers on the stackexchange about bcrypt salt and where exactly to place the salt for hashes such as bcrypt it is trivial to extract salt from the hash so it doesn t matter really where you place the hash but i am assuming here that a hash is generated such as  or something as such  most people recommend that the best way to put salt is in a different column or alongside with the password hash itself as far as i understand this also improves performance for credentials validation easily and also i assume the server side ensures different salts are used to hash different passwords so in case of database is leaked attacker will have to compute rainbow tables for all the hash+salt which is tedious  but i am considering an edge case here let s assume the attacker is specifically interested in cracking password for one particular user attacker dumps the database gets access for the hash and salt as both are either placed alongside each other or in different columns now attacker will be able to build a custom rainbow table for that salt to run attacks  so assuming if you are going to do a trade-off between timing to check password &amp security isn t storing salt alongside with password a bad idea?  wouldn t you prefer using a secondary isolated database just for storing the salt with respect to id field to pin point which id the salt is applicable to in the   table in main database?  would not that be a better option rather than the option which i see daily on internet everywhere? or are there any other better options than this ? i do not mind trade-offs between timing/performance of the webpage &amp security,0.02577319587628866,15,sse,hash|password-management|passwords,is storing the bcrypt salt in isolated database a good idea,4,leak|attacks|protection|weak password,0.3694215416908264
37018,entropy of a file is calculated based on it s randomness in other words more unpredictable a file is more is the entropy value the values in cuckoo sandbox analysis report are calculated through shanon s formula and might range between 0 to 8 more closer the entropy value is to zero less is the randomness in bytes of file same and vice-versa it helps in decision making process during the analysis of a sample for it s malicious activity high value of entropy might mean the file is either encrypted strongly or tightly packed.for more information go through the link: https://www.talentcookie.com/2016/02/file-entropy-in-malware-analysis,2016-01-04 06:40:12.473 UTC,168,34585913,when i submit a file for analysis in cuckoo sandbox i always see at the report under section that every section has entropy column  i got values like 0 7.91863415033 4.4345104565  what this entropy mean? i tried to search on google any information about entropy of section but i didn t find something helpful,0.011904761904761904,2,so,analysis|entropy|malware|sandbox,entropy of sections in cuckoo sandbox analysis report,1,malware,0.3693009614944458
14571,in simple terms the vunerability is about guessing a really long password which is the key used to encrypt your session state amongst other things?  imagine you wrote a routine to check a password   this would allow a timing attack on the password algorithm because you can check a character a time because it takes longer the more correct your password is ie.imagine the password is  carrots   calling checkpassword ca  will take longer than checkpassword aa  so you can iterate through the character at a time  because somewhere in the asp.net stack there is a bad implementation like this adding a random sleep helps throw out the timing attack.. but it is not perfect i imagine  for more information see   http://en.wikipedia.org/wiki/timing_attack   this is to prevent people constantly triggering your error page and exploiting  the recent asp.net vulnerability  they need a large number of failures to take advantage of this exploit  the sleep delay will not  prevent  access to your page think of it as being analogous to brute forcing a password if you have to wait 5 seconds between guesses instead of 5ms you will take a little more time to find the pw,2010-09-28 02:08:07.707 UTC,240,3809206,i cannot figure out how can a random small sleep delay can be a solution to prevent an attacker from probing our site  this is his code snippet,0.029166666666666667,7,so,asp.net|asp.net-mvc,why did scott guthrie suggest us to use a random small sleep delay in a error.aspx,3,attacks|exploit|vulnerability,0.3689896762371063
15347,"see  this question  for a discussion on the same theme in a nutshell collision attacks do not directly harm hmac but the existence of collision attacks implies that the compression function on which the hash function is built is not a random oracle and this voids the hmac security proof  the strongest attack known against hmac is based on the frequency of   collisions for the hash function h birthday attack [pv,bck2] and   is totally impractical for minimally reasonable hash functions  as an example if we consider a hash function like md5 where the   output length equals l=16 bytes 128 bits the attacker needs to   acquire the correct message authentication tags computed with the    same  secret key k! on about 2 64 known plaintexts  this would   require the processing of at least 2 64 blocks under h an   impossible task in any realistic scenario for a block length of 64   bytes this would take 250,000 years in a continuous 1gbps link and   without changing the secret key k during all this time  this attack   could become realistic only if serious flaws in the collision   behavior of the function h are discovered e.g  collisions found   after 2**30 messages such a discovery would determine the immediate   replacement of the function h the effects of such failure would be   far more severe for the traditional uses of h in the context of   digital signatures public key certificates etc.  note this attack needs to be strongly contrasted with regular   collision attacks on cryptographic hash functions where no secret key   is involved and where 2 64 off-line parallelizable ! operations   suffice to find collisions  the latter attack is approaching   feasibility [vw] ***while the birthday attack on hmac is totally   impractical  in the above examples if one uses a hash function   with say 160 bit of output then 2 64 should be replaced by 2 80.*   a correct implementation of the above construction the choice of   random or cryptographically pseudorandom keys a secure key   exchange mechanism frequent key refreshments and good secrecy   protection of keys are all essential ingredients for the security of   the integrity verification mechanism provided by hmac  the security implications of hmac are described in detail in the security section of  the rfc  in a nutshell a very strong attack indeed is required before the security of the hmac is threatened the existing collision attacks on sha-1 certainly don t constitute such hmac is specifically designed to make attacks difficult and ordinary collision attacks won t generally suffice     the security of the message  authentication mechanism presented  here depends on cryptographic  properties of the hash function h the  resistance to collision finding  limited to the case where the initial  value is secret and random and where  the output of the function is not  explicitly available to the attacker,  and the message authentication  property of the compression function  of h when applied to single blocks in  hmac these blocks are partially  unknown to an attacker as they contain  the result of the inner h computation  and in particular cannot be fully  chosen by the attacker   i recommend reading the whole section it goes into more detail about exactly what attacks  would  suffice to break an hmac and just how much effort would be required",2010-07-26 11:53:20.363 UTC,578,3334524,is the security of the hmac based on sha-1 affected by the collisions attacks on sha-1,0.03806228373702422,22,so,cryptographic-hash-function|cryptography|hashcode|hmac|sha1,hmac security - is the security of the hmac based on sha-1 affected by the collisions attacks on sha-1,3,flaws|attacks|protection,0.36874276399612427
12418,"this is fine for  a single request  if the only side channel observable by the attacker is the response time  however if an attacker makes enough requests this random delay could average out as noted in  @scott s answer  citing  ircmaxell s blog post      so if we needed to run 49,000 tests to get an accuracy of 15ns [without a random delay] then we would need perhaps 100,000 or 1,000,000 tests for the same accuracy with a random delay or perhaps 100,000,000 but the data is still there   as an example let s estimate the number of requests a timing attack would need  to get a valid 160 bit session id like php  at 6 bits per character which gives a length of 27 characters  assume like the  linked answer  that an attack can only be done on one user at once as they are storing the user to lookup in the cookie  taking the very best case from the blog post 100,000 the number of permutations would be    on average the attacker will find the value halfway through the number of permutations  this gives the number of requests needed to discover the session id from a timing attack to be 86,400,000 this is compared to 42,336,000 requests without your proposed timing protection assuming 15ns accuracy like the blog post  in the blog post taking the longest length tested 14 took 0.01171 seconds on average which means 86,400,000 would take 1,011,744 seconds which equates to 11 days 17 hours 2 minutes 24 seconds     could a random sleep prevent timing attacks?   this depends on the context in which your random sleep is used and the bit strength of the string that it is protecting if it is for keep me logged in functionality which is the context in the linked question then it could be worth an attacker spending 11 days to use the timing attack to brute force a value however this is assuming perfect conditions i.e fairly consistent response times from your application for each string position tested and no resetting or rollover of ids also these type of activity from an attacker will create a lot of noise and it is likely they will be spotted via ids and ips  it can t entirely prevent them but it can make them more difficult for an attacker to execute it would be much easier and better to use something like     which would prevent timing attacks entirely assuming the string lengths are equal  your proposed code   note that the php     function  is not  cryptographically secure     caution  this function does not generate cryptographically secure values and should not be used for cryptographic purposes if you need a cryptographically secure value consider using     instead   this means that in theory an attacker could predict what   was going to generate and then use this information to determine whether the response time delay from your application was due to random sleep or not  the best way to approach security is to assume that the attacker knows your source code - the only things secret from the attacker should be things like keys and passwords - assume that they know the algorithms and function used if you can still say your system is secure even though an attacker knows exactly how it works you will be most of the way there functions like   are usually set to seed with the current time of day so an attacker can just make sure their system clock is set to the same as your server and then make requests to validate that their generator is matching yours  due to this it is best to avoid insecure random functions like   and change your implementation to use   which will be unpredictable  also as per ircmaxell s comment   is not granular enough as it only accepts an integer to represent the number of seconds if you are going to try this approach look into     with a random number of nanoseconds  these pointers should help secure your implementation against this type of timing attack     this kind of approach can totally prevent timing attacks? or just make the work harder?   neither it doesn t prevent timing attacks nor does it make them any more difficult at all  to understand why look at the  docs for sleep  specifically the meaning of the first parameter     halt time in seconds   so your app takes 0.3 seconds to respond without sleep with sleep it takes either 0.3 1.3 2.3 etc..  so really to get the part we care about the timing difference we just need to chop off the integer part   but let s go a step further let s say that you randomly sleep using  usleep  that s a lot more granular that s sleeping in microseconds  well the measurements are being made in the 15-50  nano second scale so that sleep is still about 100 times  less  granular than the measurements being made so we can average off to the single microsecond   and still have meaningful data  you could go further and use  time_nanosleep  which can sleep to nanosecond scale precision  then you could start fuddling with the numbers  but the data is still there the beauty of randomness is that you can just average it out   run that enough times and you ll get a nice pretty graph you ll tell that there are about 10000 different numbers so you can then average away the randomness and deduce the private 15  because well-behaved randomness is unbiased it s pretty easy to detect statistically over a large enough sample  so the question i would ask is  why bother with sleep-like hacks when you can fix the problem correctly?  anthony ferrara answered this question in his blog post  it s all about time  i highly recommend this article     many people when they hear about timing attacks think well i ll just add a random delay! that ll work! and  it doesn t",2015-02-08 15:23:38.083 UTC,1081,28395665,from  wikipedia      in cryptography a timing attack is a side channel attack in which the  attacker attempts to compromise a cryptosystem by analyzing the time  taken to execute cryptographic algorithms   actually to prevent timing attacks i m using the following function taken from  this answer    but i was thinking if is possible prevent this kind of attack using a random sleep like   or maybe augmenting the randomness of sleep   this kind of approach can totally prevent timing attacks? or just make the work harder,0.028677150786308975,31,so,cryptography|php|security|timing|timing-attack,could a random sleep prevent timing attacks,3,attacks|protection|timing attack,0.36843249201774597
47726,in the standard library you can look at the   module   if you need more functions than   supports you could look at the library   which i use and like a lot   but if possible you should see if you can recast your equations to work entirely in the log space  try calculating in logarithmic domain as long as possible i.e avoid calculating the exact value but keep working with exponents  exp-1200 is a very very small number just as exp1200 is a very very big one so maybe the exact value is not really what you are interested in if you only need to compare these numbers then logarithmic space should be enough,2012-10-29 02:26:00.797 UTC,171,13115176,i am trying to calculate the exponential of -1200 in python it s an example i don t need -1200 in particular but a collection of numbers that are around -1200   it is giving me an underflow how may i go around this problem?  thanks for any help ,0.011695906432748537,2,so,exponential|math|python|underflow,exponential of very small number in python,1,underflow,0.3683173656463623
27073,i found in it     if you only care password in   model it maybe 191 read this    django s default user model   authenticates via username and password as stated in  docs      the raw password is not stored in the database instead a hashed  version of the password is stored and the hash is computed each time  a user attempts to log in   the default password hasher is  computing expensive  to increase the difficulty of attacks which use brute-force methods     django does not impose any maximum on the length of the plaintext  password meaning that an attacker can simply submit arbitrarily large  -- and guaranteed-to-fail -- passwords forcing a server running django to perform the resulting expensive hash computation in an  attempt to check the password a password one megabyte in size for  example will require roughly one minute of computation to check when  using the pbkdf2 hasher      this allows for denial-of-service attacks through repeated submission  of large passwords tying up server resources in the expensive  computation of the corresponding hashes   and in the proper model   password field is defined with,2019-07-06 11:05:16.79 UTC,248,56913501,my problem is that i don t feel sure because i don t understand how work the password field in django that is to say i think that this field hasn t got any max_length and it can be attacked with a buffer overflow is this possible?  i am using the default django user,0.028225806451612902,7,so,django|django-forms|django-models|python,what is it the max_length in the password of django user,4,attacks|buffer overflow|denial of service|plaintext password,0.3681250214576721
25167,there are several weaknesses in your scheme actually you encrypt the password and throw away the key   you take only 5 base64 encoded characters as the key so anattacker needs about 1 giga tries to crack the hash in the databasefor comparison everybody can crack 8 giga md5 hashes per second.this cracked database-hash can then be brute forced with the  scheme which is also ways too fast  since you do not add a random salt in the md5 part an attackercould prepare rainbow-tables for specific accounts like useradmin then the only protection is the weak encryption part   actually there is no advantage over the standard way with the  password_hash  function this function will produce a bcrypt hash and the cost factor determines the needed time for calculation to thwart brute-force attacks     is this password encryption safe enough?   probably not but it depends on your threat model  two users with the same password will have the same entry in the password database that s a loss in prp-security pseudo-random permutation that is an attacker will be able to distinguish a random answer from a real answer          you should probably look at openwall s  portable php password hashing framework phpass  and john steven s  secure password storage  steven even takes you through the threat model and explains why you do things,2014-02-28 10:48:51.62 UTC,296,22092979,i was thinking of doing a simple password upgrade to a system that at the moment has something like this   the password will pass through a javascript hashing algorithm before being sent and this always makes a three characters password about 20+ characters  i thought using the ends of the hashed password as key and iv to encrypt the rest should should suffice   am i correct,0.02364864864864865,7,so,encryption|md5|openssl|passwords|php,is this password encryption safe enough,3,attacks|weakness|protection,0.36809563636779785
47151,it is usually more efficient and has less overflow problems to compute the quotient of the terms and use that quotient to update the terms in each step   compressed the term of index   is   so that the quotient to the last term is   and    which then allows to reformulate the summation function as    you failed to supply the error message but i m betting it s in computing   from the factorials within the   loop  there are several ways to get around this  one is to switch the order of operations such that you eliminate common factors rather than starting with 58! which exceeds the default integer limit  this involves a bit more coding or perhaps calling a combined factorial function such as one the will do cn k -- @lutzl mentions the binomial function  granted you aren t doing  quite  a binomial but you could use it and then adjust the numerator as needed for the   factors  another way is to switch to large integers see documentation for numpy &amp scipy so that you solve the problem in all packages,2017-05-31 16:15:47.8 UTC,232,44289857,i am having trouble calculating this the code works for all values of n up to and including n = 57 but throws an overflow error 34  result too large  for n greater than or equal to 58 is there a way around this? thanks,0.01293103448275862,3,so,exponentiation|integer-overflow|math|python,overflowerror during exponentiation,3,overflowerror|overflow error|integer overflow,0.3661413788795471
64768,password hashing works on  preimage resistance  the password hashing process should be such that it is not computationally feasible to find an input a password matching a given output the hash save by trying a lot of potential inputs and get lucky the get lucky attack is still a concern because passwords are chosen by human users who are not as imaginative and random as could be wished for which is why we need more than a simple hash function we need salts we need slowness see  this  however  collisions  have no impact on the security of password hashing the ability to generate collisions at will does not give extra cracking power to the attacker  as others have pointed out collisions can be used for attacks in other setups involving  signatures  these are scenarios where the attacker can   generate colliding pairs of messages with sufficient control on the format of these messages so that one message looks innocent and the other suits the attacker s goals  have the innocent-looking message  signed  by a third party whose signature function begins by hashing the message as is   under both these conditions the third party will produce the signature which will be valid as far as signature verifiers are concerned for  both  messages this is like having the signer issue a signature on a message without showing it to the signer a demonstration has been  done in 2008  with the messages being x.509 certificates and the signer a certification authority a real-life application of the same concept was done in the  flame malware   a point to be made is that though raw signatures are vulnerable to collisions as explained above they can be protecting by having the signer include some randomness at the beginning of what it signs in the context of fake x.509 certificates the attacker must be able to predict all the bits of the certificates up to the public key this includes the  certificate serial number  which is chosen by the ca if the ca uses big random serial numbers then the attack does not apply even if md5 is used as hash function on the other hand ca who use sequential or time-based serial numbers are predictable   another worry with collisions is about  security proofs  some cryptographic protocols can be proven secure under some specific assumptions about the cryptographic primitives used in the protocol for instance some protocols using hash functions can be proven to be secure as long as the hash function is assumed to be collision-resistant or some other property an example is  hmac  hmac reuses an underlying hash function and was designed so as to accommodate hash functions of the  merkle–damgård persuasion  notably md5 and the whole sha family such a hash function is built around an internal compression function hmac has been proven secure under the assumption that the compression function behaves like a  pseudorandom function   however if the compression function of md5 is a prf then it is not feasible to compute collisions for md5 with cost less than 2 64  on average we know how to produce md5 collisions for much less than that this implies that the md5 compression function is  not  a prf so this voids the security proof on hmac/md5  this does not mean that we know how to break hmac/md5 with collisions ! it is just that whatever mathematical guarantee of security that we had has just evaporated like morning dew under the merciless midday sun in that respect md5 collisions are not a tool to be leveraged by the attacker but are still worth some attention  one common example i ve seen involves digitally signing documents using md5   say you have two documents one of the documents is very innocent and the other is something malicious since you can create both of these documents you can make it so that they have the same md5 signatures using postscript to make the pdfs for example   now you you get somebody to sign the innocent document which is typically done by signing a hash of the document but because the hash of both documents is the same the person just accidentally also signed the malicious document which might say something like i give permission for this person to take a million dollars out of my bank account now you can present the malicious document to someone else and they will believe the signature on it because of the collision in the document  take a look at this link -  http://th.informatik.uni-mannheim.de/people/lucks/hashcollisions/  - to see an explanation and the example  it works exactly as oleksi said in his answer you have two documents that will generate the same md5 hash the hash is signed so that both documents can be valid,2013-07-11 23:00:30,994,38801,i ve often read that md5 among other hashing algorithms is vulnerable to collisions attacks i understand the collision part there exist two or more inputs such that md5 will generate the same output from these distinct and different inputs  there are 20 examples of such inputs given here   fair enough but how does this specifically lead to a vulnerabilities? what is the vector of attack on a system with this kind of a flaw?  for example say i have a system that simply hashes users  passwords using md5 for encryption now i want to look at 2 scenarios for simplicity also assume no new users/passwords are added to the system   scenario 1  none of the passwords in the system are the kind of inputs that could generate collissions in other words all passwords processed by md5 generate unique hashes is there still a collision vulnerability there?   scenario 2  one of the passwords in the system is this beauty  collision 14 message 1  from  the list    so there exists another unsightly string that generates an md5 collision with this password however unless the attacker already know this information how is the vulnerability exposed,0.022132796780684104,22,sse,cryptography|hash|known-vulnerabilities|md5,how exactly is md5 vulnerable to collission attacks,6,flaws|attacks|malware|protection|vulnerability|known vulnerabilities,0.3653120994567871
22613,are there any inherit flaws with this algorithm?   it s not open to rainbow attacks due to the random salt sha512 is a fairly new algorithm but currently has no known collisions so it s probably pretty safe the way your storing passwords is done well the checking process is also important rate limiting bruteforce attacks and locking down the servers from attacks from other angles which could try to get access to the database if an attacker was able to gain access to the database he could probably extract out simple passwords fairly quickly but any complex password would probably beyond a simple bruteforce attack even if he had direct access to the hashes   is it ok to store the salt in the same database and table as the salt+password hash?   you pretty much have to store them together if you want to be able to validate the passwords i assume you want to the main reason for salting a password is to remove the possibility of a rainbow attack often this data is even stored in the same column as the hashed password using a symbol to separate them   will having a large 128 character password cause login performance issues on a magnitude of several seconds if i have several hundred thousand users in the table?   benchmark how long in seconds it takes to check a password   find the inverse of that number and that s probably close to how many password attempts you can handle per second per cpu core   can this data be reversed to produce the original password?   yes but it would take alot of effort since your using a random salt rainbow tables will no longer work and sha512 takes a fair amount of cpu to run and has no known collisions if the password was fairly simple it could be guessed if your worried about reversing the hashes putting a low bound on the complexity of the password might be a good idea checking it against a dictionary does it contain upper/lower/numbers/symbols  everything kendall said and .   . skip the hashing that you perform client side in javascript instead just buy a ssl certificate and post the credentials over https will protect you from novice eavesdroppers as well as seasoned attackers  and besides once you hash on the client side that effectively  becomes  your password if an eavesdropper gets hold off the hashed password he can pass it to your server and things would just work,2010-08-20 02:52:51.287 UTC,559,3527859,this question is about a specific programming problem i am having - i want to make sure that my code and software algorithm are sufficient enough to store user credentials in a database   here s the database entry for a particular password      password   69a78a7586a111b8a567b2d4f42f93f01fb59d337f7fa3c35949a66b246095778c1fa01ff4026abace476091e1e9a183bbdec1c31b12ce3f786921895c98cf6f   salt   69mt6nexl1rsjwnu011s53mpb/wmt4vl   questions    are there any inherit flaws with this algorithm?  is it ok to store the salt in the same database and table as the salt+password hash?  will having a large 128 character password cause login performance issues on a magnitude of several seconds if i have several hundred thousand users in the table?  can this data be reversed to produce the original password?    for fun   i ll paypal you $5 if you can provide me with the original password using the salt and salt + password hash,0.02146690518783542,12,so,authentication|passwords|php|security,does my php security algorithm effectively store user credentials,4,flaws|attacks|protection|eavesdropping,0.36522385478019714
45288,just replace   with,2016-06-15 15:30:56.877 UTC,47,37839749,the code below counts the number of files of certain types in a directory and subdirectories   how would i count the number of all files that are not of these types,0.02127659574468085,1,so,directory-traversal|powershell,how to count the number of files not of certain types,1,directory traversal,0.3646668493747711
51379,easy   either    add a length limit that is high but not dangerously high - e.g 128 chars is more than most real people will ever use  but  add an appeals process i.e a link that people can click to request specific exceptions from a human being   or    have a javascript element compute the hash client side and only submit the hash - this is better for security anyway as the password never leaves the client  if you are looking to protect yourself from a dos you would have to use post-max-size -  http://www.php.net/manual/en/ini.core.php#ini.post-max-size   this allows you to control how much post data will be accepted by the php server be mindful as this also controls how large files can be if your form is uploading files  an attacker can always send big data to your server whether this is a password field or another field it is not possible to prevent this client side with html or javascript what you can do is to stop working on server side when huge input was sent though this limit should be really high  keep in mind that for iterated hash functions like bcrypt only the first calculation has more data to work with other iterations work with the previous hash in my opinion there is no reason to limit a password to less than 1kb,2014-04-16 05:55:39.153 UTC,301,23101110,i have a password field in my form that has no maximum length limit  the password will be hashed into the database with the phpass framework  i ve read some places that big data entries through fields like that with no maximum limit can dos a site  so my question is     how should i handle big password data entries on the server-side with php,0.016611295681063124,5,so,denial-of-service|limit|passwords|php,password with no maximum limit how to handle it server-side,3,attacks|protection|denial of service,0.3641512393951416
14929,hashing problem is way exaggerated on this enthusiast programmers site in terms of hashing passwords any algorithm would suffice especially when this problem compared to other parts of usual php application using some strong hash on a usual php site is like using a safe lock on a paper door of a straw hut  if you really want protect user passwords from being stolen using some vulnerability on your site while such a vulnerability you ought to be way more concerned of and then bruteforced and then used against these users on other sites then the thing you really should care of is  password strength  and salt not hashing algorithm itself  no  hashing technique would protect silly pass like 1234 or joe     so   is better than   and quite enough  sha-256 is more than sufficient for password hashes just make sure to use a different  edit  salt for each user the salt could be the user s id a  created on  time-stamp a guid tied to that user etc.. you d append the salt to the plain text password before hashing  enforcing some sort of password  best practice  would help to avoid any brute-force attacks don t allow users to use weak passwords less than n characters contains characters from only 1 character set etc...  none of the above the problem with general purpose hashing functions is that they are fast which means an attacker could realistically bruteforce a huge amount of passwords very quickly with the prospect looking bleaker as computers get faster  use  bcrypt  instead which is built-in to php with support for all platforms starting at 5.3.0 if you are using an older version of php you can set up bcrypt with the  suhosin patch    the advantage of bcrypt is that it has a work factor which you can set to slow down the hashing process you are not overly concerned if it takes  you  a little time more to check if a given password is valid but it will take an attacker significantly longer to bruteforce passwords   if you want to go beyond that you might also want to check out  scrypt,2011-09-19 17:43:37.31 UTC,404,7475030,i am looking for a good one way encryption/hashing for safe storage of passwords using one of the algorithms provided by my host  here s a list of all the algoritms provided by the host  http://www.eresig.tk/hash.php,0.024752475247524754,10,so,encryption|hash|password-protection|php|security,best algorithm for hashing provided by my host,5,leak|attacks|protection|vulnerability|weak password,0.36277779936790466
117,hmac doesn t buy you much here though i guess it acts as a replaceable hash function replaceable by changing the key anyway it just seems like overkill here  however the first scheme you propose above fails catastrophically if $long_string becomes known to an attacker which it probably will be since the code is probably not regarded as secret and it s generally poor practice to rely on secrecy of the code anyhow   you should be using the second scheme as $nonce is needed to protect against precomputed attacks  the point of the salt is to prevent attack cost sharing if an attacker wants to attack two passwords then it should be twice as expensive than attacking one password  with your proposal your question 1 two users with the same password will end up using the same mac if an attacker has read access to your database he can try passwords by recomputing the mac and lookup the database for a match he can then attack  all  the passwords in parallel for the cost of attacking one if your   is an hardcoded constant in the application source code then all installed instances of the application share this constant and it becomes worthwhile for the attacker to precompute a big dictionary of password-to-mac pairs also known as a rainbow table  with a nonce your question 2 you avoid the cost sharing the nonce is usually known as a salt your   and the use of hmac does not buy you much here and you are not using hmac for what it was designed for by the way so you are on shaky foundations cryptographically speaking using a salt is a very good idea well  not  using a salt is a very bad idea at least but it does only half of the job you must also have a  slow  hashing procedure the point here is that the salt prevents cost sharing but does not prevent attacking a single password attacking a password means trying possible passwords until one matches that s the dictionary attack and given the imagination of the average human user dictionary attacks tend to work people just  love  using passwords which can be guessed the workaround is to use a hashing process which is inherently slow usually by iterating the hash function a few thousand times the idea is to make password verification more expensive having the user wait 1ms instead of 1µs is no hardship the user will not notice it but it will also make the dictionary attack 1000 times more expensive your    may  be used for that provided that it is really long not 2 kilobytes rather 20 megabytes  hmac  may  be used instead of a raw hash function to strengthen a password-verification system but in a different setup given a system which checks passwords with salts and iterated hash functions you can replace the hash function with hmac using a secret key  k  this prevents offline dictionary attacks as long as you can keep  k  secret keeping a value secret is not easy but it is still easier to keep a 128-bit  k  secret than a complete database,2011-04-19 12:09:12,831,3165,nota bene  i m aware that the  good answer  to secure password storage is either  scrypt  or  bcrypt  this question isn t for implementation in actual software it s for my own understanding  let s say joe programmer is tasked with  securely storing end user passwords  in a database for a web application or storing passwords on disk for logins to a piece of software he will most likely   obtain   from the end user  create   as a random value about 64 or 128 bits large  create   and store   together with   in the database    question one   why isn t the following substantially better than the above?   create   once and only once store this as a constant in the application code   could f.x be 2 kilobyte of random characters  obtain   from the end user  create   i.e create a mac using the end user password as key and store this   in the database    i would imagine the hmac has the following benefits?    collisions are less frequent?  it is computationally somewhat more expensive than plain hashing? but not anywhere near scrypt of course.  in order to succeed with a brute-force attack within a reasonable time and attacker would need to gain access to two things  1  the database where   is stored and  2  the application code where the original   is stored that s one better than a hash function where the attacker only needs access to the database?   but still  nobody seems to suggest using an hmac so i must be misunderstanding something?    question two   what would the implications of adding a salt value   be?   create   once and only once store this as a constant in the application code  obtain   from the end user  create   as a random value about 128 bits large  create   and store   and   in the database,0.02527075812274368,21,sse,authentication|cryptography|hash|passwords,hmac - why not hmac for password storage,3,attacks|hardcoded|protection,0.36269181966781616
3565,how can i mitigate brute-force and dictionary-based attacks?    use strong complex passwords or even pass-phrases with many bits of entropy so that they won t be as susceptible to brute force/dictionary-based attacks   i found that the mitigation with regard to brute-force takes more time compared to dictionary-based i wonder if there is some difference between the mitigation process between brute-force and dictionary-based attacks for ipss   i m going to be straightforward and say no i don t believe there will be a difference in mitigation between these two types of attacks as they re generally performed the same way or at least using extremely similar methods  i love this topic it s the biggest pita in security   password attacks   to start lets sum this up into a single category the goal of an attacker is to break passwords since this is what we commonly use the term brute forcing a password stems from trying every conceivable possibility and there are two ways to do it online and offline   online   online attacks are straight forward in web applications you are typically attacking a login form the most common prevention method is to set a threshold and lock the accounts being attacked forcing manual administration to look at it   offline   this attack happens when a system has been compromised and the attacker has access to a database or password file the common security measures here are password hashing and salting think ashley madison   dictionary attacks   dictionary attacks are a form of brute forcing they are performed by using rainbow tables which are dictionary based words and combinations which are common this reduces the scope of an attack because humans are flawed and lazy this attack has been very successful    entropy   at this point you may be saying i know this already whats your point entropy is my point password effectiveness is all about randomness and the time it takes an attacker to guess a hashed password adding entropy as mitigation is why we have password policies like   minimum 8 characters  must contain an uppercase  must contain a lowercase  must contain a number  must contain a special character   all these rules are adding entropy to your passwords and by extension adding mitigation techniques    the terminology is not fully universal so i will detail my definitions    a  brute-force attack  occurs in a situation where there is some secret value  x  that the attacker wishes to obtain and the attacker can test whether a given potential value for  x  is the right one or not so the attacker tries potential values until he hits the right one  such an attack can be  online  or  offline  in an offline attack the test for a potential value is a computation that occurs solely on the attacker s computer for instance  x  is the key for an encryption system and the attacker has at his disposition a message encrypted with key  x  and the attacker knows enough on what the decrypted message should look like to unambiguously detect whether his current guess for  x  is the right one or not  when the attacker cannot run an offline attack he must resort to an online attack each test involves talking with the defender s systems e.g trying to log on the server the attacker really prefers offline attacks because they are a lot more discreet everything happens on the attacker s machine without leaving any trace and also because he can speed them up by throwing more hardware at the problem    a  dictionary attack  is a special-case of a brute-force attack where the secret  x  is something human-compatible this is a value that a human user accepts to memorize and possibly chooses himself typically a  password  the brute-force attack is then about trying out potential values i.e values which are simple enough for the human user to handle in his brain the set of such values is called the  dictionary  because what humans choose and keep in their minds is usually  words   there again dictionary attacks can be offline or online  an important point to consider here is that when  x  is chosen by a human in his brain then not all possible values for  x  are equiprobable some words are a lot more likely to have been chosen by the user than others i.e a human password is much more often dragonfly than kelxdparf even though both are sequences of 9 lowercase letters the dictionary is thus a list of guesses for a brute-force attack ordered from most to least probable     there are basically three ways to defeat brute-force attacks    make it so that the space of potential values for  x  is large enough that the attack is doomed from start e.g specify that  x  is a sequence of 128 bits and generated with a  strong prng  since a strong prng is used no sequence of 128 bits is more probable than any other so there is no way for the attacker to optimize his search there is no order of guesses that is better than others and the space of possible 128-bit sequence is huge it has 2 128  = 340282366920938463463374607431768211456 elements so the attacker s probability of hitting the right one within the next century is abysmally low    make it so that no offline attack can be performed this is not necessarily possible e.g in the case of a password-encrypted file the attack context assumes that the attacker got a copy of the encrypted file and tries to crack it in which case an offline attack is always possible but if you can arrange for all attacks to be  online  then you can apply countermeasures e.g refusing to talk to a given requester after 10 wrong guesses an extreme case is a smart card that requires a pin code and locks itself after a few wrong guesses    if offline attacks cannot be avoided then make it so that each guess is unavoidably expensive this is the whole idea of  password hashing  the processing of a given input is done through a function with many iterations such that all usages are slowed the concept is that taking 10 milliseconds to verify a password instead of 10 microseconds is normally no hardship for a server because it does not have to do it many times per second anyway but making each try 1000x more expensive will also make the brute-force attack 1000x more expensive this is a trade-off making password processing more expensive tends to imply increased fragility against denial-of-service attacks     the main difference between brute-force and dictionary attacks is that when the secret is human-compatible the dictionary attack context then this also means that the space of possible values for the secret is awfully small human minds are simply not good at all at remembering complex values there are even worse at making random choices thus when a dictionary attack applies the first defence method i explained above extending the space size is not possible so you  have  to resort to one of the two others  avoiding offline dictionary attacks means using communication protocols and architectures that never expose anything that could be used as offline test for a potential value for the secret  x  this is outside of the scope of ips what an  ips  can do is to enforce rejection rules for the second method mitigation of online attacks a good ips should be able to detect when many requests come from the same source and efficiently block them upon entry  in that sense there is no real difference for the ips between a dictionary attack and a non-dictionary brute-force attack at best we can say that brute-force attacks that are  not  dictionary attacks should be defeated through using large spaces for secret values i.e in cryptographic terms for symmetric encryption use 128-bit keys not 40-bit keys making ips irrelevant,2015-08-30 23:51:28,1377,98153,i am working with bro ids and cif  cyber threat intelligence using the intel framework i am testing   how  can i mitigate   brute-force and dictionary-based attacks? i found that the mitigation with regard to brute-force takes more time compared to dictionary-based i wonder if there is some difference between the mitigation process between brute-force and dictionary-based attacks for ipss,0.04429920116194626,61,sse,ids,difference between brute-force and dictionary-based in the mitigation process of the ipss,3,flaws|attacks|denial of service,0.36231836676597595
11668,this depends on how it is implemented  ideally the encryption key would be derived from the password using secure schemes like pbkdfv2  if so a weak password would have no effect on the security of the encryption  however if the attacker knows that the key was derived using pbkdfv2 a weak password will be more vulnerable to brute-force / dictionary attacks used to derive the key  yes if an attacker were to know that the password used to derive the key is just four digits it greatly reduces the keyspace he d have to try in a brute-force attack   0000 to 9999 is just ten thousand combinations   if it was ten case-sensitive alphanumerics you d have 62^10 about 8 * 10^17 combinations too many to try them all    even if an attacker does not know about your user s requirement he d probably start brute-forcing with short passwords so he ll stumble upon a four-digit combination pretty soon too  the strength of the encryption is different to the strength of key generation one could implement a secure modern key generation mechanism such as pbkdf2 as slabks says but use a relatively weak encryption cipher like 128-bit rc4 compared to a stronger cipher like 128-bit aes   however as christian says a single 4 digit pin has a maximum of 10000 passwords which is not a lot of entropy to generate a key from if there is some form of hash or other tamper detection it would be relatively easy to brute force the encryption document with a dedicated modern system as you say yes the encryption strength is reduced by the password weakness  to add to christian s comment above you see pins now on some phone and tablet apps like dropbox because as christian says they are being authenticated by the server and lock you out when too many incorrect pins are entered it is acceptable for atm cards and these apps due to the limited input capabilities of atms phones and tablets but is not an acceptable excuse for a system with a full keyboard  for a 4-character all-numneric password there are 10000 passwords in total it doesn t matter how strong your encryption is brute force isn t even brute my pocket calculator could do that  note the reason this works with atms or sim-cards is because you don t get a chance to brute force the atm will eat your card after 3 failed attempts a sim card will lock up as well until you enter another 8-digit code which will permanently lock up the card if you don t get it right in 10 attempts,2012-09-14 13:02:52.523 UTC,490,12425158,i have a pdf document that is 128bit encrypted  the user insists on a four character password which is all numeric e.g 1558 or 6977 or 4793  128bit encryption is strong - is this strength negated by the password weakness,0.022448979591836733,11,so,encryption|pdf|security,how does strength of password relate to encryption,4,attacks|weakness|weak password|vulnerability,0.36220866441726685
66482,"not really if the files had a very large common header 64kb or more you might be in luck else you re essentially trying to guess 64,000 characters assuming the file used only 64 characters e.g it was a base64 encoded file for example purposes that would be 64^64000 possibilities that s a number with over 100,000 digits in  given that guessing a 12 character long password is considered hard and that only has 64^12 4,722,366,482,869,645,213,696 - only 22 digits! possibilities you can see the issue guessing a 64kb string is over 4500 times harder and that s ignoring that files aren t restricted to 64 characters in reality  in practice a lot of file types wouldn t require all the bytes to be guessed but most file headers are much smaller than 64kb so there will be quite a lot of unknown data even so",2016-08-10 11:12:32,187,133476,i heard about a ransomware virus going around that encrypts only the first 64 kb of a file  if i was infected with this would it be realistically possible to retrieve my files,0.0213903743315508,4,sse,ransomware,ransomware encrypted first 64 kb of file,2,virus|ransomware,0.3615175187587738
54052,,2014-04-19 01:57:40.967 UTC,135,23164926,i have some old rar files and i know the general format of passwords i used to use so i have generated a dictionary file combining those words in a variety of ways the dictionary file is 500 mb and i ve wrote a python program to use the dictionary   the issue is that the program has been running all night and is only 1% of the way in i ve now divided the dictionary file up and i am now running multiple instances of the program but each instance is running pretty slow  i would love any advice on how to improve the speed i m very new to python which will be obvious by my code but i m really enjoying python   sample  dictionary.txt,0.014814814814814815,2,so,dictionary-attack|python-2.7|rar,python rar dictionary attack - optimization,2,attacks|dictionary attack,0.3614615201950073
43743,use setimmediate to clear the context when you call back happen in your timeout,2018-03-29 13:36:14.98 UTC,55,49557539,i have been trying to write a code where i can see heapused should  not be increased when setting timeout and removing the next time but regardless the heapused is constantly increasing,0.01818181818181818,1,so,javascript|memory-leaks|node.js|settimeout,node memory timeout heapused increasing,1,memory leaks,0.361162006855011
14465,the line     is setting the keysize changing 1024 to 2048 or greater should put you in compliance,2017-07-27 16:46:02.77 UTC,72,45356622,veracode has reported a vulnerability in rsakeyhelper the key size specified for this algorithm is not large enough to protect it from brute force attacks as per the remediation comments key size should be 2048 bit  could you provide some guidance on fixing or mitigating this vulnerability?  jar spring-security-rsa-1.0.3.release.jarfile rsakeyhelper.java,0.06944444444444445,5,so,java|spring-security,inadequate encryption strength,4,attacks|protection|vulnerability|inadequate encryption strength,0.3597772419452667
17952,never encrypt your password always hash it!!!reference  https://www.owasp.org/index.php/cryptographic_storage_cheat_sheet  rule - store a one-way and salted value of passwords  for shiro to use salt and hash  check  how to stock and use a shiro&#39;s salt from database  for storing salt and password in db  do not encrypt password iterate over an hmac with a random salt for about a 100ms duration the salt needs to be saved with the hash use functions such as password_hash pbkdf2 bcrypt and similar functions the point is to make the attacker spend a lot of time finding passwords by brute force  see owasp open web application security project  password storage cheat sheet   see  how to securely hash passwords the theory  on security stackexchange  to see how hashed passwords are attacked see password list at  seclists  infosec  password-cracking-tools  arstechnica  how i became a password cracker   if you are using hashing algorithm e.g sha1 sha256 etc. you need to run the same algorithm on the input and compare the result  if you are using encryption algorithm e.g aes normally you must to decrypt first.pay attention that when encrypting something a good algorithm will use some salt meaning that you will receive different result when applying the same algorithm on the same string so decrypting is your only option  passwords are normally stored hashed since you don t need to know the real password while other sensitive data e.g credit card number is stored encrypted  and one last point both hashing and encryption algorithm provide a byte array not string so you are supposed to store and compare the byte arrays and not the strings if you need to store/compare string you can encode the result using some encoding algorithm e.g base64   update  owasp have a very good  cheat sheet  about the password storage   update 2  regarding encryption small example that uses apache shiro run it and you can see that no two lines are alike,2016-07-07 08:49:59.45 UTC,477,38241172,i have a scenario where i have to match the password/sensitive information and check for the validation now there are two ways here i can come up with  1 we can compare the password/sensitive field in the encrypted form by fetching the right password from db which is stored there in encrypted form  2 or we can decrypt the passwords first in to the plain text form and then compare them now in this scenario there is an extra call to decrypt utility which sort of an overhead  i have looked at the equals method of string class which runs in amortized constant time so if the encrypted string is insanely long string then it will have impact on the performance of the equals method.but here in my case encrypted strings are not so long  but my main concern is what is the standard generally followed.   update 1       update 2,0.016771488469601678,8,so,encryption|java,standard way of comparing the password/sensitive fields,5,owasp|attacks|sensitive data|sensitive information|open web application security project,0.3593686521053314
38629,,2015-02-04 08:22:20.933 UTC,265,28316644,i have some issues with memory i have a process that increases its resident memory too much i am trying to determinate whether there is a memory leak or not   just opening the process memory occupies 151 mb  virt  res  shr792m  151m 14m   after a couple of days the process increases the resident memory too much  virt  res   shr784m  480m  9204  i have generate two dumps one opening the process and another after a couple of days i have generated the dumps like this  jmap -dump:file=file.bin pid  the first dump occupies 23 mb and the second dump 244 mb   if i open both dumps with the memory analyzer tool i found that both dumps occupies almost the same ~7mb 6.6 mb for the first one and 7.4 mb for the second one   ¿can anyone explain me how this is possible and how can i determinate if it exists a memory leak or not? ¿how can i justify this memory increment?  thank you very much!!  as an additional info smaps shows an strange entry that seems to be the guilty  as an additional info smaps shows 487a0000-5fbcd000 rwxp 00000000 00:00 0  size             381108 kb  rss              381108 kb  pss               10420 kb  shared_clean          0 kb  shared_dirty          0 kb  private_clean         0 kb  private_dirty    381108 kb  referenced       374832 kb  anonymous        381108 kb  anonhugepages    370688 kb  swap                  0 kb  kernelpagesize        4 kb  mmupagesize           4 kb  locked                0 kb  but i can not understand what it means,0.011320754716981131,3,so,java|jmap|jvm|memory|memory-leaks,process with 500 mb resident memory generates a dump file of 250 mb  memory analyzer tool just shows 7.5 mb,1,memory leaks,0.35914188623428345
38072,typically processes that loop and don t take particular input or produce particular output are measured in terms of the rate at which they complete iterations in games and media applications it s common to talk about frames per second fps in other embedded and real-time applications cycle time can be an important performance metric higher-level metrics such as jobs per minute or maximum queue latency can be important as well,2012-12-07 17:37:11.68 UTC,112,13768192,my algorithm has an intentional looping  the program repeat itself each time by recursive calling  so how can i explain the cost cost analysis of it ,0.017857142857142856,2,so,algorithm|analysis|infinite-loop|performance|recursion,how to calculate the cost of algorithm has infinite loop,1,infinite loop,0.35863664746284485
7085,this  could  be ok depending on the implementation but it could also easily go wrong an attacker would be unable to recover the pepper assuming it has enough entropy and they would be unable to hash arbitrary values  however your peppered hash is likely vulnerable to a  length extension attack  if an attacker knows some data and its hash this would allow them to  append  data while still being able to calculate the new hash as if they knew the pepper to prevent this instead of using the secret as a pepper use it as the key for an  hmac  even if you aren t worried about length extension attacks this is desirable as it provides more security properties with no downsides and it sounds like a  mac  is what you re really after,2018-06-21 14:15:52,315,188199,my app is a educational game for elementary schools involving no money or anything of value so i am not worried about sophisticated attackers having any interest in this still i would like to follow best practices in case any of this code ever does wind up guarding something attractive  the security consists of hashing values with a constant pepper to produce a hash generally the values are things like the user id and other state info and hash is used to preventing tampering i.e so you can t change the user id to a different value and access someone else s data  therefore over time the attacker has access to a large set of state info the pre-hash and the hash but not the pepper the hash algorithm is also server-side and unknown though limited in possibilities  with this knowledge can an attacker determine the pepper and hash algorithm and be able to generate a suitable hash for any message breaking our security,0.031746031746031744,10,sse,exploit|pepper|validation,is a constant pepper at risk if an attacker knows the value and hash,3,exploit|attacks|vulnerability,0.3580400347709656
38138,if the memory usage stagnates at a maximum i wouldn t really call that a memory leak i would rather say the gc not being eager and being lazy or just don t want to physically free memory if it is frequently reallocated / needed if it would be really a memory leak used memory wouldn t stop at 300 mb    will result in a call to     and 32 mb will be used as the value of   parameter the value of   variable defined in   since you upload a larger file 80 mb a buffer of size 32 mb at least will be created - eventually this is implemented in     since     is used to read the content the reading process will start with a small or empty buffer and reallocate whenever a bigger is needed  the strategy of buffer reallocations and the buffer sizes are implementation dependent and also depends on the size of the chunks being read/decoded from the request but just to have a rough picture imagine it like this 0 bytes 4 kb 16 kb 64 kb 256 kb 1 mb 4 mb 16 mb 64 mb again this is just theoretical but illustrates that the sum  can even grow beyond 100 mb just to read the first 32 mb of the file in memory at which point it will be decided that it will be moved/stored in file see the implementation of     for details this reasonably explains the 96 mb allocation  do this a couple of times and without the gc releasing the allocated buffers immediately you can easily end up with 300 mb and if there is enough free memory there is no pressure on the gc to hurry with releasing memory the reason why you see it growing relatively big is because large buffers are used in the background would you do the same with uploading a 1mb file you would probably not experience this  if it is important to you you can also call   manually with a smaller   value e.g   doing so much smaller and fewer buffers will be allocated in the background,2015-06-05 02:32:51.087 UTC,451,30657454,the following server code   being run and then calling it with   where the   is about 80mb seems to have some form of memory leak in go 1.4.2 on darwin/amd64 and linux/amd64  when i hook up   i see that   uses 96mb of memory after calling the service a few times eventually called by   in my code above  if i keep calling   the memory usage of the process grow slows over time eventually seeming to stick around 300mb on my machine  thoughts? i assume this isn t expected/ i m doing something wrong,0.011086474501108648,5,so,go|memory-leaks|memory-management,multipart form uploads + memory leaks in golang,1,memory leaks,0.35802724957466125
67331,both of your suggested implementations have issues  the first one fails because you  still have to worry about timing when comparing hashes  and the   operator is almost certainly not constant time  the second one fails for two reasons both of which allow an attacker to easily determine the length of the secret   you have an early out  python s  zip  truncates to the length of the shortest iterator so an attacker can keep increasing the length of their input until the time stays the same   i d first copy the passwords into a buffer of a static size then fill the rest of the buffer with a static zero string using the same copy function otherwise the copy will leak size info even then you may leak some size info as the copy may cross a page boundary but in general that kind of caching information is only leaked if an attack originates from the same machine calculating the size may also leak information round and round we go aint that fun  the static string must have a specific value that cannot be part of the input otherwise passwords that end with something in the static buffer will compare to the same value as shorter passwords hiy|aa would be identical to hiya|a where | is concatenation  another option is to first copy the length into the array so you can have a randomized string which must of course be the same for the value to compare    let s name the statically sized x and y as x  and y  then i would generate a random key and compare hmack_r x  with hmack_r y  this won t leak any info about the input value because the hmac instead of the input is randomized  for each comparison  you could possibly also use hk_r | x  and hk_r | y  for this   if you decide to use a random delay then  perform it before the operation  this makes it harder to perform any power measurements during the operations in case you re not just worried about timing based attacks,2018-05-07 17:49:22,534,185395,given is the function   which is a part of a crackme a binary file which side-channel attack can be used to find the right password password is made up by ascii coded big and small letters from a-z and decimals and how can you design this function such that there isn t any side-channel?   so i have already executed the crackme binary and entered some passwords of different length i realized that if the password has length   it needs waaay more time to print   wrong password.. if the password has any other length than   it immediately prints   thus the password is most likely of length   it indeed is! and we probably have a  timing attack  here because we see from the above code that the two strings   and   are compared character by character and alert mismatch when they see a difference between the two characters that are currently being compared  is this correct obersvation so far?  we can fix this side-channel like this   is it alright like that i m really not sure?  ,0.02247191011235955,12,sse,cryptography|encryption|password-cracking|side-channel|timing-attack,change the function such that there are no more side-channels,3,leak|attacks|timing attack,0.35750097036361694
4126,as long as   is sufficient to cover any delays in responses introduced by a ddos style attack where   is the number of seconds  that is hitting a service listener that calls the above in order to encourage   to take longer than   could reveal to the attacker how long on average   takes to execute once the   barrier is broken  to mitigate this you could mask this extra time by detecting when your function takes longer   note though  with enough requests  it might be possible to get an average that will even out leaking the time taken to execute    with higher values of   the risk can be lowered further at the expense of performance you could also introduce detection of whether your web service is requested repeatedly from a certain user or ip address and then rate limit the connection,2016-02-13 20:43:51,195,113568,for example would this pseudo-code work?   or similar? that way the time that the message is sent doesn t depend on the timing of    would an attacker still be able to determine how long   took to run,0.035897435897435895,7,sse,timing-attack|web-service,can timing attacks over a webservice be eliminated by standardizing the time it takes to return a request,4,ddos|leak|attacks|timing attack,0.35719406604766846
14467,a hashtable will be used throughout the application its not just in once place  for example when you add post variables to a page they get processed internally in a hashtable so if you had a massive amount of hashtable collisions ie post variables with the same name on a page it d happen here its a very efficient memory storage system for accessing a array using a collection of words think of a dictionary   this is one of these things whilst it can be exploited the better thing to do is use best practises and monitor cpu usage limit maximum post size per page and limit requests from a single host   the hash table in question is    the server parses the form data and places the key-value pairs into the   collection if the form data contains keys that produce the same hash code it produces hash collisions which reduces the performance of the hash table  so it s not one table per server or per session but one table per post request,2012-01-09 01:50:56.17 UTC,240,8783138,"on the 28/12/2011 us-cert released a bulletin about the majority of web servers being vulnerable to dos attacks due to the way they handle hash table collisions article  here   could someone please explain where this hash table fits in to the asp.net lifecycle? is it one hash table per session or one big hash table per server instance?  thank you,fidel",0.020833333333333332,5,so,asp.net|hash|iis,asp.net hash table vulnerability,4,attacks|exploit|vulnerability|denial of service,0.35669317841529846
47652,,2019-02-28 03:51:17.833 UTC,94,54918119,i wanted to make a guessing game this game should guess a 4 digit number and then tells what number did i guess right and tells the place of the digit whether it is the first-second-third or fourth digit  lastly after guessing the game and checking the correct placements of the number it should also display the score  can someone kindly help me i m new in using python and i ve done some research but i couldn t find an exact answer,0.010638297872340425,1,so,if-statement|python|python-2.7|random|stack-overflow,creating a guessing game 4 digits,1,stack overflow,0.35622814297676086
34496,yes you need to call   to match every call to    per the  uikit framework reference      when you are done modifying the context you must call the    function to clean up the bitmap drawing  environment and remove the graphics context from the top of the  context stack you should not use the   function to  remove this type of context from the stack,2014-11-06 15:41:55.43 UTC,294,26783443,under ios 8.1 i m getting a memory leak when rendering a pdf page into a graphics context with cgcontextdrawpdfpage  it doesn t happen in the simulator but i get hundreds of leaks of 272 bytes of malloc d memory every time i do it on ipad air  if i comment out the cgcontextdrawpdfpage the leak goes away  does anyone else get similar behaviour?     here s a stack trace reversed  malloc  38.58 mb      36.7% 148743        std::__1::list >::liststd::__1::list > const&amp;  19.61 mb      18.6% 75610          std::__1::vector cg::path::subpath cg::allocator   19.61 mb      18.6% 75610           void std::__1::vector >::__push_back_slow_pathcg::path::subpath&amp;&amp;  19.61 mb      18.6% 75610            cg::path::sequence::move_to_pointcgpoint const&amp cgaffinetransform const*  19.61 mb      18.6% 75610             cgpathmovetopoint  19.59 mb      18.6% 75506              ttruetypequadoutlinecontext::addpointbool int int  19.59 mb      18.6% 75506               ttruetypefonthandler::renderglyphunsigned short ttruetypequadoutlinecontext&amp unsigned int const  19.59 mb      18.6% 75506                ttruetypefonthandler::getoutlinepathunsigned short tglyphoutlinebatch const&amp; const  19.59 mb      18.6% 75506                 fpfontcopyglyphpath  19.59 mb      18.6% 75506                  cgfontcreateglyphpath  19.59 mb      18.6% 75506                   cgfontcreateglyphbitmap  19.59 mb      18.6% 75506                    cgglyphbuilder::create_missing_bitmapscgglyphidentifier const* unsigned long cgglyphbitmap const**  19.59 mb      18.6% 75506                     render_glyphs  19.59 mb      18.6% 75506                      draw_glyph_bitmaps  19.59 mb      18.6% 75506                       ripc_drawglyphs  19.59 mb      18.6% 75506                        draw_glyphs  19.57 mb      18.6% 75434                         draw_glyphs  19.55 mb      18.6% 75359                          simple_draw  19.55 mb      18.6% 75359                           cgpdftextlayoutdrawglyphs  19.55 mb      18.6% 75348                            op_tj  19.55 mb      18.6% 75348                             pdf_scanner_handle_xname  19.55 mb      18.6% 75348                              cgpdfscannerscan  19.55 mb      18.6% 75348                               cgpdfdrawingcontextdrawpage  19.55 mb      18.6% 75348                                pdf_page_draw_in_context  19.55 mb      18.6% 75348                                 cgcontextdrawpdfpage,0.017006802721088437,5,so,ios|ios8|memory-leaks|objective-c,memory leak in cgcontextdrawpdfpage in ios 8.1,1,memory leaks,0.355215847492218
18401,the password is limited to 72 characters internally in the crypt algorithm   to see why let s look at   s source  ext/standard/crypt.c    the   field is a simple   field so there s no length information all that s passed is a normal pointer  so if we follow that through we ll eventually land at      the important part is the loop     is defined to be 16 so the outer loop will loop 18 times    the inner loop will loop 4 times 4 * 18 == 72  and there you have it only 72 characters of the key will be read no more  note  now there s an interesting side-effect to that algorithm because it uses c-strings strings terminated by a   null byte it s impossible for it to use anything past   so a password that contains a null-byte will lose any entropy past it example  http://3v4l.org/y6onv,2015-03-09 20:42:48.41 UTC,316,28951359,the general attack scenario   in 2013 django had a general vulnerability as an attacker could create extremely intense cpu calculations via very large passwords [ see the security notice here ] i m unsure if this is still possible when using php s  password_verify  and other password-hashing methods without any further checks   the php documentation says      using the password_bcrypt for the algo parameter will result in the password parameter being truncated to a maximum length of 72 characters     but php s code maybe says something different   the  c code behind php 5.5.0 s  password_verify  function  however does not limit the passed argument directly maybe on a deeper level inside the bcrypt algorithm ? also the  php implementation  does not limit the argument   the question   is  password_verify  and other functions of the same function set vulnerable against dos via maxed out post parameters ? please also consider site-wide config situations of post upload sizes much larger than 4mb,0.022151898734177215,7,so,c|hash|php|security,is php s password_verify safe against extremely long passwords dos attack,3,attacks|vulnerability|denial of service,0.35520482063293457
25223,distributed denial of service being by nature decentralized would not all come from a single client as such you would not be able to limit the entire botnet s access to the page.. while it would help reduce the effect of each singular drone it would not necessarily prevent them from hitting your server repeatedly. however in the case of a singular attacker it would massively increase the time for a brute attack but could be less pain to the user by implementing a three-strikes before delay rule or similar,2011-09-08 00:53:30.107 UTC,140,7342056,my question is if you limit the refreshes let s say 30 to a period of 10 sec will this prevent ddos flood brute force attacks in the login or other page,0.05,7,so,php|security,will limitting refreshes of a login-page in a period of time protect the site from attacks,4,ddos|attacks|protection|denial of service,0.3547954559326172
6993,given that you say the guid is sequentially assigned and sent in network traffic no it is not suitable for use as a salt while it will still prevent the typical scenarios of attacker wants to crack all your passwords so they build a rainbow table that can be used to look up any hashed password and two users used the same password and you can tell because their hashes match it s still weaker than it should be  in particular it does  not  prevent the scenario of attacker wants to crack a  specific  password so they build a rainbow table  using that user s known salt  yes the attacker still needs to get access to the user s password hash which implies they d have access to the salt anyhow but normally that then begins a race between the site admin noticing the breach and the attacker managing to brute-force the slow hash you did use an expensive hash function right? if the salt is known in advance the attacker can perform the slow process of brute forcing the hashes by building a rainbow table and then instantly determine the password once the user s hash becomes known,2018-05-19 10:46:46,349,186109,i use the   for creating an aes key for protecting the user s data for the salt value i would like to use the user s id which is a guid  my understanding is that the only drawback of using the user’s id would be that it will get easier to crack keys which were previously built for this specific user in case the attacker is in the posession of the database but since the attacker has already cracked the current key there is no need for cracking elder keys therefore i don t see this as an issue    is it a problem to use the user’s id as a salt for the key derivation?   guid type  as an additional information the guid is a windows sequential guid therefore it is not really random but quite unique,0.025787965616045846,9,sse,key-generation|salt|uuid,using the user s guid-id as a salt for rfc2898derivebytes,3,attacks|weakness|protection,0.3534097671508789
19686,people dislike hard-coded salts as they are accessible to all developers of the project and possibly the public in the case of open source projects or reverse engineering attackers can then compute rainbow tables for your particular salt and start attacking your system  a good choice of salt value is something that   is available each time you check the password  doesn t change between password checks  differs for each or most password calculations   a username would then be a decent choice provided it cannot change or generate a completely random value when you first create the user and store that as the salt along with the user data in your database  a salt is designed to protect against multi-target attacks by making each target behave differently rainbow tables are just one particular incarnation of multi-target attacks where the computational effort is expended before you obtain the targets  there are situations where multi-target attacks are applicable but rainbow tables are not  one example of this assume you re using an authenticated encryption scheme with semantic security such as aes-gcm with unique nonces now you ve obtained a million different messages encrypted using different password  if you use no salt to check if a password applies to any one of these the attacker needs  one kdf  operation and  one million decryption  operations if you use a salt the attacker needs  one million kdf  operations and  one million decryption  operations since the kdf is slow compared to the decryption an attack against the first scheme is much faster than an attack on the second scheme  a salt should be  unique for each password  that means create a random password for every password you want to hash the salt is  not  a secret and can be stored plain text with your calculated hash-value  the idea of the salt is that an attacker cannot use a prebuilt rainbowtable to get the passwords he would have to build such a rainbowtable for every password separately and this doesn t make sense it s easier to brute-force until you found a match  there is an example in  msdn  where the salt is gotten from the  random source of the operating system  this is the best you can do to get a safe salt do not derrive it from your password  others already explained the purpose of the salt and why it can be public information  one additional part of the answer to your question  do not  derive the salt from the password itself that would be very similar to the programming blunder that ended up exposing millions of passwords after the ashley madison hack  the problem is that when you use the password to generate the salt you are essentially making the password available in a second and much-easier-to-crack form the attacker can completely ignore the output of the pbkdf2 and simply reverse the salt itself that is only protected with sha1 which is already broken  at ashley madison the error was similar the passwords were stored in the main database using bcrypt and thought to be secure but then somebody discovered that the passwords for many accounts were actually stored twice and the second copy was only protected with md5  i don t really know what is   but i can tell you the following salt doesn t has to be secured now you said you have seen people complaining about hard-coded constant values for salt and whoever said that is right salt should be a random value never a constant one otherwise its purpose is defeated  do you understand what salt is used for? you clearly don t using the hash as salt is a bad idea because password  x  will always be salted with the same value  y  again defeating its purpose,2012-09-21 11:25:54.26 UTC,923,12529574,i m using   to securely generate encryption key and initialization vector from user-supplied string password to use with symmetric encryption e.g aesmanaged  i m taking the sha1 hash of password as a salt parameter to   is that ok? if not then where should i get the salt from? i will need the same salt when decrypting right? so i have to store it somewhere unencrypted - unsecured if i have to store it securely then it just becomes another password isn t it?   i ve seen using the constant hard-coded salt and i ve seen people complaining about it i thought deriving salt from password would be the better idea but i m not sure this is an optimal solution  shortly i have a file that needs to be encrypted and password string input by user how do i properly use   to derive secure encryption key and iv?  thanks   edit   thanks for your answers i now understand that the main maybe only? purpose of salt is to make generation of rainbow tables impossible - you can t pre-generate the hash of p@$$w0rd because it will have a different hash for each possible salt value i understand this perfectly but.. is this really relevant to symmetric encryption? i m not storing the hash anywhere right? so even if the attacker has the rainbow table for all possible password combinations he can t do much right?  so my question now is is there any advantage of using the random salt in each encryption operation compared to using password-derived or even hard-coded salt  when used with symmetric encryption algorithms  like aesmanaged of .net,0.020585048754062838,19,so,.net|c#|cryptography|encryption|security,is it ok to use sha1 hash of password as a salt when deriving encryption key and iv from password string,3,attacks|hard coded|protection,0.35305410623550415
3338,you can use ip-bas rate-limit even if the attack can control multipl ip address for you keep track of ten of thousand of ip address is easi just creat a databas tabl with ip address account access and number of authent error if they are abov a threshold slow down the respons simpli sleep for for exampl 1 second isn t suffici becaus the attack can simpli put in more request befor the first one finish in thi case you need to look on your tabl see if there s more than say 5 request and add the wait time at the end of the request so no matter if the attack sent 10 or 1000 request he will have to wait the wait time should be exponenti not linear to further increas the attack cost for the attack control more than hundr of ip address is not trivial they will end up be slow down by the rate limit increas the cost of the attack substanti but the victim will not be affect,2020-04-01 09:02:21,370,229065,i was recent read thi question where the accept answer claim that it is easi for attack to bypass rate limit that is base on ip which make any sort of ip rate limit to prevent a brute forc attack much less use but if it is base on the account that is a victim then it becom veri easi for an attack to block access to a victim s account what is the best way to defend against both account-level dos attack and onlin brute forc attack and anyth els that is in thi same category? simpli sleep for for exampl 1 second isn t suffici becaus the attack can simpli put in more request befor the first one finish 1 second latenc but unbound throughput and throughput is what matter for brute forc if subsequ request are block until the first one finish then they must be block per-ip or per-us which produc the same problem 2fa isn t alway a good solut either becaus for wors mani peopl fail to use it,0.04864864864864865,18,sse,account-security|brute-force|denial-of-service|passwords,how can one mitig both account-level dos attack and onlin brute forc attack at the same time,3,bypass|attacks|denial of service,0.3530120551586151
16659,the salt you are describing is commonly called a pepper the question as to whether or not to use a pepper has been answered very completely here   best practices salting &amp peppering passwords?   in summary you shouldn t use peppers because   you cannot change the value of the pepper because the hash function is one way  adding a pepper effectively changes the hashing algorithm and this should only be done by experts   there is however another option which can improve security if the assumption that it is harder to obtain the static secret than it is to obtain the password hashes holds true rather than adding pepper simply encrypt the combination of hash and salt using a standard symmetric algorithm this addresses both of the issues mentioned above while still improving security if the encryption key is not compromised,2016-02-10 12:44:16.997 UTC,460,35315668,so in asp.net   will create a hashed salted string that is unique every time this is great it means that if someone managed to get hold of a hashed password say in a sql injection attack they wouldn t be able to use lookup tables to find the original password however as far as i can see they would still be able to easily write code to attempt to brute force it locally without involving the victim s server e.g   .. so wouldn t it be better to add your own extra salt when calling hashpassword where the extra salt is stored outside of the database e.g   .. or is this pointless for some reason? seems to me worth doing to make the above brute force attack much more difficult but i ve never seen this idea mentioned anywhere in usage examples for   so is there something i m missing?  edit this isn t a duplicate of  the necessity of hiding the salt for a hash   that question is asking whether an unhidden salt has any use to it at all i m not disputing that a salt stored in the same place as the hashed password is useful or that passing an unsalted password to crypto.hashpassword is useful.what i m really getting at that adding your own additional salt stored elsewhere even if it s the same for every user provides an extra layer of protection on top of that provided by crypto.hashpassword   in the case of a sql injection attack that exposed the database the additional salt in say a web.config file would still be protected or would at least require an entirely different attack to access it   makes the hashed password much less useful for an attacker than   for certain types of attack and without much additional cost in terms of computing power or programming time,0.021739130434782608,10,so,asp.net|security,why shouldn t passwords passed to to crypto.hashpassword be salted,3,attacks|protection|sql injection,0.35232028365135193
7742,tricky one! i can t seem to trigger the exponential complexity this   pattern should introduce i can make it be a little slow with a huge string a few megabytes but i guess that is to be expected with any regular expression in python   in the first test we supply a long string where the   pattern is repeated this matches just the first part namely   and not the second namely   the second test does the opposite it repeats the  .b  pattern which matches the second part a lot of times where the backtracking should be and not the first  as output you can see the difference in time it takes   both strings are equally long so the pattern does take some more time  usually one would use the second pattern and then make the end not match so remove the   part or something the regex engine would start to backtrack and take exponentially longer with each iteration i m not sure why that does not happen here  so far as i can see you might be safe by just limiting the input length to a few kilobytes but i am probably just too stupid to trigger it using a pattern that is known to be vulnerable is just asking for someone to come along and figure it out ;,2019-03-01 11:02:42,314,204542,findbugs flagged the following email address validation regex as vulnerable to dos   here s an easier to read version that substitutes   for     based on my reading of the  owasp redos page  this isn t actually vulnerable i can t see a way to ambiguously apply the pattern because the repetition inside the groups doesn t apply to the period character which ought to result in a canonical application of the pattern  have i misunderstood the threat here? can anyone spot a way to abuse this pattern,0.025477707006369428,8,sse,denial-of-service|regex,is this regular expression vulnerable to dos,3,owasp|vulnerability|denial of service,0.3517587184906006
10918,use php built in function     from the documentation     verifies that the given hash matches the given password      note that password_hash returns the algorithm cost and salt as part of the returned hash therefore all information that s needed to verify the hash is included in it this allows the verify function to verify the hash without needing separate storage for the salt or algorithm information      this function is safe against timing attacks   if you use symfony password encoder it has method   which internally uses this function so you don t need to worry about timing attack  source code   you need to take into consideration that if the user doesn t exist but you still want to calculate a hash to fool an attacker trying to enumerate user accounts there will be no password to be hashed so you have two options there can be more but right now i only thought about these two  1._ have a hardcoded password that will be hashed when a user doesn t exist  2._ use a pseudorandom number generator prng to generate a random password which will be the one that you ll hash we don t really care abount unpredictability here so there s no need to use cryptographically secure one  the problem with the first attempt is that since you re always hashing the same password the response times when the users don t exist will be pretty consistent so even if the attacker doesn t see an evident faster response when the user doesn t exist he can notice that there s an average response time for these scenarios the second attempt has a similar problem but here you re hashing a random password and generating that random password so you re adding overhead and the response is highly likely to always take a longer amount of time than when just authenticating an existing user so an attacker can also notice the pattern  what you could do is to add a random noise to every request both for existing and nonexistent users where you ll add a random delay of a few milliseconds by doing this you can use a fixed password for nonexistent users and even when you ll be calculating the same hash for nonexistent users that random noise will make it harder for an attacker to know if it s a real or fake login attempt,2016-12-15 13:05:48.99 UTC,554,41164921,we have a pretty standard symfony2 application there s a provider a password encoder an so on the passwords are hashed with pretty large hashing cost  in other words - when someone tries to log in not necessarily with correct password - it is obvious when there s a real user to check the hash and when it isn t in the second case it s much quicker - no user - no need to hash etc  so it s easy to know  if a particular email is registered in our app   is there any built-in mechanism in symfony to prevent this sort of leak? or any established best practice?   upd in response to the first answers how to handle this in general is more or less clear the question is how we can do it without much pain within symfony s security framework,0.018050541516245487,10,so,hash|php|security|symfony,how to protect symfony login against timing attacks,4,leak|attacks|protection|hardcoded password,0.35145407915115356
18051,this is a result of the chart trying to find a common denominator even if you do hard code a   value the chart will behave this way  setting the   property to   will fix this   fiddle    http://jsfiddle.net/jlbriggs/9vmtz3nf/2/    reference    http://api.highcharts.com/highcharts/chart.alignticks,2016-11-09 21:40:48.827 UTC,176,40516492,i have been stuck on this for a while now and despite several attempts and a lot of research i have yet to find the solution.i am building charts with two y axis everything works just fine except that the highcharts computed max y value is always much higher than it should be resulting in a zoomed out graph.i ve replicated the problem here   http://jsfiddle.net/9vmtz3nf/   i cannot hardcode any max value as it will always depend on the data coming in from the db and there is a very wide margin of values possible  so basically how can i tweak the options and parameters to make sure the graph max value is set to the closest tick 150/600 in the jsfiddle example ?  thank you in advance,0.017045454545454544,3,so,highcharts,highcharts calculated max value not working with 2+ y axis,3,weakness|hardcoded|hard coded,0.35140204429626465
859,collisions are not an issue for key fingerprinting key fingerprinting relies on resistance to  second preimages  to say things simply    in a  collision attack  the attacker tries to build two distinct messages  m  and  m   which hash to the same value    in a  second preimage attack  the attacker tries to build a message  m   distinct from a fixed imposed message  m  such that both messages hash to the same value    the difference is quite crucial while md5 is thoroughly broken with respect to collisions it is not for second preimages as such there is no urgency to replace it  new protocols  should rely on better hash functions like sha-256 but there is no use in breaking existing protocol by evicting a still serviceable md5,2012-11-10 10:20:42,219,23850,even though md5 has been broken for years and its vulnerabilities have been used to create real-world rogue x.509 cas and other horrible things openssh and putty still use md5 as the default public key fingerprint algorithm  is there a way to switch to something more secure for this purpose? even sha-1 would be somewhat better although i would not prefer it  notice that for the validation of public keys via  sshfp dns  records sha-1 is already in use and in a  proposed extension  also sha-256,0.0273972602739726,6,sse,hash|md5|sha|ssh,replacing weak ssh fingerprint algorithms,3,attacks|weakness|vulnerability,0.35031262040138245
1396,i ll answer your 2 questions below    as for the salt a salt is supposed to be public sometimes an extra secret value is used e.g by pre-pending to the password this is often referred to as pepper    try tls with client authentication use the credentials within the client certificate after successful authentication &amp session establishment to provide access control    yes it is possible to derive a des key in a good way use pbkdf2 with a high iteration count and large random salt the des key is however not secure anymore once it is used for encryption because of brute force attacks     generic answer  if password-based authentication can be performed offline then the app necessarily contains all that is needed to decide whether a given password is the correct one or not this unavoidably implies that by dumping the complete app code and contents the attacker can emulate the offline server on his own machine and try passwords at his leisure limited only by the computing power he can muster how many pc he will buy or rent and the inherent computational cost of verifying every single password this is called an  offline dictionary attack  situation   at best  in such a case you can raise the cost by using slow hashing many iterations of an underlying hash function or something similar and salts to prevent parallel attacks cost sharing precomputed tables... see  this answer  for a complete treatment  offline dictionary attacks are a worry being in such a situation is not comfortable if users accept a 1-second wait for authentication on their smartphone then the smartphone cpu must necessarily be able to verify a password within one second an attacker with a few good pc will then be able to try at least a few dozen passwords per second make that a few hundreds if the hashing algorithm maps well to what off-the-shelf gpu can do a few thousands is the attacker is industrious and went straight to some  fpga  implementation also the attacker is often patient and can afford to wait one or two  weeks  before obtaining a user password this adds up to very roughly  one billion password tries  for the attacker  training average users to choose passwords which will resist a brute force attack of one billion potential passwords is hard see  this question  for some discussion on the subject on the other hand if  you  get to choose the password not the user then select a sequence of 15 random letters that s 70 bits of entropy and this will resist long enough to deter attackers a password space of size 2 70  is  huge    note that  salts  are not meant to be secret and are effective even when known by the attacker a secret salt is not a salt but a  key  you cannot have that in your situation one must assume that the attacker dumped and reverse-engineered the complete app code to a large extent your context is similar to that of password-based encryption protecting the confidentiality of a piece of data with regards to a secret password without specific hardware,2013-07-30 09:18:29,705,39857,i m currently thinking about a web-app that can go offline after being online and still be able to provide authentication to the user securely.for example in a multiuser environment it s quite essential to prevent other users  access.  right now i was considering a password-hash authentication with some salt and just saw this so question  https://stackoverflow.com/questions/7879641/user-authentication-in-offline-web-apps   but since this is a web environment i have to assume that others have access to the source code of the app and thus the salt itself is leaked.also the attacker might use the hash itself to bruteforce the password itself.  so...to summarize my 2 questions are   is salting effective even when it s leaked?i m guessing no to the question since it takes ox^m+n to bruteforce the unknown... and it gets worse when the attacker uses the password hash to bruteforce the passwwd.  are there any effective secure ways to authenticate the user?  any good way to generate a des key for encrypting the user s data and provide a way for the server to distinguish between tampered data and real one,0.02553191489361702,18,sse,authentication,is secure web-app off-line auth possible,3,leak|attacks|protection,0.3500974178314209
9891,one can discuss if such a backdoor is desirable or not but if you cannot decide it for yourself there are two ways to protect such a hardcoded password storing only the hash is surely much better than the plaintext password because everybody with read-access to the code could otherwise use it on a running system  the first possibilty is to use a slow hash algorithm with a cost factor such as   which you proposed in your example if an attacker has read-access to the code he would have to brute-force this password to use it on a live system a slow hash algorithm will thwart brute-forcing while md5 and other functions are ways too fast  8 giga md5 per second  the drawback is a slowdown in your application when the password is verified  the second possiblity is a very strong password choose a long enough random combination e.g 70 characters generated with a password-manager and use it as the master key even fast algorithms like sha256 are out of scope for brute-forcing then and your key is immune against dictionary attacks and does not even need salting  if it s purely a read only incident and all they get is the hashed version it shouldn t be an issue that being said i prefer to use the   function rather than   but that s just my personal preference  the only issue is if the attacker who reads the php file and obtains the hashed password has a way to reverse it i.e a rainbow table in this instance your system becomes insecure this is where my preference for   over   comes from with   to generate the hashed form i can say   then in my uploaded code i can take the output of the above code   and hard code that string into my code this adds two different salts two different hashing functions and it reduces the already low chance of a rainbow table attack because the attacker would need a collision that occurs for both md5 + the first salt and sha256 + the second salt this is i dare say the word impossible sure pigeon hole effect says it s possible and it is but this is so near impossible that i wouldn t fret over it  that all said using   and storing that hash is fine if you really want to i just personally dislike it   edit adding on implementation details   i believe based on the comments that there was some confusion as to how this would be implemented let s say my master password was   super secret and super secure i know well we d need to generate the hard coded hashed form the hard coded hashed form would be found by saying    important  you would not upload this to the server this would just be used for generating the hash and it should then be permanently deleted  the output of this would be    so in your code that requires a master password to override you d enter   again to clarify based on information from the comments this is more secure than just using sha256 because a brute-forced master password must be found that produces   when   is appended to the end of it and it also must produce   when   is appended onto the end of it this means the collision must satisfy two different algorithms which is much more difficult to do than satisfying just the sha256 algorithm on it s own  however as noted in the comments this does take a few more milliseconds to process so if decreasing execution time is a big priority to you then it might be recommended to only use sha256 on its own rather than using md5 and sha256 combined  if the user has access to the source code they can just change it so it doesn t really matter,2014-06-06 20:17:14.053 UTC,788,24089989,i m developing a php web application that uses an ldap server for authentication however in case the ldap server goes down or something else goes wrong and i still need to access the system i want to have a master password written directly in the code  the previous iteration of this system which i did not write simply stored the passwords as plaintext ! so   which of course is extremely unsecure  so i want to fix that i was thinking of hashing the password so doing   so i would hard-code the  hash  for the master password not the actual password itself is this secure? i know a stolen hash is bad but it s better than a stolen plaintext password  of course all user-input would be appropriately sanitized  alternatively any other thoughts for having a master-password access system,0.01903553299492386,15,so,hash|passwords|php|security,php hashed master passwords in code,7,attacks|backdoor|protection|hard coded|sanitization|plaintext password|hardcoded password,0.3492600917816162
66961,zlib deflate is used by gzip to create ab 10k bomb out of 10m input you might use   and i think the post is right in claiming that you have a maximum ratio of about 1000 more information are at  http://www.aerasec.de/security/advisories/decompression-bomb-vulnerability.html  old but  http://blog.cyberis.co.uk/2013_08_01_archive.html  shows that most modern browsers still no have protection against it  the source of the 1032:1 figure is given on the  zlib site  where it is told that     the limit comes from the fact that one length/distance pair can represent at most 258 output bytes a length requires at least one bit and a distance requires at least one bit so two bits in can give 258 bytes out or eight bits in give 1032 bytes out a dynamic block has no length restriction so you could get arbitrarily close to the limit of 1032:1   this is a quote from mark adler one of the two designers of zlib having myself implemented a library for deflate i can confirm that given how  the algorithm  works this asymptotic limit is indeed true you cannot go beyond but you can get as close as you wish deflate works by encoding copies each element is either a new byte or a copy of a past sequence copies can overlap i.e you can copy a sequence of length 30 at distance 4 yielding 15 times the past sequence of 4 bytes huffman codes are applied on the sequence of elements to get a minimal length copy you need it to overlap a lot and each copy is worth at most 258 bytes so the data which is to be compressed will have to be a long string of identical bytes  as @steffen says compressing with   a long sequence of zeros will yield a more than 1000:1 compression ratio it does not have to be zeros any sequence consisting of the same byte value over and over will do the trick but a linux machine has a   not a    compression bombs were already active quite a long time ago i saw it employed to kill netscape back in 1996 at that time netscape handled background pictures but mosaic did not a very small gif file could encode a huge background which netscape allocated as a single big pixmap in the x11 server,2014-02-06 22:55:29,565,51071,can you give me an example of a short data string that when decompressed using zlib s deflate method expands to something much much longer?  more precisely what is the nastiest decompression bomb that one can build for zlib s deflate?  the figure of merit here is the compression ratio  if the compressed file is  n  bytes long and after decompression it yields something  m  bytes long then the compression ratio is  m/n   i am looking for something that maximizes the compression ratio where hopefully the compressed data is very short  can anyone give me an example of such a decompression bomb?   related  this post  claims that deflate can asymptotically approach a compression ratio of 1032 is that the best one can do or can one achieve a higher compression ratio if we pick a carefully chosen sequence of compressed bytes?  libpng  defends  against decompression bombs by imposing resource limits but they don t give a concrete example of a specific decompression bomb  see also  zip bombs  the corresponding attack but for the zip file format,0.024778761061946902,14,sse,appsec|attacks|compression|denial-of-service,zlib deflate decompression bomb,5,bomb|attacks|protection|vulnerability|denial of service,0.3490832448005676
39790,it s a bug of twemcache fixed in 2.5.3 release. https://github.com/twitter/twemcache/issues/17,2012-11-10 14:30:22.457 UTC,77,13322911,i started twemcache with twemcache -d -m 20 it should consume no more than 20 mb.but according to test it keeps consuming more memory with set then get    i found this because i had a service with -m 4000 and exceeded the cap and finally killed by os anyone know how i can avoid? thanks in advance,0.012987012987012988,1,so,caching|memcached|memory|memory-leaks,twemcache exceeds max memory setting,1,memory leaks,0.3490259647369385
12599,this is mostly a theoretical question so how does cracking a hashed value work? there are so called rainbow tables that are just list with common words and theire hash value for salted hashes an attacker needs such tables also with salted hashes so in theory with unique salts for every user an attacker needs one table for every salt => user if you have a static salt he just needs one table for your db its quite expensive to create such tables so in most cases its not worth to create it just for a single page  conclusion its of course safer to use unique salts for every user but on a veeery high level a static salts is usually safe enough  think of the web sites you ve programmed - i bet the most powerful users in these systems have very common usernames like  admin   root  etc as an attacker i can generate a precomputed hash list containing the most common usernames with the  weakest and most common passwords  - that is if a web programmer is naive enough to salt  their  passwords with usernames my job as an attacker has become much much easier - the  collisions  are very predictable   using an email address as a salt is better but the same principle applies assuming i ve cracked one database that uses an email-based salt i ll have a much easier time cracking every other database that does the same - at least for email/password combinations that exist across databases given the amount of login reuse that s a very likely it s not as simple as with the username salts but the collisions are out there waiting to be discovered   as a programmer what i really want is a password hash that won t collide - ever a universally unique hash for each user that can t be found in any other database in the world that s a tall order but it s doable with a sufficiently long randomly generated salt    there s a herd immunity effect in play - using a bad salt makes it easier for attackers to attack similar systems after they ve compromised your database a strong salt will always protect your users and help other userbases from being compromised   can t really help you in terms of security but if you look at vbulletin for example each user gets their own generated salt which they use the encrypt the password like this   so the salt will be different for each user and any site where vbulletin is running at least a pretty good chance that it will be different so the stored password in turn will be different for each site  it s not what you were asking for but something to meditate on :  the point of a salt is  not  to be unknown it is to prevent attackers from amortizing the cost of a brute force or dictionary attack across all users of a site or even all users of many sites  thus the problem of using a non-random salt like the email address is that it would show an attacker which users are using the same password on several sites and which would therefore yield access to several accounts at once if cracked via brute force or dictionary attack for the email address and everything that is unique per user this is a rather hypothetical problem since it assumes the attacker has the account data of several sites with considerable overlap in users  another problem with using the email address is that users will want to change it - which would be impossible if you use it as salt unless you store it in a separate salt column as well or require people to always change their password together with their email   if the salt is already known then you have bigger problems on your hands   http://en.wikipedia.org/wiki/rainbow_attack   assume the hacker has both password and salt and access to your hashing formula  this will not prevent a dictionary attack contrary to popular beleif  it will stop a simple dictionary attack but iterating the dictionary with the salts per user account is perfectly possible  see  why do salts make dictionary attacks &#39;impossible&#39;?  for more related information  this is why when you generate the hash of the password instead of hashing once with salt ie   you would do   the multi hash function should take a significant fraction of a second to calculate  this means when the hacker gains access to the database the dictionary attack then becomes exponentially more time consuming only allowing him to crack a very small % of user accounts  the first attack i can think of is   a user has the same salt and password at two sites  both sites have a flaw to allow reading the salted passwords  one site makes reading a password or brute-force guessing a password easy   an attacker could quickly look at identical salted passwords on both sites and find users with identical passwords at both sites then read the password or guess the password on the weaker site and use it on the more secure site  granted different salts wouldn t make the problem  significantly  better because all million passwords can be tried eventually but  knowing  which users have identical passwords would be much  quieter  than just blindly trying all the users  passwords on the stronger site,2011-02-21 09:13:58.067 UTC,1054,5064105,i ve been told that email is a bad salt because it s not unique and connected to the user and if a user uses the same password on 2 sites there will be equal hash   so what s wrong with it? what is attack scenario? suppose we have both hash and salt so other site has the same hash in their database how can we do any harm to this user on the other site? can we at all?  i don t see any possibility but i am not an expert in security so i d like to hear from ones who are  with practical and concrete answers of course   i am not going to break anything i am asking this question in the context of this one is email or registration timestamp a good salt?  certain and practical answers please,0.022770398481973434,24,so,passwords|php|salt|security,is it possible to attack a user password with known salt,4,flaws|attacks|weakness|protection,0.34881192445755005
7259,it s not really broken just has issues with collisions it s recommended that you don t use it because it is the unsalted and is very quick to hash it still would take a  very  long time to crack still you don t want to use it for passwords if you don t have to  md5 was never broken but it was discovered there could be generated collisions hence weakening it  i would suggest ospfv3 it allows sha or md5  ospf is not as simple as rip it is intended for more complex networkings but imho would be a very good choice complexity/authentication security  this is a sample cisco config using sha-512  source,2018-08-27 13:36:56,177,192404,while performing a vulnerability assessment i stumbled upon ripv1 poisoning routing table attacks the recommendation is to use ripv2 with md5 authentication the idea is that the routes need to be authenticated by a password before becoming active  isn t md5 broken? i feel unsure in recommending this to the client is this insecure,0.02824858757062147,5,sse,authentication|hash|server|vulnerability-assessment,is ripv2 md5 authentication insecure,4,poison|attacks|weakness|vulnerability,0.3468598425388336
24889,if the salt &amp hash is only available from a post to the login handler then the damage here is very limited  if there is some webmethod   that returns the data then this is a risk should their be any cross-site scripting xss vulnerabilities elsewhere on the site any attacker could call this method via the xss and then retrieve the hashed password and salt for offline cracking  another low risk is if the json response does not  output anti-caching headers  then another user of the same computer may be able to retrieve their password hash  i am more concerned that the password hashes are in   format rather than in a format using a secure algorithm such as  bcrypt  or pbkdf2   it s not the greatest but it is generally ok to disclose the salt  you re thinking of a  pepper  which is to be kept secret  the salted hash is not meant to prevent a brute force attack  it is meant to prevent a  rainbow attack   by including the salt in the input value to the hashing algorithm it becomes impossible to precompute lookup tables unless the hacker creates a lookup table for each and every possible salt  in my opinion even when it s not something like giving away a password you re giving away information that your front-end will not need at all and that could lead to an attacker getting the password! i mean yes if an attacker gets that information he still needs an exhaustive search with all the possible password combinations concatenated with that salt or hashing a password dictionary with that salt but you re giving him resources for an offline attack and now he can try as much different passwords as he wants until he gets bored or he gets the real password  someone may be thinking that it s the same as an attacker trying to authenticate with different passwords but the main difference is that in an online attack you can limit the number of login attempts so he ll not be able to try as much as he wants while in an offline attack he can try as many passwords as he wants  all this could be avoided by just sending a boolean instead of the full object and since it s not like it will require a huge refactory or something like that i think that it s something that needs to be fixed and you should also take a look at what he does with that information in the worst case scenario he s retrieving the password s hash to store it in a cookie or local storage to keep authenticating the user or something like that,2017-01-05 23:09:21.52 UTC,558,41496339,i have inherited an app to maintain and i just discovered that when a user logs in the returned json from a successfully login contains   primary key of the user record in the db  user name  encrypted password  the password s salt   it seems that having the salt and encrypted password voids the purpose of the salt in general   a brute force or lookup table attack is now available again as a cracking approach   am i correct on this and is there more of a threat than just that,0.025089605734767026,14,so,cryptography|encryption|password-encryption|salt|security,what is the security risk of giving away both the salt and encrypted password,3,attacks|vulnerability|cross site scripting,0.3465106189250946
20309,it helps limit the strain on your wcf server if you allow 1 000 connections and each connection is allowed to send you 1 mb of data - you potentially need 1 gb of ram in your server - or a lot of swapping / trashing might occur  the limit on the message size and the limit on the concurrent connections / calls helps keep that ram usage and also cpu usage to a manageable level  it also allows you to scale depending on your server if you have a one-core cpu and 4 gb or ram you probably won t be able to handle quite as much traffic as if you have a 16-way cpu and 32 gb of ram or more with the various settings including the maxreceivedmessagesize you can tweak your wcf environment to the capabilities of your underlying hardware  and of course as you already mention many settings in wcf are kept off or set to a low value specifically to thwart malicious users from flooding your server with dos attacks and shutting it down,2010-01-22 13:29:13.56 UTC,265,2117447,the only time we notice this value appears to be when the service crashes because the value is too low the quick way to fix this is to set it to some very large number then no problem  what i was wondering about is are there any negative consiquences to setting this value high?  i can see that it can potentially give some protection from a denial of service attack but does it have any other function,0.022641509433962263,6,so,maxreceivedmessagesize|wcf,what is the point of wcf maxreceivedmessagesize,4,attacks|weakness|protection|denial of service,0.3463500142097473
9816,the goal of using salts is to protect user s password in case of a database dump from a hacker because a hash is a one way conversion you can t reverse the conversion to get the plain text password hackers are using dictionary of hash to guess user s password using common passphrases so adding a salt will add an additional protection layer against this type of attack  from wikipedia   dictionary attack    rainbow table attack   the salt is not sensitive information it doesn t matter if it is disclosed the salt simply prevents rainbow table attacks on hashes note that salt should have a fairly high level of entropy and the birthday may not be as secure as say 32 bytes from,2013-08-16 12:33:39.637 UTC,306,18273431,i understand the idea of hash+salt when i create new entry to dtb if i have some fixed string for the salt it might not be hard to implement it but how to do it when i want to use for example user s birthday as a salt? saving that password to database is easy but how to hash this during login? i ve googled this piece of code for my   file where they use   value for salt     so if i understand it correctly it means that if i would like to use user s birthday as salt i would have to have it stored in my dtb pull it out from dtb and then use it as a salt? it doesn t make sense to me because if i have in my   table columns       then the password can be hashed but for the possible attacker is it quite clear that the   value will be used as salt is there something i m missing or does it really work so,0.026143790849673203,8,so,java|password-hash|salt|security|spring-security,implementing hash and salt in spring 3 security,3,attacks|protection|sensitive information,0.34566447138786316
66921,in general cpu power is not the first resource to be exhausted when doing dos and even when it is it is not because some specific assembly instruction is cpu intensive but rather because a task e.g sort this list or solve this equation system is cpu intensive  so if you want to exhaust cpu power your best bet it to find such a task aim for something where the cpu usage grows with the size of the input and then provide an unexpectadly large input  but if you just want to dos a system you are likely to be better off targeting something other than cpu usage - bandwith number of simultaneous connections internal memory disk usage etc,2017-11-01 11:27:22,248,172630,assuming i want to overload a server which is running some kind of web-app or gaming server what is the best way for me to figure out which usage of legitimate features on the web-app or gaming server use the most cpu cycles or disk usage?  this is assuming i can t install the same web-app or gaming server on my own machine and then monitor the cpu usage while i m requesting different things from the server to figure out what uses the most cpu cycles   are there any particularly resource heavy asm instructions that are often called?  when are they most frequently called? what are the general ways which i can force the server to call these functions/instructions,0.020161290322580645,5,sse,appsec|attacks|ddos|denial-of-service,most resource intensive assembly instructions,3,ddos|attacks|denial of service,0.34543800354003906
10234,the wikipedia has an entry on the key size  http://en.wikipedia.org/wiki/key_size   it means that with 2 112  trial encryptions you can guarantee that you find the key that was used to encrypt something with 3des that however uses a meet in the middle mitm attack that also requires an absurd amount of memory  also note that 3des only really needs 112 bits of key material you do an encrypt/decrypt/encrypt ede most common or decrypt/encrypt/decrypt ded cycle but the first and last can use the same key 168 bits means you re using a separate key for each cycle that s fine but not entirely necessary  as far as what the key itself is it s basically just a collection of bits that are known but by strong preference difficult or impossible for anybody else to predict or guess in a typical case you don t directly use what the user has entered as a key at the very least you run that through a cryptographic hash to get a constant-size result given the predictability of what user s typically enter you often want to go a step further generate a much more random key and use the user s password only to allow access to that key this means for example that although it may be fairly easy to recover the user s key given physical access to their computer the data sent over the wire where it s generally easier to intercept uses a key that s much less predictable than their password itself  a far as 128-bit vs 1024 bit goes the two aren t normally entirely comparable a huge amount here depends on the difficulty of an attack the worst case is generally a key-exhaustion attack this basically means that you simply try every possible key until you find one that works  if that s the best attack known against an algorithm then a key size around 128 bits is almost certainly adequate against almost any conceivable improvement of conventional technology--that is even if we assume cpus will continue to get a lot faster for a long time and assume processors of tens or even hundreds of tera-hertz can be build carrying out 2 128  operations to exhaust the key space still remains completely impractical like with millions of 100 tera-hertz processors you still don t finish before the heat-death of the universe  larger key sizes with symmetric encryption are  largely  intended to protect against practical quantum computers a working quantum computer of sufficient capacity can carry out different types of attacks than conventional computers and could exhaust a key space much more quickly than a conventional computer so that s the source of things aes 256  when you get to keys substantially larger than that you re typically dealing with encryption like rsa for which practical attacks are known which do  not  involve doing trial encryption until you find a key that works in the case of rsa part of the information that needs to be publicly known can be fairly easily converted to a large number that s a product of two prime numbers if you can factor that number to obtain those two primes you can break the encryption fairly easily  so in this case the strength we need is based on that product being large enough that factoring it is impractical the largest number that would be suitable to use as an rsa key that s publicly known to have been factored was 768 bits  so if you re interested in safety vs a well equipped attacker for rsa you pretty clearly need a key larger than that  note that the meet in the middle attack on 2des the attack on 2-key 3des and factoring large rsa keys have some characteristics in common that make attacking them quite different from attacking plain des with des you can easily trade off between how much you spend and how fast you get a result spend nearly nothing and it takes a  long  time to get a result but you will eventually the more you re willing to spend the faster you get your result  with the other attacks however you need a large amount of memory to carry out the attack at all for these you mostly can t just scale things so spending half as much gets you a result half as fast rather you d need to switch to for example using ssd instead of dram for your storage--and immediately lose a  lot  of speed alternatively you could switch to another algorithm that s asymptotically less favorable but doesn t have quite such a high barrier to entry again for a practical attack you ll typically lose a lot very quickly this way  bottom line for some attacks you get a fairly smooth trade-off between time and money with others you get much more of a step function in many cases a practical attack will simply exceed most people s capacity now and for quite a while to come in most cases  okay let s take this one at a time   key size   in cryptography you have three major elements   the plaintext the ciphertext and the key  the key is combined with the encryption algorithm to encrypt the plaintext and get the ciphertext it s like the state of a function that is   if k was not equal to this is the key and was equal to this is another key then the value of c would be different  the keysize is the size of the key used for example aes256 uses a 256 bit key that means that the key is literally 256 binary numbers long   what is the difference between 128 bit encryption and 1024 bit encryption?   the difference is in the title 128 bit encryption encrypts data with a 128 bit key 1024 bit encryption encrypts with 1024 bits of data the important thing to note here is that with 128 bits you re usually looking at a  symmetric key cipher  and with 1024 bits you re usually looking at an  asymmetric encryption algorithm    brute force attack   yes following the aes256 example if you perform 2^256 possible combinations of a key you will eventually crack any and all aes256 ciphers however 2^256 is a massive number and it s considered computationally unfeasible to attempt that value,2013-03-09 17:28:26.837 UTC,1147,15313728,i read about the key_size in cryptography but i don t understand exactly that what is the specific meaning of key size?  for example in wikipedia saied      triple des has a key size of 168 bits but provides at most 112 bits  of security since an attack of complexity 2^112 is known     is this mean that if we brute force 2^112 times then we can crack all of 3des hashes?  what is the difference between 128 bit encryption and 1024 bit encryption,0.017436791630340016,20,so,cryptography|encryption,what is the meaning of n bit key-size,3,attacks|protection|man in the middle,0.34514325857162476
382,there is no need for the hash function to be deterministic between runs but you can still provide your own hash e.g for unordered containers if it s a behavior you rely on  as for why  cppreference  says     hash functions are only required to produce the same result for the same input within a single execution of a program this allows salted hashes that prevent collision denial-of-service attacks   if the   requirements tells it to be deterministic then you wouldn t be able to provide a salted hash without breaking the requirement  here is the  actual explanation why    this answer  and links in it suggested by  @nathanoliver  is ultimately helpful let me cite important parts   for a non-cryptographic hash function it s possible to pre-calculate massive inputs with the same hashed value to algorithmically slow down the unordered containers and results in a denial-of-service attack  from   issue 2291 std::hash is vulnerable to collision dos attack    for this reason language designers are migrating to random hashing in random hashing the hash value of the string “a” can change every time you run your program random hashing is now the default in python as of version 3.3 ruby as of version 1.9 and perl as of version 5.18  from   do you realize that you are using random hashing?      move to ready rather than immediate as even the permission has been contentious in reflector discussion  from   issue 2291 std::hash is vulnerable to collision dos attack    in practice as far as i understand no implementation of   implements random hashing but you can write your own    from  this answer     p.s  i just googled &quot;hash table dos&quot and found an informative page   the moment when you realize every server in the world is vulnerable,2020-03-06 16:34:37,458,60568162,hereafter we use  n4140  c++14 standard   according to  § 17.6.3.4 hash requirements       the value returned shall depend only on the argument     for the duration of the program       [ note thus all evaluations of the expression   with the same value for    yield the same result  for a given execution of the program  — end note ]   and  § 20.9.12 class template hash  says     ..      the instantiation   shall      1.1 — satisfy the hash requirements 17.6.3.4 ..      1.2 — ..    this means a hash value of   i.e   may take a different value if you restart the program  but why? this limitation was not in the standard of c++11 but in the standard of c++14 c++17 and c++20 as a user not a stl developer it would be quite useful if   were deterministic are there any mathematical difficulties in implementing a deterministic hash function? but hash functions we daily use e.g deprecated   or safer   are all deterministic is there a problem of efficiency,0.026200873362445413,12,so,c++|hash|language-lawyer|std,why is std::hash not guaranteed to be deterministic,3,attacks|vulnerability|denial of service,0.3449742794036865
11136,brute force refers to a solution to a problem that relies on computers being fast to get an answer generally it works by trying all possibilities for example if you want to know the sum of all numbers 1 through 100 you could do something like   that d be brute force you could also do notice that 1 through 100 contains 50 pairs totaling 101 and solve it like this   that s an intelligent approach note that brute force is generally easier to come up with  the concept is extended to security in an obvious manner for example if you want to break into someone s account on a system that requires 8-character passwords you could just start trying passwords — aaaaaaaa aaaaaaab .. — and eventually it ll work that s brute force you could try a list of common passwords less brute force-ish or you could notice the site stores who you re logged in as in a cookie and edit the cookie not brute force  similar with breaking encryption you could try all possible keys brute force not going to finish this lifetime on a reasonable cipher or you could analyze the cipher for weaknesses very hard if its a good cipher definitely not brute force  and to tie it all up if you want to take down a site you could just send a bunch of traffic/requests/whatever in its general direction that s brute force that s a dos attack  just to give  an example of a dos attack that doesn t involve brute force  pretend that there s a website that locks a user account after three failed login attempts  i know that you have an account on that site and i know that your username is jdoe  i decide i don t want you to be able to use the site so i try to log in as you three times failing each time  your account gets locked out and you have to call the admin to get it reactivated  then i do it again the next week just to make a nuisance of myself  in essence i m using the site s lockout feature to deny service to you but brute force isn t involved   i suspect that the confusion between the two stems from the following  the most popular cases of dos involve overwhelming servers with network requests sounds like somebody is applying brute force to the server and in common everyday language that might be right  but really brute force has a special meaning in computing it describes algorithms that exhaustively search a solution space for a correct solution instead of using more refined methods like heuristics intelligent guessing or whatever so in security a brute force attack involves trying all possible keys all possible passwords etc  dos or denial of service is an attempt to make a computer resource unavailable to its intended users it generally consists of the concerted malevolent efforts of a person or persons to prevent an internet site or service from functioning efficiently or at all temporarily or indefinitely  a brute force attack is a method of defeating a cryptographic scheme by systematically trying a large number of possibilities for example trying a large number of the possible keys in a key space in order to decrypt a message   brute force  attacks use a technique of attempting to try every combination of passwords/keys to gain access to a particular system  what the hacker does when they gain entry to the system depends on the motivation of the hacker   dos denial of service  attacks describe cases where the motivation of the hacker is to bring down the system causing maximum inconvenience to the users of the system  they can t really be compared against each other as brute force is a  technique  to gain entry and dos is a  type  of attack  it is possible that an attack could be  both  brute force and dos  a brute force attack refers to attempting every possible combination usually in a cryptographic context for example if i m guessing your password i can start with a and then b and then c and so on or if i m trying to solve a sudoku puzzle i can try every possible combination until i find one that works.  obviously this is unrelated to a denial of service attack which usually refers to sending so many bogus requests that a server is overwhelmed if you re seeing both phrases in the same context the author is probably confused,2009-01-16 06:13:26.943 UTC,808,449650,i was reading about dos attacks on apache servers but the brute force word pops up sometimes i know dos attacks but brute force seems to be similar is there a difference or it is just another word of dos,0.034653465346534656,28,so,apache|security,what is the difference between dos and brute force attacks,3,attacks|weakness|denial of service,0.34459835290908813
67592,first row is guess leaking by guessing second timing for this guess so best guess that syscall sysread is at 3a50 that matches reality,2018-01-03 20:00:00,97,176613,based on this poc   https://twitter.com/brainsmoke/status/948561799875502080        i see there are four rows outputed the second one are two bytes of address of sys_read what is the first one? would third row be sys_write? i did some calculations on my laptop with kernel 4.10 and the offsets of the syscalls on my laptop and the screenshot outputs don t match ...  anybody has ideas what are those,0.020618556701030927,2,sse,exploit-development|intel,kpti - meltdown - poc analysis,2,leak|exploit development,0.3439079225063324
65435,changing the algorithm because the attacker will not know it is a  very poor reason  for changing an algorithm to begin with the attacker may infer that you will use an algorithm for which support exists in the code which limits the possible choices and the algorithm can then often be disambiguated based on what can be seen on the output if the output is 160 bits that s sha-1 not md5  it would be a severe delusion that changing the algorithm really brings by that attacker won t guess it reasoning significantly increased security algorithms cannot be assumed to be secret because information about the algorithm leaks everywhere in the code in the code  behaviour  e.g execution time in the head of developers.. and the space of possible algorithms is tiny  now changing from md5 to pbkdf2 or bcrypt is a  very good idea  not because the attacker would assume that it is md5 and never guess that bcrypt was used in fact bcrypt s output tends to be obvious in that respect they have a specific encoding of output + salt as a string which states clearly that bcrypt was used but because a basic md5 is a  terrible  way to hash passwords if you want to store hashed passwords at least  do it correctly  which means right now pbkdf2 or bcrypt,2014-01-13 15:20:51,502,48587,numerous clients have told me that my suggestion on changing the algorithm on a web app does not increase security i ve tried reasoning that if the files have not been exposed the attacker has to guess at the algorithm  a few days ago i was assessing a friend s site and pointed out a few sqli vulnerabilities i didn t have any access to the files like above my friend told me i was wrong to suggest a different algorithm even though he can handle the trade offs to a different algorithm  my reasoning with him was that an attacker wouldn t know your algorithm so you ruled out a minority of attackers just by changing algo if they used a tool like hashcat and tried to crack the hash with a preset it wouldn t lead them anywhere even though this is security through obscurity it works in a way that you defended yourself just a little if you have another sqli  he insisted on not changing the algorithm because anyone who exploits the site is an automatic white flag he assumes that if a hash is retrieved everyone will take the time to crack it  the default hash is md5 i suggested pbkdf2 or bcrypt  just as a double opinion:who in theory and actuality is correct in this situation?  is it correct that he should put up a white flag if someone retrieves an admin s hash?would i be correct in saying this is a practical use of security through obscurity?.. or if it s not security through obscurity at all,0.021912350597609563,11,sse,hash|sql-injection,am i right by changing the default algorithm on a web app,5,leak|attacks|exploit|sql injection|vulnerability,0.3437425196170807
3337,,2020-03-03 13:23:48,127,226740,half_md4 doe not look a veri strong hash for me although googl for it doe not reveal too much i suspect the directori entri could be organ by a half_md4 hash hash tabl if it is so then creat mani file have the same hash on an ext4 filesystem would not be veri hard and consid that most directori oper are atom in most ose it could be use for dos attack for exampl any websit where document could be upload and store in the server-sid filesystem by their origin name could be slowen or even die with it is it a possibl danger? doe ext4 has some inher protect against it,0.031496062992125984,4,sse,denial-of-service|hash|linux,is the half_md4 dirent hash algorithm a secur problem in ext* filesystem,3,attacks|protection|denial of service,0.3435956835746765
24676,md5 has some known vulnerabilities whereas sha-256 does not because of this i would suggest sha-256   here s a link that describes why  although i have no idea if they are actually affiliated with the us department of homeland security the site does explain what i mean   the wikipedia article on md5  also discuses the weaknesses,2010-09-22 00:56:48.08 UTC,80,3765582,is there any real difference between the two,0.0375,3,so,java|password-protection|passwords|spring|spring-security,which password encoder for spring security s daoauthenticationprovider md5 or sha-256,3,weakness|protection|known vulnerabilities,0.34318166971206665
1353,lanman passwords are maximum 14 characters truncated and then split into two of 7 character length and converted into upper case this means that the maximum password strength is 26 characters+10 digits+~10 special characters which results to 46^7 or 435818 million combinations on a modern computer with a fast gpu these combinations can be tried out brute-forced very fast in a matter of minutes assuming you have access to the hash - if any networking is involved it s much much slower but better don t even store that lm hash if not needed by any very old system such old systems would be windows for workgroups wfw 3.11 or such systems from 1993 or so  so the recommendation is to disable lm hashing and use only ntlmv2 with ntlmv2 the entire password is hashed no splitting in two parts and also no case changing so if you use a long password longer than 7 chars and mixed characters upper case lower case digits special then it s not easy to try our all combinations but one could still easily try out all 6-char passwords in minutes or all 7-char passwords that don t contain upper case characters etc in the same time as lm passwords  fortunately lm hashing is disabled on machines newer than windows xp by default  in ntlm both v1 and v2 the authentication protocol itself is not bad -- as long as you play it within a secured connection indeed     active  attackers who can eavesdrop on the network and also inject pakcets of their own can hijack open connections at will if your authentication protocol is performed on a basic tcp connection an attacker just has to wait for a normal connection from an authorized user and steal it once the authentication step is done    even passive attackers observing the protocol messages will obtain enough information to run an  offline dictionary attack  they can try potential password until one is found that matches the messages seen on the wire and they can do that on the privacy of their own machines that s the offline part     ssl/tls  fixes both issues if the client first opens a ssl connection to the server with due server authentication through its certificate and so on and the rest of the communication including the password-based authentication protocol occurs within the encrypted tunnel then you will be safe from attackers however under these conditions a simpler show the password protocol in which the client simply sends the password as is in the ssl tunnel would be better  indeed attackers are inventive and they don t restrict themselves to simply spying on the network it often happens much too often that attackers gain some glimpses of server data e.g through sql injections or by recovering old hard disks from dumpsters this is why servers  should store only hashed passwords  but there are good hash functions for that and there are also  bad  hash functions see  this answer  for a lot of details when using the simple show the password protocol the server is free to use any password hashing function that it sees fit however when using ntlm the server  must  store the hashed value that the protocol mandates i.e nt hash or lm hash  unfortunately nt hash is very weak against a brute forcing attacker it is very fast an attacker with a pc+gpu can try  billions  of passwords per second and it is unsalted attackers can collude and use precomputed tables such as  rainbow tables  to run attacks almost for free lm hash is  even worse   so you really should not use ntlm if you can avoid it   a weak password is weak however you put it because attackers can also perform  online  dictionary attacks by simply speaking themselves to the server under these conditions a weak password can be tolerated only with strict lockout procedures such as closing the user account if four consecutive bad passwords are entered this is what most smartcards do however this is hardly appropriate for a server because it would make it too easy for evil-intentioned person to lock the accounts of other people  so you need to  educate  your users into choosing strong passwords for that matter ntlmv1 adds some extra insult to the injury through  lm hash  which restricts passwords to 14 characters  and  maps lowercase letters to uppercase password strength depends on the  entropy  a measure of what the password could have been entropy needs some room especially because humans are involved humans are bad at making random choices but also at  remembering  random choices their task is made easier if they are allowed to choose  long passwords  but lm hash does not allow long passwords the next best thing then is to add extra entropy by for instance making each letter uppercase or lowercase on a random basis but lm hash prevents that too !  allowing the storage and use of lanman lm hashes is a security weakness as lan manager splits the password into two case insensitive 7 character pieces before hashing which makes cracking the password hash much easier  in general forcing ntlmv2 and disabling the storage of lm hashes is a good idea from a security perspective and as no supported version of windows requires lm hashes would seem like a pretty safe bet these days  a weak password can almost always be cracked if the attacker gets a hold of the hashed version of it that is stored on a system  an attacker can use a dictionary attack which is usually pretty quick to crack many weak passwords and for short non-dictionary passwords rainbow tables can be acquired to speed up the cracking process,2013-07-07 05:44:47,1011,38546,"a client that allows to respond to the server with lm and ntlm with a password which is simple, is it insecure  isnt a more secure way is to have all the communication with ntlmv2 scheme? and in addition to that have a complex password  is it true that a weak password can be cracked even in ntlmv2",0.026706231454005934,27,sse,authentication,ntlm security and password complexity,6,hijack|attacks|weakness|weak password|eavesdropping|sql injection,0.34114065766334534
4228,what you are referring to is  kerchoff s principle  beside that a couple of formalizations of security criteria along with very simplified explanations are  ciphers    indistinguishability under chosen-plaintext attack ind-cpa    indistinguishability under [adaptive] chosen ciphertext attack ind-cca1 &amp ind-cca2    these roughly say that ciphertexts should not leak information about the plaintext and that plaintexts should not allow an adversary to infer anything about their corresponding ciphertexts some more details  here   macs and digital signatures   strong existential forgery under chosen message attack suf-cma  weak existential forgery under chosen message attack wuf-cma   see  page 11 of this paper  these roughly describe the ability of an adversary to forge a signature for a given message without having knowledge of the signing key  hash functions    collision resistance    preimage and second-preimage resistance    these roughly say that given the output of a hash function it should computationally intractable to find the input to the function that produced that output or to find two values that produce the same output when fed through the hash function,2016-03-13 04:38:18,227,117293,i remember reading a book on cryptography with a section detailing the objectives that modern cryptography must accomplish one being that even if the algorithm is known the method remains secure i.e the so-called  kerckhoffs s principle  what are the other standards an algorithm must accomplish and why are they necessary,0.03524229074889868,8,sse,cryptography,the requirements of a cryptographic algorithm,4,leak|attacks|forgery|weakness,0.3406089246273041
1153,,2020-02-05 17:43:53,85,60081340,i have two small posit real number u w such that u > w given i d like to find a numer stabl way to calcul one possibl way of transform the preced is there are stabl method which calcul for small p and subsequ log-sum-exp trick may be employ the problem is may be of order of -10000 henc i don t think straightforward exponenti is an option here any suggest greatli welcom,0.011764705882352941,1,so,exp|numerical-methods|underflow,equival of log sum exp trick for subtract,1,underflow,0.33959639072418213
15600,generally speaking hashing is still fine these days the thing that matters here is what hashing algorithm you use and how many iterations in case of database leaks you want to make it as difficult as possible to match inputs commmon password/dictionary based attack salting does help a little bit so does having an irregular pattern salt or a hidden number of iterations based on the username etc having different hashing strategies helps as long as the attacker doesn t know how your hashing is implemented accessing the database server is one thing accessing your source code another. it s about causing effort to the attacker  now about hashing algorithms sha-2 is easier to attack than for example bcrypt due to being targetable by gpus the number of iterations on the hash will take the attacker more time for each password bcrypt isn t supported by   but at least sha-512 is   supports iterations  see docs  rule of thumb is having an iteration count that takes at least a second to process on your server hardware  on a side note don t trim the password input people might intend to use leading/trailing whitespaces,2018-02-16 21:21:30.517 UTC,371,48835037,i m creating new login system for my single page app this system will require administrator to create account for the users once they setup account for the user i will send them an email where they have to enter their information like security question and password so i have done some research and looked over our existing system there is   function that is used together with   i read few articles and there is a lot of debate on hash being vulnerable  also i see that in this case hashed password is stored as well as salt they are in separate columns is this good practice to store salt in db? also is there better way to store password in database? here is example of the logic that i found   i m currently using cold fusion 2016 i m not sure if there is some better way to encrypt the password in cf if anyone can provide some useful resource or example please let me know thanks,0.018867924528301886,7,so,coldfusion|coldfusion-2016|hash|passwords|salt,best practice for storing password in database,3,leak|attacks|vulnerability,0.3395521342754364
66827,"the login credentials were found in password dumps from other sites they were credentials where the username was a .gov email address  the concern is that people tend to reuse passwords and the passwords used on these sites are  the password for their government login credentials  either the passwords were stored in plaintext or the hashed passwords were guessed/brute forced  what s the point of stealing hashed passwords?  let s say i steal a hashed password i can take a random string hash it and see if the hashes match if they do then i ve just cracked your password  for example assume that under some hash function we get the following hash table   if i steal a password database and see that your hashed password is   then i know that the plaintext password is dog technically more than one string will hash to ab but since the server is comparing hashes not raw passwords any of them will let me in so in effect your account has multiple passwords any string which hashes to ab is a valid password for your account.  attackers can crack hashed passwords by pre-computing hashes for large numbers of potential passwords usually obtained by combining dictionary words or from lists of previously cracked passwords and then using this as a lookup table to convert hashes back into plaintext passwords   this kind of attack is generically referred to as a  rainbow table attack  with introductory explanations  here  and  here   one issue with rainbow tables is that to cover any reasonable number of candidate passwords the size of this lookup table grows to terabytes and requires multiple hard drives just to store it as you can imagine attackers have developed fancy tricks for optimizing the reverse-hashing process so while the general idea of rainbow tables is simple actual optimized implementations get very complicated very quickly   it s worth mentioning a few of the common counter-measures that db admins and developers use against rainbow table attacks      salting   is the trick of appending a random string the  salt  to the raw password before hashing it each user has a unique salt which means that in effect each user uses their own unique hash function so an attacker can t re-use rainbow tables and has to pre-compute a brand new table for each user     take the password and run it through a large number of salt-and-hash iterations of a strong cryptographic hash function before storing it in the db for example 100,000 iterations of sha-2 which makes it much more expensive to pre-compute the rainbow tables it also makes it slower for users to log in    use a hash function which takes in a private key these hash functions are called  hmacs  the idea is to use a private-key known only to the server in computing the hashes so that it s impossible to compute a rainbow table without knowing the key the problem with this is that when an attacker has enough access to steal a whole database they can usually steal the private key as well    in your question you also mention storing encrypted passwords this is similar to 3 in that unless you have a good way to protect the encryption key for example dedicated hardware like an  hsm  then the attacker will steal the key along with the database  i think you are missing something -- for the opm break in the problem was that they stole a  live  credential  that is very useful  having said that note that no password should be useful  in theory every worker is issued a smartcard that hosts an e-auth level 4 authentication mechanism in their personal identity verification card  this is the fips 201 thing or hspd-12.  while at this point no system is required to get rid of all passwords of which i am aware all systems are required to be compatible with piv authentication and it is a no-brainer that if you have the thing and need to interop with it go ahead an use it  hopefully with some saml hook that allows you to have alternatives when your piv isn t available but has the same strength  phone callbacks are pretty normal",2015-06-25 19:44:22,774,92421,in the wake of all these leaked/stolen/hacked government login/passwords being stolen it got me thinking  how is the leaking of these passwords even possible? or are they of any real use?  aren t all these passwords hashed or encrypted somehow? unless the passwords are stored in plaintext what s the point in even trying to steal them in the first place,0.01937984496124031,15,sse,credentials|data-leakage|government|hash|passwords,how are leaked government credentials possible and what s the point,4,attacks|protection|data leakage|plaintext password,0.3391832113265991
3332,user names are predictable i would prefer using random value as a salt an attacker could begin the generation of precumputed hash table targeting one user admin? even before obtaining the hash&amp;salt to crack  so you should  not    edit    the main purpose of having a salt is that rainbow tables are useless from my point of view using a guessable value even with static value added as salt is bad practice as you allow an attacker to prepare the hashes before obtaining your database  in some scenarios  if you distribute your code and you have a default admin account an attacker will be able to reuse the rainbow table against all the installations of your software  other example from @anti-weakpasswords   pyrit  a wpa/wpa2 cracking tool tool advantage of the wpa/wpa2 protocol using the ssid username equivalent as the salt to allow the precomputation of reasonably sized dictionaries against common ssid s - linksys dlink 2wire047,2015-06-21 17:14:07,237,92130,instead of creating a separate salt generator for each user can t i just use their username as a salt? it would be unique for everyone and achieves the purpose of the salts   if the username is too short to be used as a salt i can just add a global string to the username   would this be sufficient for security?it still creates a unique salt for each user and prevents rainbow table attacks,0.02531645569620253,6,sse,hash|salt,using username as salt safe enough,3,attacks|weakness|bad practices,0.33831408619880676
19847,imagine a long block of material to compare  if the first block does not match and the compare function returns right then you have leaked data to the attacker  he can work on the first block of data until the routine takes longer to return at which time he will know that the first chunk matched  2 ways to compare data that are more secure from timing attacks are to hash both sets of data and to compare the hashes or to xor all the data and compare the result to 0  if == just scans both blocks of data and returns if and when it finds a discrepancy it can inadvertently play warmer / colder and guide the adversary right in on the secret text he wants to match  jfriend s answer is correct in general but in terms of this specific context comparing the output of a bcrypt operation with what is stored in the database there is no risk with using ==  remember bcrypt is designed to be a  one-way function  that is specifically built to resist password guessing attacks when the attacker gets hold of the database  if we assume that the attacker has the database then the attacker does not need timing leak information to know which byte of his guess for the password is wrong he can check that himself by simply looking at the database  if we assume the attacker does not have the database then timing leak information could potentially tell us which byte was wrong in his guess in a scenario that is ideal for the attacker not realistic at all  even if he could get that information the one-way property of bcrypt prevents him from exploiting the knowledge gain  summary preventing timing attacks is a good idea in general but in this specific context you re not putting yourself in any danger by using ==   edit  the bcrypt.compare  function already  is programmed to resist timing attacks  even though there is absolutely no security risk in not doing this  the point of a constant time string comparison is that the comparison will take the exact same amount of time no matter what the comparison target is the unknown value  this constant time reveals no information to an attacker about what the unknown target value might be  the usual solution is that all characters are compared even after a mismatch is found so no matter where a mismatch is found the comparison runs in the same amount of time  other forms of comparison might return an answer in a shorter time when certain conditions are true which allows an attacker to learn what they might be missing  for example in a typical string comparison the comparison will return false as soon as an unequal character is found  if the first character does not match then the comparison will return in a shorter amount of time than if it does  a diligent attacker can use this information to make a smarter brute force attack  a constant time comparison eliminates this extra information because no matter how the two strings are unequal the function will return its value in the same amount of time  in looking at the  nodejs v4 crypto library  i don t see any signs of a function to do constant time comparison and per  this post  there is a discussion about the fact that the nodejs crypto library is missing this functionality  edit node v6 now has      there is also such a constant time comparison function available in this  buffer-equal-constant-time module,2015-06-28 03:36:55.97 UTC,662,31095905,github s  securing webhooks page  says     using a plain   operator is not advised a method like   performs a “constant time” string comparison which renders it safe from certain timing attacks against regular equality operators   i use   when comparing passwords  what makes this a secure compare and can i do this using the standard   library in node,0.028700906344410877,19,so,cryptography|node.js|security,what s the difference between a secure compare and a simple ==,3,leak|attacks|exploit,0.3382054567337036
54783,the v1 orm caches every model that gets created performing a query in a loop is also not the best way to go as if you have hundreds of thousands you will be performing hundreds of thousands of queries  if you have that many rows to work on at once you have two options the first is to use the db class to perform your queries the second would be to run the loop in smaller batches at a time   i can also recommend using   to load more than one model at once cutting the query out of the loop,2014-06-04 07:29:12.14 UTC,165,24031488,i am running a code like this   and memory consumption is increased with each loop too much to ignore - around 300kb each time and number of iterations is hundreds of thousands   so there is a question - what i have to destroy or close to keep memory consumption level,0.012121212121212121,2,so,fuelphp|memory-leaks|php,memory is leaking when i create models in a loop in fuelphp,1,memory leaks,0.3379024863243103
25883,"can anybody elaborate on such security features? does the database lock-down after maybe like 30 consecutive attempts or something?   no but it uses 64,000 rounds of pbkdf2 by default quoting  the wikipedia article on pbkdf2      pbkdf2 applies a pseudorandom function such as a cryptographic hash cipher or hmac to the input password or passphrase along with a salt value and repeats the process many times to produce a derived key which can then be used as a cryptographic key in subsequent operations the added computational work makes password cracking much more difficult and is known as key stretching    basically the attacker has two choices    try things like dictionary attacks e.g testing against a list of common passwords the pbkdf2 rounds add overhead for each check such that cracking the passphrase will take inordinate amounts of time for all but the weakest passwords    bypass pbkdf2 overhead and try attacking using generated derived keys directly in this case there are far too many possible keys to try so again it becomes impractical to find the desired key    it is the type of encryption used  sql cipher uses 256-bit aes encryption  this form of encryption is extremely secure against brute-force attacks i.e programtatically attempting attempting to guess the encryption key by trying every possible value     this article  does a nice job of describing just how secure aes is to quantify this the article states     if you assume every person on the planet owns 10 computers there are 7 billion people on the planet each of these computers can test 1  billion key combinations per second on average you can crack the key  after testing 50% of the possibilities then the earth s population  can crack one encryption key in 77,000,000,000,000,000,000,000,000  years!   the sqlcipher design and security features is described in detail  here   the source code is also available  here  for review",2014-08-04 00:08:00.02 UTC,368,25110237,so the commonsware book seems to suggest at location 30091 that sqlcipher has many protections against a user copying the database and manually trying many different password to guess the correct version  can anybody elaborate on such security features?  does the database lock-down after maybe like 30 consecutive attempts or something,0.019021739130434784,7,so,android|sqlcipher,sqlcipher security against multiple access attempts android,4,bypass|attacks|weakness|protection,0.33776605129241943
199,dictionary attacks can happen off-line against data that the attacker has captured  strong passwords are thus the best solution  i do not know if your question is about dictionary attacks in general or dictionary attacks in the case of a wifi network with password protection   for the general question of dictionary attacks  there are two kinds of dictionary attacks the  online  attacks and the  offline  attacks an offline attack is one such that the attacker got enough data to test passwords on his own machines at a rate which is limited only by whatever computational power he can muster for instance the attacker got a copy of the hash of a password on the other hand an online attack is one where the attacker must interact with an honest system one which knows the correct password e.g a target server or the client itself for each guess  a password strength can be measured by its  entropy  which is a way of stating how many values that password  could have  assumed for instance a password with 25 bits of entropy is such that it has been chosen randomly and uniformly among a list of 2 25  possible passwords the notion of entropy can be refined a bit in case the password selection process is not uniform we say that a password has  n  bits of entropy if an attacker trying a list of potential passwords in decreasing order of probability i.e beginning by the most frequently chosen passwords will hit the right password after an average of  2 n-1   trials depending on your user base you  might  have a bit of success at educating your users into choosing strong passwords but it is not realistic to expect more than say about 32 bits of entropy if you enforce too strong password selection rules users will actively work against you e.g by writing passwords on stick-up notes or sharing passwords with other sites or other users  the first mitigation in the presence of offline attacks is to use a  salt  the idea is to tweak the password-verification data that the attacker can know with a publicly known value the salt which is different for each password this will not hinder an attacker bent on breaking a single password but it will prevent cost sharing if the attacker wants to break 10 passwords it should cost him 10 times the cost of breaking one precomputed tables in particular the much hyped  rainbow tables  are a specific case of cost sharing a good salt is chosen randomly and uniformly with a good random generator and stored along with the password hash  the second mitigation is to make password derivation expensive you do not hash  once  you hash  ten thousand times  this makes normal password usage for verification 10000 times slower but this can often be tolerated we are talking about 10ms instead of 1µs but also multiplies the work factor for the attacker by 10000 which turns say a one-minute attack into a one-week attack security is then achieved if  2 n-1  &gt p*s  where  n  is the password entropy  p  is the ratio between the attacker s and the user s patience e.g if the attacker is ready to invest one week of effort and the user cannot wait for more than one second the ratio is 7*86400 = 604800 and  s  is the ratio between the attacker s computing power and the normal system power e.g the attacker has 20 pc with big  gpgpu  and the normal system is a smartphone this brings  s  in the range of 500 or 1000   bcrypt  is the oft-recommended password hashing method which combines a salt and a configurable number of iterations  a more thorough solution is to avoid offline dictionary attacks you should not let an attacker get hold of any data which allows him to perform such an attack in a web/internet context this means that for instance you will perform authentication within a  ssl/tls tunnel  something known as https you would still want to do good password hashing for password  storage  on the server in case the attacker gains a read-only access to your database another kind of protocol is  password authenticated key exchange  a cryptographic protocol which results in a shared key suitable for subsequent symmetric encryption of data with mutual authentication of client and server relatively to a password this protocol can be played in full view of the attacker and it is still inherently resistant to offline dictionary attacks the most recommended pake protocol is  srp   if you can force the attacker to play things online then you can thwart him by enforcing arbitrary limitations in the number of requests he may submit the most extreme case is what smartcards do after three wrong pin the card commits suicide weaker rules e.g refusing to process more than 10 guesses per minute will already dispel most attackers   about wifi  there are several  authentication protocols  which can be used in wifi in wep and wpa-personal systems authentication is called psk pre-shared key encryption and integrity checks will be performed with keys derived deterministically from the wifi password this gives plenty of data for an attacker who wishes to perform an offline dictionary attacks since the key derivation protocol does not include provisions for a high number of hashing iterations after all it must be implementable with 30$ home routers dictionary attacks tend to be quite effective so the only real defense here is to select big fat random passwords so that the entropy is high  with wpa-enterprise authentication is done through a generic layer called  eap  which encapsulates messages for an underlying protocol the base station is supposed to forward those messages to a  radius  server there are many authentication protocols which are then applicable some of which being of the psk persuasion but others are arguably stronger for instance there is an eap-eke which is a pake protocol hence resilient to offline dictionary attacks another one is eap-tls which internally performs a full ssl/tls handshake and thus potentially may use  srp-with-tls   thus with wpa-enterprise you  may  use authentication protocols which tolerate passwords with relatively low entropy but this depends on what the client and the base station will support   a word to the wise  i only talk about the use of passwords in wifi authentication i do  not  claim that once authentication has been performed the wifi link is adequately secured it is best to treat a wifi link as an internet-like link subject to eavesdropping the main goal of the authentication protocol is to deter attackers who want a free internet access,2011-08-08 10:37:34,1167,6020,i know a few people with pretty weak passwords what kind of systems exist to prevent dictionary attacks? would it make sense to restrict the number of connection attempts in a certain timeframe?  obviously a strong password would be the best system but most private networks do not take much consideration into setting up their wifi,0.038560411311053984,45,sse,brute-force|cryptography|network|passwords|wifi,dictionary attack on wifi,5,attacks|weakness|protection|weak password|eavesdropping,0.33752259612083435
67053,,2012-05-23 15:41:30,53,15298,i ve been messing with hydra brute force to solve the damn vulnerable web appbrute force section but the problem is when i use   it said   and the brute force command is   i can t find out why the module is bad,0.03773584905660377,2,sse,attacks|brute-force|http|linux|web-application,how to use http-get-form in thc-hydra,2,attacks|vulnerability,0.33561211824417114
25025,this  decreases  security  the md5 hash usually shortens the password this means an attacker only needs to iterate over the md5 space even if you consider the md5 space large it is trivial to map a word dictionary into it this simply adds no benefit but potentially reduces the space  regarding collisions you do not win the galactic lottery with bcrypt with md5 however things are a tiny bit less certain which is another reason to stay away from it  you do not need to add a salt bcrypt adds a good unique salt by itself already if you use bcrypt correctly you never need to think about the salt  it is a bit strange that your source only sends md5 hashes i would usually recommend to send the password plaintext over an encrypted connection   if the connection is encrypted it is secure to send it as plaintext  if it is not encrypted it does not matter if you hash the password first an attacker can read if from the network anyway   although its perfectly fine to use   in your application security wise it has little benifit also it isnt needed to add your own salt to the encryption bcrypt will take care of that internally and you dont need to save the salt anywhere  an attacker targets the weakest link and if the other server just uses md5 they can attack that site to get the password and then it doesnt matter how strong you secured it  but then again closing one door is still beter then leaving everything open,2013-05-06 13:57:10.897 UTC,394,16400329,i m trying to share logins between my app and an ipb forum  i ve seen that invision is providing a module to share the credentials  ips connect   to make it simple there is a master application and one or severals slaves the slaves are sending the credentials that the master need to store through an api  the stuff is that for the   or   methods ipb is sending an   hash of the password there is no way i ll store an   in my db so i was think to use   on the   hash doing something like    what do you think about this alternative is it a good practice to hash with bcrypt on top of a md5 hash,0.017766497461928935,7,so,bcrypt|md5|password-protection|passwords|php,is it a bad practice to use bcryptmd5pwd + salt,4,attacks|weakness|protection|bad practices,0.33485400676727295
20268,at the end it would just be a simple string comparison which is 2 orders of magnitude faster than the actual hmac calculation in my tests   but it is  not constant time  just because they are done fast doesn t mean the difference is not measurable for   values python first tests for equal length and equal first bytes before using     to test the rest  for strings python compares length then  kind  if the string uses 1 2 or 4 bytes per character then also uses    the linux manpage for   explicitly states     do not use   to compare security critical data such as         cryptographic secrets because the required cpu time depends on the         number of equal bytes  instead a function that performs comparisons         in constant time is required  some operating systems provide such a         function e.g netbsd s   but no such function         is specified in posix  on linux it may be necessary to implement         such a function oneself.   a sufficiently determined attacker can exploit this weakness to figure out what hash you have stored vs the hash of the data it is sending  timing attacks make it possible to  forge signatures  say a service stores authorization information in a token shared with the client if the client could alter this token they could gain access they would not otherwise have to protect against this the token is signed using an hmac signature letting the server verify the returned token before accepting it as valid if the authorization data doesn t match the signature the token is rejected  if the server does this   then an attacker can detect how many characters of a forged signature match the expected signature and adjust accordingly they start with   then try   etc until the time taken by the service increases indicating that they have a matching first character then the second character can be altered until the full signature matches,2019-03-20 12:41:38.073 UTC,441,55261040,python has a method specifically for comparing hmac to prevent timing attacks  https://docs.python.org/3.7/library/hmac.html#hmac.compare_digest   and i read about timing attacks here  https://security.stackexchange.com/questions/74547/timing-attack-against-hmac-in-authenticated-encryption   my question is how could it possibly ever not be constant-time? it would be necessary to calculate the actual hmac in order to compare it and it s not like you could calculate the digest 1 character at a time right? at the end it would just be a simple string comparison which is 2 orders of magnitude faster than the actual hmac calculation in my tests so where exactly is the attack surface here? could someone please give an example of exactly where the actual vulnerability is if i don t use  ,0.027210884353741496,12,so,hmac|python-3.x|security|timing-attack,how could hmac comparison ever not be constant-time in python,7,attacks|exploit|weakness|protection|timing attack|vulnerability|attack surface,0.3338087201118469
3149,please look at how siphash replaced old hash mechanism to counter-measure dos through hash collision   https://131002.net/siphash/#at   -> slides of the presentation hash-flooding dos reloaded attacks and defenses,2015-04-24 00:24:25,67,86760,is there an exploit for  this  vulnerability?  edit  python dos via hash algorithm collision -  ocert advisory 2011-003  affected numerous languages no cve for python but cves for other languages,0.14925373134328357,10,sse,denial-of-service|python,is there an exploit for python hash collision,5,cve|exploit|attacks|vulnerability|denial of service,0.3337242007255554
2603,the browser checks both fingerprints the idea behind that is if it is possible to create a fake certificate with the same   or   hash there is a much lower almost zero probability the same certificate second hash also matches this could be called dual hash fingerprinting both   and   are considered vulnerable in theory   also in practice this is one reason to move to   even if a collision is possible for   it is almost impossible to get the same collision for    a note here is that longer fingerprints may be truncated and   has longer fingerprint than   and the same with   and,2014-10-29 13:04:18,196,71891,when a https site shows multiple finger prints i.e md5 and sha1 or sha256 and sha1 how does the browser check?is it both just the strongest or whatever is convenient?  this question arose because somewhere it was stated that a site used weak protection because of sha1 but that site also showed a sha256 fingerprint  is it possible that because of also having sha1 that that counts as the weakest link for spoofing man-in-the-middle etc.?or in other cases md5 as weakest link,0.03571428571428571,7,sse,hash|http|internet|tls|web-browser,how does https check md5 sha-1 and sha-2 fingerprints,5,weakness|spoofing|protection|vulnerability|man in the middle,0.33056172728538513
28047,,2018-07-28 18:51:34.967 UTC,151,51574301,i m trying to extend the code found  here  for the random walk problem to handle walkers walking in a 2d grid in 4 directions to simplify the problem a bit my code assumes the subdomain size to be always 4 and the number of processes world size is always a perfect square that will be used to calculate the grid my first step is calculating the 4 neighbors of each process then i do the walk all that seems to be fine my problem is the program seems to always reach a deadlock and i m unable to find an efficient way to solve it my idea was to calculate the row index of each process in the grid and processes in the even rows send first i.e   however that doesn t seem to work here s my code for the send/receive    ,0.013245033112582781,2,so,deadlock|mpi|random-walk,2d radom walk with mpi,1,deadlock,0.3305559754371643
19782,you must use the same entropy value  you could make a salted hash table out of the entropy value  is this the way to salt and store a password in db?   i assume you meen it is protected with protect and read with unprotect  you don t need to protect the entropy its not the key dpapi on windows uses the user s login credentials to generate the key effectively entropy in this case is just like a salt value for a password hash in fact its probably literally the salt being feed to pbkdf2  now there is a problem with dpapi everyone can read the data once you are logged in e.g i can write a program that if it can read your configuration file  and it probably can i can make the same call you can and since your logged in the value decrypts as a slight mitigation to this you might want to hard code part of the entropy value in your program and read the other half from the config file> this doesn t do too much since a motivated attacker could read your binary but it does do something,2012-04-19 20:14:15.11 UTC,311,10236016,i have an application written in c# using net 4.0 where the user will be storing their e-mail password using system.security.cryptography to the user configuration file    the actual password is stored in a securestring and encrypted using system.security.cryptography.protecteddata.unprotectencrypted data entropy currentuser  the password is only converted to a normal string when sending it soon to be over an https session   what i m wondering is given the entropy value need to stay the same or else you can t decrypt the password correctly what s the best way to keep prying eyes from finding the entropy value but insure that the entropy value will be constant,0.02572347266881029,8,so,.net|c#|cryptography|entropy,how do you secure your entropy value,3,attacks|protection|hard coded,0.3292599320411682
6348,if you re storing  only the hash itself  as base64 this shouldn t materially alter the one-way nature of the hash or its protection of the original plaintext  as long as no other pre-hashing plaintext data or related metadata is included that would in any way hint at potential plaintext then a leak of base64-encoded hashes wouldn t be any easier to attack than a leak of the original hashes  an attacker would simply analyze the data discover that it consists of base64 strings convert them back to their original hashes form and then carry out the attack this won t slow most attackers down but it also won t make things any easier for them,2017-10-18 04:50:50,181,171569,scrypt produces a binary hash of a password instead of storing it as bytes directly we are considering to store the base64 encoding of this string  would that make the password hash more vulnerable in case of a leak? the password salt is generated on a per user basis,0.055248618784530384,10,sse,databases|encoding|passwords|salt,does storing base64 encoding of scrypt password hash make it more vulnerable,4,leak|attacks|protection|vulnerability,0.32838910818099976
8574,as the commit message said it is due to avoid   algorithmic complexity attacks       an  algorithmic complexity attack  is a form of computer attack that  exploits known cases in which an algorithm used in a piece of software  will exhibit worst case behavior this type of attack can be used to  achieve a  denial-of-service    by using   the hash result will be randomized every time you start a new ruby execution context otherwise if is not randomized the attacker know the algorithm and could find out the worst case behavior which could used as the   dos   attack,2014-04-28 02:45:59.403 UTC,182,23331725,i just noticed that the return value of   changes each time i start up ruby   i looked at the mri source to see why this was happening   it turns out   is defined in     and though i can t find what   is i assume that it s not a deterministic function  after a bit of messing around i managed to find  this commit  by tanaka akira that changes   to use   due to avoid algorithmic complexity attacks what does that mean?  thanks,0.054945054945054944,10,so,ruby,why are ruby #hash methods randomized,3,attacks|exploit|denial of service,0.32814913988113403
37641,the problem is resolved now instead of using struct.unpack to read multiple data files of varying length numpy.fromfile is found faster and there is no more issue of multiple files with varying data length so numpy.fromfile is recommended to read multiple files with varying data length i have tested for around 2gb >250 files.sabih,2014-10-26 13:26:43.553 UTC,164,26573485,i am reading multiple data files of varying length using struct.unpack it works fine for files around 200mb but gives memoryerror in struct.unpack when the size of files exceed 200mb a related discussion at  http://bugs.python.org/issue14596  mentioned that the struct.unpack works fine for same length of data files and it produces memory leaks if the data files are of different length i further tested multiple files of same size which accumulate upto 2gb and it works fine  so what is the solution for reading multiple files of varying length using struct.unpack?  thank you-sabih,0.018292682926829267,3,so,data-files|memory-leaks|struct,memoryerror due to the memory leak of struct.unpack when reading multiple files of varying size,1,memory leaks,0.3274647295475006
51914,may be my experience useful to you        i generated hex-code signatures using n-gram opcodes and using menonic and i used more than 2000 viruses to min signatures the min length was 16 byte and max size was 68 bytes also for signatures was created for both malware and benign the approach was  heuristic and data-mining    the length of benign signature was less than malware and i though it was because benign are written in high-level language so compiler generated code and more similarity reduce the length of benign signatures where as malware are written in comparatively low level assembly or in inline-assembly embedded-assembly within high-level language so produces lengthy signature comparably         also long signatures are useful in detail analysis in offline scanning,2012-04-15 15:12:47.763 UTC,181,10163253,i haven t found information about this anywhere is there a minimal required length for virus signatures? i ve read in book by peter szor that for 16-bit applications 16 bytes is enough even to avoid false positives is there equvilent minimum for 32 and 64-bit applications too?  thanks,0.06077348066298342,11,so,antivirus|malware|malware-detection|virus|virus-scanning,minimum length of virus signatures,2,virus|malware,0.3271700143814087
45806,the primary change between 64 bit and 32 bit is the width of the address field which has increased to 8 bytes 64 bits from 4 bytes32 bits so evidently more the number of references/objects in your application more is the memory consumption in 64 bit read more about 32 bit vs 64 bits  here   having said that it s bit suspicious that it consumes 3 times more memory in 64 bit version i would get a proper memory profiler and test if your application has any memory leaks if there are no memory leaks there is nothing much to be worry about memory consumption differences in 32 bit and 64 bit,2015-08-18 11:41:43.12 UTC,225,32071440,"in this vast world of development i m still a newbie and keep learning stuff,recently whilst monitoring a service i noticed an abrupt behavior  i have a simulation tool which generates load on my application if i deploy my application as 32-bit application then the application hardly consumes 300mb on full load however if it deploy the console application as a 64-bit application it starts consuming resources hungrily and at same load level as 32-bit application it consumes minimum of 900mb in memory  can anyone explain what is happening with the application? any kind of help will be appreciated",0.013333333333333334,3,so,.net|64-bit|console-application|memory-leaks|memory-management,64 bit application taking more memory on same load,1,memory leaks,0.32605046033859253
29623,dpapi is meant to secure data-at-rest not data for transmission  ryan dobbs is correct above or below? i can t figure out how stackoverflow sorts unaccepted answers... weakening your encryption to attain a smaller payload is a very bad idea the right way to address this is to secure the connection tls-style ssl then you can just send things plaintext or as ryan suggests drop a properly-encrypted payload somewhere that both sender and receiver can access it   but to answer your question more directly the payload size is controlled by the hashing function encryption key size only tells you the cryptographic complexity of the encryption algorithm -- how hard the encryption is to break the part that says   is a sha-256 hash which means it produces a 256-bit output    is 128-bit but it s generally insecure only good for checksums  the documentation says the key size and hash size must be equivalent so you can t go to 128 bits with sha the shortest sha available is the old sha1 algorithm   which is 160 bits but the expectation is that anything less than 256-bits will be insecure relatively soon the sha2 algorithm yields   and,2017-09-26 18:36:47.24 UTC,347,46433553,i m quite new to .net and had a question regarding dataprotector  when using dataprotector.protect without any configuration the resulting encryption becomes too long for the api i need to pass it to i was wondering if using the configuration methods as seen  here  would help? i tried the following in the class where i needed to protect the data   however even after changing the encryptionalgorithmkeysize from the default 256 to the minimum 128  protect  was still resulting in an encryption of the same length which makes me think that the configuration isn t working or configuration doesn t affect encryption length  does anyone know if this is being done the right way or if there is a better way to reduce encryption length?  for example a simple 9 character string gets encrypted to 134 characters  any help is much appreciated thanks,0.023054755043227664,8,so,.net|asp.net-mvc|c#|data-protection|encryption,shortening dataprotection encryption length in .net,2,weakness|data protection,0.32602113485336304
56472,matplotlib generates a diagram numpy does not add   to your first code to see where the work goes   you may want to try with a smaller number for   first to see something quickly   edit   does not really make sense to create the same plot 100 times,2013-05-31 04:54:05.183 UTC,113,16849996,running the following code will result in memory usage rapidly creeping up    however when substituting the call to pylab with a direct call to the numpy histogram method then memory usage is constant it also runs significantly faster   i was under the impression that pylab is using the numpy histogram function there must be a bug somewhere.,0.017699115044247787,2,so,histogram|matplotlib|memory-leaks|numpy|python,memory leak in matplotlib histogram,1,memory leaks,0.32564258575439453
3324,depend on the attack you want to generateattack type the capac of your singl host cpu memori and on the network capac bandwidth and also on the applic that is receiv the request in the case is a l7 attack and probabl more variabl to take into consider,2020-02-18 12:49:48,101,226065,i am work on a task where i have to evalu how much throughput one can gener in packet per second i know i can send them as fast as possibl with but is there a way to calcul their number reliabl for a singl host,0.0594059405940594,6,sse,ddos|denial-of-service,calcul pps for ddo evalu,3,ddos|attacks|denial of service,0.32539433240890503
493,they want to alert you to the fact that math.random is not a true random generator but a  prng  if you need this to be safe you need a  csprng    here is the spec       using pseudorandom number generators prngs is security-sensitive       when software generates predictable values in a context requiring unpredictability it may be possible for an attacker to guess the next value that will be generated and use this guess to impersonate another user or access sensitive information      as the math.random function relies on a weak pseudorandom number generator this function should not be used for security-critical applications or for protecting sensitive data in such context a cryptographically strong pseudorandom number generator csprng should be used instead      ask yourself whether         the code using the generated value requires it to be unpredictable it is the case for all encryption mechanisms or when a secret value such as a password is hashed    the function you use generates a value which can be predicted pseudo-random    the generated value is used multiple times    an attacker can access the generated value         you are at risk if you answered yes to the first question and any of the following ones    code example,2020-05-27 06:09:56,250,62036514,when i declared a variable like   i am getting this error in sonar     make sure that using this pseudorandom number generator is safe here   how should i address this? what is wrong with my code?   can anyone help me,0.024,6,so,ecmascript-7|javascript|sonarqube,make sure that using this pseudorandom number generator is safe here,5,attacks|protection|sensitive data|sensitive information|weak pseudorandom number,0.32506123185157776
9963,as discussed in great detail over on  security stackexchange  there are a number of schools of thought  it definitely seems a good idea not to  lock  and require manual intervention as that just leads to denial of service attacks and a need to hire more helpdesk staff  the incremental delay seems to be valuable - it will break a brute force attempt but still allow a regular user a few mistakes in typing without ruining his day for example 5 seconds 10 seconds 15 seconds 30 seconds 1 minute 2 minutes 3 minutes 5 minutes 10 minutes still allows someone to get in without having to call for a password reset and is pretty much immune to any normal brute force attack  i would choose a low number like 3 or 5 and then prompt all login attempts for that  ip address and user account  with a captcha when the answer a correct catpcha they get another couple of guesses it is pretty easy for an attacker to obtain a large list of proxy servers brute forcing tools like  thc-hydra  make use of proxies  things to keep in mind    if you lock user accounts an attacker can use this to lock out the administrator    an attacker may have a list of user names with some application its easy to enumerate all users by iterating over the user s table primary key an example would be the user s profile page.    there are  very common passwords     one method that i ve heard of it but not implemented was to increase the wait time between each login and double it  so after the first failed attempt make the user wait 1 second then after that 2 seconds then 4 seconds and so on this way it won t lock a user out after failed attempts but will stop brute force attempts since it ll take 2^x where x is the number of failed attempts seconds per attempt  this is probably something that should be configurable  regardless of what values you select as defaults a particular attack at some future time may make different values more desirable at least temporarily  i would allow 5 login attempts per ip address and then block login attempts from that ip for 15 minutes  that would make bruteforcing very diffcult because changing ip is time expensive do not rely on cookies because they can be easily manipulated by users or hackers. i once tried to bruteforce some systems all on my own website of course and i m pretty sure that the way mentioned above is the most secure way  btw isn t it very difficult to wrong-type a password 5 times?  and yes locking the account means not to allow a login even with a good password,2011-04-18 11:36:31.267 UTC,527,5702230,in order to protect against brute force attacks on the login system how many failed login attempts should you allow - and in what period of time?  what does it mean to lock the account - simply not allowing them to log in even with a good password,0.017077798861480076,9,so,brute-force|security|web-applications,how many failed login attempts should you allow - and in what period of time,3,attacks|protection|denial of service,0.32505565881729126
20416,"the hmac-md5 of data and key is define as   where pad1 and pad2 are two fixed constants some of the more obvious things you might do are cryptographically weak   is fatally flawed consider how md5 works it splits the input up into blocks of a certain size 512 bits for md5 and sets up some initial state h0 h1 h2 h3 it then does a bunch of transformations to mix the first block of data with that initial state to produce new values of h0 h1 h2 h3 the second block of data is then combined with those to produce a new set of h0,h1,h2,h3 and so on the final value of the hash function is just the concatenation of h0,h1,h2,h3  this means that if you give me   then i can calculate what   is without knowing key at all you can do the same thing with sha1  if instead you did   then any known collisions in md5 are easily used to make messages with the same hmac value   apparently   may be flawed too even if two different keys were used hmac was designed to mitigate all of these attacks",2012-07-06 07:26:44.277 UTC,224,11357681,openssl::hmac#hexdigest using md5 vs digest::md5#hexdigest what are the differences and/or advantages? couldn t you just use digest::md5#hexdigestkey + data or is that consider cryptographically weak,0.022321428571428572,5,so,md5|openssl|ruby,difference between openssl::hmac#hexdigest vs digest::md5#hexdigest in ruby,3,flaws|attacks|weakness,0.3240252733230591
61151,,2012-10-09 05:09:59.833 UTC,163,12793164,"i have made a gauge which is showing maximum intensity coming in camera for making gauge i have first put a dialer in background of a panel and then drawing needle on a picturebox which is on that panel and picturebox having transparent background  my code is as follows..    angle_array  is a array   having 4097 angle value using these value needle will rotate   readimagecamerabuffer  is a function with is returningmax,min,avg,max_minintensity values   intensity_values  is a structure of max min avg max_min values   my gauge is like this..     left side having min intensity 0 and right side having max intensity value 4096   now my problem is that when i am running my application it starts and after 2 3 seconds it hangs or can say stops working i think problem may be memory leak but i am not getting where is the problem   please help me to get rid of this problem  any help will be appreciated  thanks",0.012269938650306749,2,so,memory-leaks|visual-c++|visual-studio-2010,intensity meter is hanging,1,memory leaks,0.32365089654922485
19354,yes..  netty is not affected as we limit the header size already see [1] for more details  [1]  https://github.com/netty/netty/issues/141   looks like trustin has already considered this,2012-01-10 20:41:52.347 UTC,250,8810238,denial of service through hash table multi-collisions  http://www.nruns.com/_downloads/advisory28122011.pdf  i m hoping someone has a reason why netty s querystringdecoder isn t susceptible to this attack  from the referenced .pdf  == java ==java offers the hashmap and hashtable classes which use the string.hashcode hash function it is very similar to djbx33a instead of 33 it uses the multiplication constant 31 and instead of the start value 5381 it uses 0 thus it is also vulnerable to an equivalent substring attack when hashing a string java also caches the hash value in the hash attribute but only if the result is different from zero  thus the target value zero is particularly interesting for an attacker as it prevents caching and forces re-hashing  different web application parse the post data differently but the ones tested tomcat geronima jetty glassfish all put the post form data into either a hashtable or hashmap object the maximal post sizes also differ from server to server with 2 mb being the most common  a tomcat 6.0.32 server parses a 2 mb string of colliding keys in about 44 minutes of i7 cpu time so an attacker with about 6 kbit/s can keep one i7 core constantly busy if the attacker has a gigabit connection he can keep about 100.000 i7 cores busy,0.028,7,so,netty,has anyone considered netty s suceptibility to ocert-2011-003,3,attacks|vulnerability|denial of service,0.3224431276321411
55313,email     hello      we do not ever display the time in a local time zone every timestamp  on our site always uses utc time      regards stack overflow team,2016-07-18 17:50:50 UTC,85,38443028,since i have became a user on stack-overflow i m having issues with the time stamps on questions and comments the time stamp is not taking my local time zone i have looked around in the profile settings and searched for a solution but no luck.any idea ,0.023529411764705882,2,so,stack-overflow|stackexchange,how to set stackoverflow page to local time zone,1,stack overflow,0.32243356108665466
8769,just to merge the comments into an answer here  the crux of the matter is that   will happily accept a zero-byte as part of a string and will not treat it as whitespace thus will not stop reading further bytes into the string  redirect a file or use   or the like redirecting an input file is probably the preferred way here   the final command i used was   where the first set of 16 bytes up to and including the first   were a randomly generated string which when hashed produced the second set of 16 bytes with the required   ending the last 3 bytes there weren t copied anyway but i left them in to show the string and hash,2016-02-26 18:31:29.383 UTC,594,35659152,as part of a security cs course my class has been given the task of exploiting a vulnerability to beat a password check using a stack/buffer overflow the code with the vulnerability is as follows   i understand the classic stack-smashing principle i think and there is a clear overflow vulnerability here where the first 14 bytes of the   array can be overwritten by inputting a password longer than 15 characters when prompted however i don t understand how to leverage this to allow the   check to pass completing the challenge some of the things i have discovered/attempted    setting   to be the equivalent of   doesn t work as   gets hashed using   setting the two to be equal is impossible anyway as   will insert precisely one unique ascii   character into the 30 spaces available to it meaning the two arrays can never be equivalent   characters additionally to my knowledge cannot be inserted in the middle of a   string    overwriting the maximum number of bytes with   which will always append a   character means the last 3 bytes of   will always be   attempting to randomly generate a 16-character password that then hashes to something with these last 3 bytes/characters reasonably computationally easy given the use of md5 doesn t work however due to the use of   which will give a value of  29  instead of something convenient like 16 thanks to only finishing the length count upon hitting a   character in the call to   this means that instead of hashing the 16-character password to produce the expected output the call to   will hash 16 characters from   followed by 13 characters from   producing a different final hashed value    to get around this problem i believe one would have to find a 29-character string call it s where the first 16 characters of s hash to a string r comprised of the last 13 characters of s followed by   i m not sure how viable finding this through random md5 hash computation is but it don t fancy my chances      some notes mentioned in the explanations above      is limited to a 29 character string but will append an ascii   character allowing 30 characters total 16 from the   array 14 from the   array to be overwritten    ascii   characters cannot be input via   so the   in the call to   if the maximum password length is used will be 29     so the question here is how else could i go about doing this? am i missing something extremely obvious? is random generation a viable solution or even the only solution?   the solution has to use a buffer overflow otherwise i imagine i could do something like preload a   that always returns 0 and has to be executable as a shell script if that s of any relevance,0.016835016835016835,10,so,buffer-overflow|c|md5|stack-overflow|stack-smash,stack-based buffer overflow - challenge in c using scanf with limited input,5,exploit|stack smash|vulnerability|stack overflow|stack based buffer overflow,0.32229822874069214
2799,"aborting early is fine as it doesn t reveal timing for anything secret unless you consider the  length  to be a secret which doesn t seem plausible  as for your edit comparing password hashes in non-constant time is also fine the attacker has no efficient mechanism to choose password hash outputs such that he can iterate through all possibilities for a byte at one position while holding the preceding bytes constant so there is again no timing attack — not even to reveal the hashed password  if the data against which you are comparing is not a secret then it doesn t really matter what you do timing attacks are used to learn information to which you do not already have access if you already have access then the attack is unnecessary  if you are protecting a secret then it doesn t matter if it is a password or not you should handle it the same way with the same precautions in order to avoid timing side channels you ensure that the input length will be the same as the stored value s length by hashing them both and doing a constant time comparison  of course you should store the hashed value of the stored value in the database so that you aren t recomputing it every time see pseudocode below  pseudocode for constant time comparisons     aborting early during comparison   aborting early during the comparison is problematic as it leaks info about the comparison and can be used to learn the password directly   attacker tries secrets of length 1 one of these secrets will take a few milliseconds longer to return than the others since there is one 1-length secret whose first character matches the stored secret attacker learns the first character of the secret  attacker tries 2 all character secrets with first letter beginning with known correct value one of these secrets will be a few milliseconds faster since the second character compares correctly  attacker repeats separately attacking each character of the secret eventually the attacker learns the stored secret   this is quicker than a brute force attack because the attacker can attack each character separately instead of the whole secret at once   aborting early before comparing   aborting early as mentioned by steven tousset leaks the length of the secret this doesn t look like a lot at first but it lets the attacker speed up their attack by orders of magnitude let s say that the system accepts secrets of length 1-5  keeping it very simple the secret is composed only of digits thus there are 10+100+1,000+10,000+100,000=111,110 possible values a brute force attack needs to make about 100,000 tests to try every possible value   if the system lets the attacker know the length of the secret an attacker can dramatically reduce the number of values that s/he needs to test by first trying 5 values  1   12   123   1234  and  12345  now the attacker knows how long the secret is if it s 1 character s/he now only needs to make 10 tests instead of a hundred thousand if it s 2 characters long she only needs to make 100 tests without this key piece of information the attacker needs to make a lot more tests before they will find the secret   another issue recoverable storage   there s another issue too if the known secret can be compared against the input value then you have to store the secret in recoverable form search this site for details related to passwords but this is often considered a bad practice with storing passwords and should be avoided in general - with any kind of secret - if you don t absolutely have to have the raw value it should not be stored in recoverable form irrecoverable storage prevents several attacks - whether they matter for your secret depends upon how you use it   one more thing compiler optimization   the code below is pseudo code if you were to pass its equvalent through a compiler or a runtime that does optimization then you need to be very careful to ensure that the code does not get optimized there are various tricks fir different languages which are out of scope of this comment tha ks to @polynomial for reminding of this  problem you want to compare two sequences a user-provided sequence   and a secret sequence   whose length you want to keep secret you don t want to disclose through the elapsed time of comparison whether   is shorter than   or the length of the common prefix of   and    you can t abort the comparison after reaching the end of   because that will disclose the length of   nor can you abort after the first mismatch because that will disclose the length of the common prefix what you  can do  is repeat the last element in   until you reach the end of   for example if   is   and   is   you want to compare as if it were    the following untested code in python should illustrate the concept   this of course assumes that the comparison operator   takes constant time as does the indexing operator arranging for   to be placed at a certain alignment relative to cache lines or virtual memory pages could in theory keep the timing attack alive",2015-01-05 00:15:27,1057,77428,what is the preferred or recommended way to pursue contestant time compares when array lengths are not equal?  should we exit as early as possible? something like   or should we avoid the early exit and compare as much as possible in hopes of masking as much as possible? something like   or something else?  or is this question more appropriate for the folks on the crypto.se because its  theoretical minutia?    edit  this seems to be related to  timing attacks on password hashes  but the cited question is a particular instance of the generalized problem and i m interested in the generalized question  i m not sure i agree with the cited question s answer or assertion that  using timing attacks in this case will in no way tell an attacker more than what he would know if he had the actual stored hash and salt..  because the attacker could be trying to recover the hashed password though timing attacks given a hashed password its only a leap back to the password in some cases,0.030274361400189215,32,sse,cryptography|side-channel|timing-attack,constant time compares when array sizes are not equal,5,leak|attacks|protection|timing attack|bad practices,0.3219561278820038
2260,most obviously it s subject to replay  mallory can sit on the line and keep watching authentications r1 and recording hashk+r1 mod 2^64  any time an old r1 appears she can reuse the hash she saw earlier  similarly if r2 appears that was seen earlier as an r1 she has a 50% chance that hashk+r2 mod 2^63 will be the same i.e if k+r2 mod 2^64 has a most significant bit of 0  keyed hashes have known attacks for example hm|k hk|m hk|m|k all have some attacks that at least weakens them hmac is standard way to go that solves a lot of attacks which has the simplified form of h k| hk|m  the double hashing being key  you can search more about hmac and hash based message authentication but the point is that it is not that simple to come up with your own authentication algorithm unless you are a cryptographer that knows what they are doing  64 bit keys are also small it is advised to use bigger keys it is also not clear what is the hashing algorithm used not all are equal the r1 r2 i assume are random  apart from that this is a basic authentication technique so if you were to use hmac and bigger keys i wouldn t see any issue i am also not so much an expert to say how vulnerable it it would be interesting to share why you think it is vulnerable in the first place  the replay attack mentioned is valid though it depends how likely it is to get the same random shouldn t be likely if you use a good random generator,2014-07-14 20:41:34,325,63130,i know that the following authentication protocol is vulnerable but i can t understand why    my thinking is that the two hashes don t line up but i don t know that that would make this protocol have a major vulnerability,0.03076923076923077,10,sse,authentication|encryption,this protocol is vulnerable but why,3,attacks|weakness|vulnerability,0.32167622447013855
56394,the first answer on this page sums it up nicely performance of jquery visible   to quickly summarize  when using the jquery :visible property you are really checking more than one property  in regards to your speed checks it s possible the checks are adding up  i m sure these speeds will vary depending on browser  i don t believe this is a memory leak javascript isn t multithreaded and can only perform one function at a time the problem is that the process of   is taking longer than 0 seconds before your next timeout/setinterval is fired so it is queued up therefore causing a cumulative effect on your times,2014-05-02 14:14:24.303 UTC,279,23430255,i have a piece of code that is causing some sort of memory leak after many hours of testing i ve managed to track it down to    how i tracked it down -> my code loops on itself after a 1 second delay i set this delay to 0 and saved the average time taken to do 10 iterations i ended up with a list of integers as shown below  here is my  speed test  without any memoryleaks etc    i ve only included the first view results but you can see it takes an average of 4000ms to complete the loop ten times  next here are the values when i include     as you can see the time taken to do  x  amount of loops increases exponentially   here is the code   does anyone know of any know issues with this function? anyway to avoid it slowing down my function? i d be most interested to find out why it does this too  thanks,0.014336917562724014,4,so,debugging|javascript|jquery|memory-leaks,slowing javascript bug jquery.is:visible,1,memory leaks,0.32119041681289673
3202,"with enough requests made it is possible to carry out statistical analysis of the results check out  ircmaxell s blog post      it s been shown that you can remotely detect differences in time down to about 15 nanoseconds using a sample size of about 49,000   analysing a large number of requests will iron out any network or application latency issues  the problem is that the overhead generally averages out so if you check one string a million times and another string another million times then you would still be able to detect differences in the compare the higher the amount of tries possible the higher the chance that even minute changes can be detected sometimes it makes sense to limit the amount of tries e.g by introducing a time delay - if a higher latency is not an issue",2015-05-11 08:28:35,301,88954,related  are-there-any-working-proof-of-concept-string-comparison-timing-attacks   i was looking at doing some encryption and hashing in php and came across this note      please be careful when comparing hashes in certain cases information can be leaked by using a timing attack it takes advantage of the == operator only comparing until it finds a difference in the two strings   it seems to me that in most cases the difference in comparing two strings is only going to be a few clock cycles given the overhead in http php and changes in network latency there will be a lot of variation in the time taken to process a request i don t see that using such an attack over the public internet   has such an attack ever been demonstrated in a real-world situation?  i m not saying that you shouldn t protect against it just in case i am just wondering how big the risk actually is,0.026578073089700997,8,sse,internet|timing-attack,are string comparson timing attacks practical over public networks,4,leak|attacks|protection|timing attack,0.32053428888320923
23370,i suggest using a hash function that has been specifically designed for use as password hash such as  bcrypt  as opposed to standard hashes like sha256 this makes it much harder to crack the passwords in case the hashes leak  passwords  must never  be encrypted  this is a clear violation of  cwe-257   passwords must always be hashed and  sha256  is a very good choice   sha256 is extremely powerful and very secure because it is public and there for heavily audited  the use of a private algorithm is shunned in secuirty as it is called in security though obscurity   re is website security just based on the complexity of algorithms?  no there are many thing you need to do to make a website secure  some things you must prevent are   sql injection  code injection  directory traversal  cross site scripting  flash parameter injection  session hijacking   password brute forcing  cross site request forgery   man in the middle    and probably many more..  your question seems to imply that the encrypted password is somehow accessible from the network  that should not happen - either use   or equivalent to restrict access to that file or simply store it outside your document root that way a potential attacker won t have anything to attempt to decrypt even if they do know the encryption algorithm  that said there is a large number of  crytpographic hashing algorithms  that will encrypt your password to a form that is highly improbable but not impossible to be reversed even if an attacker does acquire your password file/text  the wikipedia article above has a  nice list  with various algorithms and the current estimates about the difficulty of reversing an encrypted password for each case  most modern environments already have support for the most potent of these algorithms for php   http://php.net/manual/en/function.sha1.php    http://php.net/manual/en/function.hash.php   the second one provides some variety to choose from..  edit  keep in mind that even the best hashing algorithm won t help very much if your chosen password is   or something similarly easy to guess using a brute force dictionary attack  quite often the weakest link of a system lies in the people that use it..  i would recommend you start by reading this article  http://phpsec.org/articles/2005/password-hashing.html   it tells you how to generate a secure hash using a salt if you read the whole article you should be able to understand how to improve the final function at the end  what you need is a hashing algorithm such as md5 or sha1 these are one-way meaning you can easily hash a string such as a password and compare the resulting hash to your stored hash you cannot however take the hash and convert it back to the password  see  http://gr.php.net/manual/en/function.sha1.php  for the sha1 function  md5 and sha1 are well known algorithms sha1 is the more secure as there are current mechanisms to facilitate easier bruteforcing of md5 hashes,2011-02-14 23:50:06.25 UTC,613,4998670,i m working on an application and i don t want anyone figuring out the algorithm for the admin username/password  i would like to clarify first of all   is website security just based on the complexity of algorithms?  what s the most secure method or maybe there s a gpl source code link you can recommend? would that not be feasible since the algorithms are accessible to anyone who can identify where i got the source code from?  i m  not  using mysql for my application i don t need to   what s your suggestion for how i get as secure as possible without spending money time is a luxury i  do  have just php and minimal javascript,0.021207177814029365,13,so,authentication|cryptography|passwords|php|security,what s the most secure way to encrypt a password using php without a database,11,cwe|leak|hijack|attacks|weakness|sql injection|code injection|man in the middle|directory traversal|cross site scripting|cross site request forgery,0.31911563873291016
20620,the problem with strcmp is that it depends on implementation if it binarily compares each byte of strings until it reaches difference or end of either strings then it is vulnerable to timing attack    now how about hashing?   i have found this security question and i belive it has the correct answer for you  https://security.stackexchange.com/a/46215   timing attack is a myth  i explain  the time that it takes to validates a text between one similar versus other different is around a fraction of second let s say +/- 0.1 second exaggerated!  however the time that it takes an attacker to measure this time is  delay of the network + 0.1 seconds + delay of the system may be its busy doing some other task + other delays  so no its not possible even for a local system lag zero the result of interval of time is always unclear  binary safe means that any bytes can be safely compared with   not just valid characters in some character set  a quick test confirms that   is not safe against timing attacks   for me this prints something like    many other string-based functions in php likely suffer from similar issues  working around this is tricky especially in php where you don t actually know what a lot of functions are really doing at the physical level  one possible tactic is just sticking a random wait time in your code before you return the answer to the caller/potential attacker  even better measure how long it took to check the input data e.g with   and then wait a random time minus that amount of time  this is not 100% secure but it makes attacking the system much harder because at a minimum an attacker will have to try each input many times in order to filter out the randomness,2014-09-15 09:02:46.31 UTC,409,25844354,strcmp  - what is means binary safe string comparison? this compare is safe for the timing attack?  if no how can i compare two strings for preventing the timing attack? compare hashes of the strings is enough? or i must use some library or own code that gives constant time for the compare?   here  writes that the timing attack can be used in the web but can be this type of an attack exists in the real world? or this attack can be used only for a small type of an attacker like government so this protection through the web is excess,0.039119804400977995,16,so,php|security,timing attack in php,3,attacks|protection|vulnerability,0.3190719187259674
6325,you are likely experiencing a knockknock attack as described here: http://securityaffairs.co/wordpress/63969/hacking/knockknock-attacks-0365.html   it affects many companies so i would not be surprised   as long as you have secure passwords the risk is pretty low with secure passwords even 1 million attempts should not come close to cracking it   you can block the ips in question or report abuse but it s unlikely to stop it completely because the ips will change it s also not too much to worry about or if the ips are owned by the attacker and not some hacked vps you can launch a ddos attack by using servers with ip spoofing enabled and using  udp amplification >:  you can test the password complexity of your environment by running mimikatz on a domain controller to dump all the password hashes and then perform a brute force attack with hashcat using wordlists and rules   i like to determine the threat of dictionary attacks by executing them against myself using the  popular dictionary files  downloaded at skullsec  rockyou.txt is a decent one and appending your local sports teams to it is an excellent method of discerning your dictionary hack risk  but at the pace of attack you re experiencing now.. you can be less concerned until they start executing additional complex layered attacks which could indicate a motivated intruder specifically after your sites  just block them by changing time interval in your jail rule   if an attacker fails 5 times in a day he will be blocked for 3 hours or you can increase that number according to your wish  prevent as much as possible before it will be to late     a back-of-the-envelope calculation shows that if the attacker checks one password every 40 minutes they ll exhaust a 10 000-word dictionary in a little less than a year not that my passwords are dictionary words mind you   if your passwords aren t dictionary words then calculating how long it ll take to go through a dictionary isn t a very useful calculation is it?  calculate out how long it d take to  brute force  your password  given that information make sure to rotate your passwords far before they ll be discovered  it s good practice to do this regularly anyways because passwords can leak in all sorts of unexpected places  since you ve noticed this attack feel free to adjust your fail2ban rules to block it more if you aren t worried about the legitimate user impact of tightening them  then you can take the new speed into consideration when doing your time until compromised calculation,2017-10-13 18:12:34,544,171238,i have installed fail2ban on my mail server and the logs show 4-5 ips regularly hitting my server at large intervals so not often enough to trigger the fail2ban rules   the ips are all vps es belonging to digital ocean ovh and the like a back-of-the-envelope calculation shows that if the attacker checks one password every 40 minutes they ll exhaust a 10 000-word dictionary in a little less than a year not that my passwords are dictionary words mind you i guess it can pay off if the attacker is hitting thousands of servers simultaneously  should i be concerned about this type of attacks,0.034926470588235295,19,sse,attack-prevention,should i be concerned about a very slow dictionary attack,5,ddos|leak|attacks|ip spoofing|attack prevention,0.31903067231178284
43149,did see the same issue revert back to tomcat switched to g1 it helped a bit but it slowed the transaction response time at some intervals of gc,2018-06-07 08:45:27.967 UTC,157,50736798,i m troubleshooting my app memory leak overtime i start my app and leave it alone for a couple of days without sending a single request  app is a simple one it has some cfx-endpoints raised and camel-routes listening to them   when i start my app heap grows rapidly and reaches maximum at about 300 mb       in a couple of days the heap average size is about 500 mb with max about 750 mb  here is profiling result       i ve looked at the stacktrace for the heavy surviving objects       all of them lead to tcptransport.connectionhandler or it s co-package classes  used library versions and pom  java 8 1.8.0_172   here s cfx-enpoint config   does anyone have an idea where memory is leaking,0.025477707006369428,4,so,apache-camel|java|memory|memory-leaks|performance,tcptransportconnection memory leak,1,memory leaks,0.3178060054779053
5310,limit the amount of attempts that can be made within the 30 seconds  so if you have a token that is valid for 30 seconds and is 6 digits for example and you limit the amount of attempts that can be made in 30 seconds to say 5 again as an example then the chances of success of bruting forcing the second factor are 0.0005000005000005%  rfc 4226 doesn t tell me what others are doing but it does have some viable suggestions in its section 7.3  apparently after the 5 attempts per account and device are exhausted either a full account lockout or exponentially increasing delays should kick in semi-permanently  the counter must apply across login sessions and across individual totp tokens as long as they are associated with the same authentication device  that way it is impossible for too many unsuccessful device factor authentication attempts to accumulate before the constant need for the administrator to reset the device factor lockout hopefully draws appropriate attention to the account under attack and once the password is eventually changed the lockouts caused by the attack will stop  lessons taken    the protection of the device factor is no different from theprotection of the password and the two need independent protectionsfrom brute force attacks regardless of the authentication flow    if the device factor is adequately protected in itself it would be redundant to speculate whether the password has leaked or whether the legitimate user is just having a modest amount of trouble with their devices   to protect the device factor it is not sufficient to protect only keys against leaking and tokens against eavesdropping or guessing,2016-12-17 15:25:45,469,145604,assume a 2fa system with user-supplied passwords and 6 digit totp tokens  it is not possible to test a totp token without authenticating with a password first so whoever submits a token is presumed to know the password  each token is generated for 30 seconds and is also valid within the preceding 30 seconds and the next window of 30 seconds alongside the then current token  it s quite common to see false alarms because the token took a little too long to enter so tokens can t be much longer to be practical   should a certain number of wrong token inputs in a row result in forced password invalidation based on the assumption that the password may have leaked?   it might not have leaked  there might be just some problem with the second factor that will eventually go away.  if current widely used 2fa systems do this what are the typical thresholds?  i am aware of rfc 4226 suggesting a throttling parameter of 5  i am not sure what exactly happens after the threshold is reached  account lockout?  password invalidated?  wait a little and allow retrying,0.029850746268656716,14,sse,account-lockout|multi-factor|totp,best practices for handling wrong totp tokens,4,leak|attacks|protection|eavesdropping,0.3173134922981262
37211,you should allow any characters in the ascii character set although  you could allow unicode  if you want to take on slightly more complexity e.g when using password strength meters you should set a maximum length of 72 ascii characters if using bcrypt as the php implementation restricts the hashing to these characters only     using the password_bcrypt for the algo parameter will result in the password parameter being truncated to a maximum length of 72 characters   this is good as some implementations impose greater restrictions on length 55 characters   with unicode this is more complicated too  as some characters may take up to 4 bytes my recommendation stick to ascii passwords can be secure enough >= 64 bits of entropy as it makes things simpler and if your users are using password managers which they should most of these only support the ascii character set  xss is only a problem if data is output to a page ideally your system should implement a policy that passwords are only ever input never output even in   fields this will enhance security of passwords long term  first the password strength this is important to prevent password guessing or brute force attack the stronger a password is — the better of course you can limit it to minimum 10 characters for example and require digits and upper case characters in it that s quite strong already but you can also require the use of symbols like         etc that password would be very secure  as for storing the password securely — this is a different topic — and you should  never  store or show a password in open form always hash them with for example   and store the resulting string in the database this is considered top secure today    to check if an entered password is correct all you need to do is use the function   to compare the entered password with the stored hashed one      will become   or   that s all you need to know if passwords match or not  so good to have this   and   today people used so many stupid and wrong ways to hash or encrypt passwords before this helpful functions were added,2015-01-19 16:07:57.233 UTC,564,28028863,with a multi-user site how strict should i verify my users  passwords?  i ve searched around a little bit and i ve only found questions about  how  to verify user passwords but i know  how  to create salted hashes for passwords to store in a database php s     and i know  how  to verify those passwords phps      i m asking what types of restrictions i should put on user passwords on my site because passwords are never shown on a webpage should i put strict restrictions on them like the usernames?  i know that usernames are shown and are vulnerable to  xss attacks  which i ve prevented against for usernames but passwords are different and can t be hacked this way i think this is how it works correct me if i m wrong  i know that passwords are often personal and very unique i don t know if i should restrict special characters that could make a password more secure including special characters similar to those in an xss attack  i m asking if any password should be allowed within certain character limits if not then what is a recommended secure method,0.014184397163120567,8,so,passwords|php|security|xss,how strict to verify passwords,3,attacks|vulnerability|cross site scripting,0.3166390657424927
65623,the   contains pre-generated  group parameters  for  diffie-hellman  a dh key exchange occurs in the early steps of the ssh connection and generates the secret shared key which will be used for encrypting the data dh works in a given  group  technically a modulus  p  a big prime integer and a generator  g  the generator is in the  1..p-1  range and it should be such that its order is a multiple of a big enough prime  q  the order  k  is the smallest non-zero integer such that  p k  = 1  mod  p   discrete logarithm  and therefore dh can be broken through with an effort which depends on both  p  and the largest prime factor of  k  which we will call  q     there is a variant of the  number field sieve  which breaks discrete logarithm if  p  is not big enough current record is for a  530-bit prime  and it is assumed that 1024-bit primes are still safe [ ed not anymore see various blog posts about diffie-hellman dated 2015-10-15 ] and 2048-bit primes will be safe for the foreseeable future    generic discrete logarithm algorithms apply with cost proportional to the square root of the biggest prime factor of  k  so it is important and sufficient that  k  admits a prime factor of at least 200 bits if 100-bit security is to be achieved    an attacker who can modify   could try to replace the nice dh group parameters with other group parameters who  look fine  but make dh-breaking easier or even easy for him there are mostly two ways to do that    choose a prime  p  and a generator  g  such that the order  k  of  g  is a product of only small prime integers this can easily be done for instance begin by generating a bunch of small primes say 80 bits each at most then try random products of such primes for each product  r  compute  p = 2r+1  and see if that yields a prime if it does then  p  is such that  any  generator  g  modulo that prime will imply weak diffie-hellman    generate a special prime  p  such that nfs is faster for that specific  p  see  this article  for some theoretical background    the   file does not contain the order of the suggested generator so you cannot simply verify if that order is prime and matches  g  however you could take  p-1  and apply the  elliptic-curve factorization method  on it this will find all the small prime factors in  p-1  and thus allow you to check that the order of  g  has not been weakened  k  is necessarily a divisor or  p-1  thus weakening of the first kind can be detected albeit not in a matter of a few seconds correspondingly attackers who do not like risk of exposure in particular spy agencies will refrain from using that method  the second kind of weakening however is hard to detect so one has to assume that  an attacker who could somehow corrupt your   file  e.g by altering the file as distributed with  openssh  through some bribery of openssh developers   could  potentially have introduced a backdoor allowing him to decrypt your ssh connections at will  ssh connections to your server since the group parameters are chosen by the server since the involved mathematics are not simple that attacker would at least be a  competent  attacker  the only good  countermeasure  is to replace the   contents with values which you  know  to be correct and not malicious by virtue of having generated them yourselves   can do that see the  man page  you could also do that with some custom code e.g with java s   class this is a matter of 50 lines of code at most  alternatively use dsa group parameters they also work which were generated as described in annex a of  fips 186-4  these are  nothing-up-my-sleeve numbers  because the candidate values for  p  are obtained through a deterministic fully-specified prng which presumably cannot be coerced into making a special prime which makes nfs easy it would have been better if openssh developers had used such a verifiable generation method to begin with,2013-09-06 15:38:44,703,41941,what are the consequences if an attacker is able to modify the   file,0.017069701280227598,12,sse,backdoor|cryptography|ssh,consequences of tampered /etc/ssh/moduli,3,attacks|backdoor|weakness,0.31501632928848267
15832,the quick answer is  no   don t use a non cryptographically-secure random generator for initialization vectors  the initialization vectors are sent unencrypted so one could think they can be generated by one of these functions  but this should lead to weakness and i ll explain  if you use a poor and   is such a weak random routine you ll narrow the space of possible ivs you are generating  an example will suffice  let s suppose you are using a 8 bytes iv in some encrypted message  but the random function you use has a 8 bit seed there are only 256 possible byte sequences to be generated from such a poor random function so probably there will be only a maximum of 256 possible ivs generated even if they show pure randomness or a wide spread hash values over the whole space  an attacker knowing this can reproduce the whole 256 space of possible iv making them completely useless,2017-07-16 15:33:40.09 UTC,241,45130330,cbc mode for aes specifies that to make each message unique an iv should be used the iv should be random and only used once otherwise it may allow people to decrypt other cipher texts which used the same key  is the   function in c suitable for generating this iv? does the fact that it is normally seeded with the current time make it vulnerable to some sort of attack,0.02074688796680498,5,so,aes|c|cbc-mode|initialization-vector|random,should rand be used for an iv,3,attacks|weakness|vulnerability,0.31491854786872864
42449,,2019-08-26 00:42:54.067 UTC,43,57650734,i have to measure stack memory sizemax limit in c#  so i want to ask a question about measuring stack memory limit size  ===  this is assignment so measuring present stack memory is important,0.023255813953488372,1,so,c#|stack-overflow,how measure the detail stack memory limit,1,stack overflow,0.3147059977054596
22838,yes you can use the default key and iv values if you like you can also explicitly regenerate a new random one with   symmetricalgorithm.generatekey  or  symmetricalgorithm.createencryptornull null   it depends on what you are protecting and how many owners of information you need to support if speed / volume doesn t matter then you are still better adopting pbkdf2 by using  rfc2898derivebytes  for the iterations  regardless you don t want to share the key across multiple users / tenants / security realms however but sure you can use the default key for a single application if you do combine the salt with it  the reasons we use user defined passwords and salts are to avoid attacks that exploit common/weak passwords or shared passwords between users and to ensure as application owners don t know their keys  the reasons we use pbkdf2 derivation with many iterations is to slow down the attacker penalty we pay 1 time per user is paid many times by an attacker  if your needs are just to have a random key for a single application or system then the default is usable assuming of course it provides the strength you need,2017-09-08 01:10:08.64 UTC,351,46107172,when i instantiate aesmanaged in c# it already has the .key property set is it safe to use it? i.e is it cryptographically strong and random enough for each new instantiation of aesmanaged and every time i call .generatekey on an existing instance?  all examples i ve seen first generate a random password and then use a key derivation function like rfc2898derivebytes or passwordderivebytes to generate the key e.g  how to use &#39;system.security.cryptography.aesmanaged&#39 to encrypt a byte[]?  this requires additional information - like salt value number of password iterations what hash algorithm to use  i understand i need all that if i want my users to come up with passwords i then need to produce random cryptographically strong keys from them but if everything is generated by the computer do i need to programmatically generate random passwords and then keys from them or can i just use whatever aesmanaged.key contains,0.017094017094017096,6,so,aes,setting key of system.security.cryptography.aesmanaged in c,4,attacks|exploit|protection|weak password,0.31458088755607605
15031,for diggesting messages in java you could use  messagedigest  according to its javadoc it supports md5 sha-1 and sha-256  an md5 hash is a digest of a file s contents  if you have a blacklist of md5 hashes yes you could compare a file against them  i think this would represent a pretty simplistic and fragile way to scan for malware but it s definitely a start  this would be fragile since comparing md5 hashes would only recognize when two files are precisely identical  any sort of randomness introduced into a malicious file would render this method of scanning useless  most languages have some standard way to generate an md5 hash  c# vb and vc++ can use the  md5  class php has the  hash  function using md5 as the first argument in java you have  messagedigest  etc  there are a number of other available  hashing algorithms  that could be used for this purpose  md5 has been shown to be lacking for certain applications and as such the sha algorithms are becoming very standard for those applications  for this application where you would not be expected a malicious attack would attempt to create a match but rather the opposite of trying to prevent a match any hash standard algorithm including md5 should be adequate to be reasonably assured that no false positives would be seen,2012-10-25 14:44:12.913 UTC,285,13071139,i m writing malware detection based on signature scanning as i understood the main idea is to compare signature of scanned file with signatures in your blacklist  here  i found that signature is some kind of md5 hash but how can i get it from file? and are there any other types of signatures,0.021052631578947368,6,so,java|malware|security|signature,signature malware detection,3,malware|attacks|malicious file,0.31362035870552063
64515,,2019-10-08 17:34:12,46,58291362,i have a program use pyside and django s orm to combine postgresql.when running the program for a few hours  the system monitorhtop had serval idle postgres pyside pysde [local] idle   have anyone give the suggestions,0.021739130434782608,1,so,memory-leaks|postgresql|pyside,pyside issue - pyside pyside [local] idle,1,memory leaks,0.3135060667991638
27754,indicates that you have made a lot of requests in a given period of time for protection against  ddos attacks  and  bots  so it won t allow you to send a lot of requests in a small period of time  to work around that you can explore the following  1 incorporate and rotate      2 use and rotate proxies,2019-09-20 10:15:00,100,58026357,i m trying to perform a google search in python   here is the code     at first it worked well but after a few more times this error pops up and closes my terminal instantly,0.03,3,so,python|search,performing google search using python code,3,ddos|attacks|protection,0.31323277950286865
67128,and to accomplish the goal you have to find the key value   do you  really  need to recover the key to accomplish the goal?  the scheme you have described is vulnerable to the classic  length extension attack  where a hash function is misused as a message authentication code the attack is described very well in the wikepedia article so i shall avoid replicating the example here  with the length extension attack you can generate a valid signature without knowing the actual key value which is presumably enough to solve the challenge,2015-09-21 10:05:07,266,100757,i am trying to solve a security challenge on a website  basically the website computes   to sign a cookie so that the user can t change it himself and to accomplish the goal you have to find the   value  i have two not really good ideas to find the secret key value     1 online  way too slow try all combinations of the authentication hash with an empty 0 bytes cookie this way the website will compute   only and will stop displaying an error when i find the correct secret key value   2 offline  still very slow but better since you don t need any network requests go on the website and get {cookie cookie_hash} try to hash   for every possible value of    when   i know that xxx == secret key   the thing is as you noticed that even the second method is brute forcing and therefore very slow  do you think there is a faster method,0.022556390977443608,6,sse,attacks|design-flaw|hash|salt|sha,challenge how to find the secret key in homebrew mac/keyed hash sha1key + cookie,3,flaws|attacks|vulnerability,0.3131190836429596
32564,,2019-07-26 10:51:19.1 UTC,78,57218552,gearmand version 1.1.18 latest works very slow when free ram runs out i have 64 gb ram on server and when gearman use all free memory and goes to 1mb of swap - gearmand starts to work very-very slowly adding any task to background que can take up to 10 seconds.removing task from que can take up to 30 seconds.que more than 10 000 000 jobs,0.01282051282051282,1,so,gearman|memory|memory-leaks|out-of-memory,gearman works very slow with out of memory,1,memory leaks,0.31254813075065613
67131,yes cache misses can be used in timing attacks  place an octet string on a page boundary validate from left to right if a cache miss occurs than everything up to the cache miss is correct because otherwise the verification would have stopped hence always test all characters of a password / pin etc. a cache miss is more likely to be detected than simply counting the bytes that are validated as a cache miss will take many more cpu cycles hundreds instead of just a few,2015-09-29 19:58:07,124,101453,what are the possible consequences of a cache miss? in other word is it possible that a cache miss could cause a security threat?  thank you,0.024193548387096774,3,sse,attacks|caching|cryptography|threats|timing-attack,what threats involve in cache miss,2,attacks|timing attack,0.3120727837085724
26639,it should work in some way and protect from simple attacks but you can improve it by requesting dynamic hash difficulty on different requests  for example you have 2 pages one displays data with specific record in db by using primary key second does difficult search and sometimes there is no necessary index in db to do search fast or page displays a lot of results attacker would like to ddos second page and make the search as slow as possible  you can setup low difficulty for first page and high for second the main idea can be if you want to take   cpu time on server spend   of your cpu time  if you request hash only once attacker can calculate necessary nonce and do million of requests by using single cookie  since a ddos operates from multiple systems on multiple connections it is easy to counteract if you just write a script that will request a new sha256 hash from the server for each individual connection will clog your server they do not need to reverse engineer but just generate  session tokens  for each individual connection requesting access to the server,2018-05-29 21:12:18.423 UTC,814,50593028,i m thinking whether the following way is a good way to completely and totally prevent ddos on my server my idea is to use the same mechanism of cryptocurrency mining bitcoin with sha256 or any other hash to prevent ddos  note i m not suggesting to mine cryptocurrency per se i m suggesting to use the same mechanism to avoid  sybil attacks    why does idea look appealing to me?  because creating a mined hash is expensive but verifying it is super-cheap it costs only calculating a hash once   what does mining mean in a nutshell?  it means that there s a specific chunk of data say a session id or a jwt token that can be stored in the server in a performant nosql server and the user or the miner in cryptocurrency has to create a hash that matches certain criteria for example if we use sha256 we can define the difficulty as the required number of the leading zeros in the 256-bit resulting number from the hash more zeros make the probability of finding that hash more difficult    how does it work?  a user would take the session token which is created by the server and combine it with a  nonce  number used once and calculate the hash in their frontend through wasm or otherwise with javascript since the user cannot reverse-engineer sha256 all he can do is keep changing the nonce again and again and again until he finds a nonce that creates a hash that satisfies the required difficulty       the graph shows the probability of finding a block finding the correct nonce in bitcoin where the difficulty is chosen to make it 10 minutes changing the difficulty will shift this curve and change its width proportionally   how long does it take the server to verify?  practically zero just calculate the hash once and ensure that it matches the given difficulty and that authorizes the user to make any anonymous request notice that none of this requires authentication with usernames and password this is all anonymous authenticated users don t need to do this as their credentials can be banned from the system this is all for anonymous users and possibly attackers   the result  the user will have to calculate this hash with this difficulty before making any request to the server once the user succeeds the mined authentication token can be stored in a cookie to be reused by the user if the user fails to provide the requested hash his connection request will be abruptly reject and thus preventing a ddos attack with sock-puppets     asic  resistance  using sha256 is not recommended because there s specialized hardware that can calculate it very fast leading to a possible coordinated attack there are hashes that are hard to print on hardware such as scrypt and argon2   choosing the difficulty  the difficulty can be static which i wouldn t recommend or can be dynamic to change with the load on the network when a high network load exists the required difficulty is increased this basically will act as a filter and protect the network during ddos attack times and never affect the users as users normally wouldn t care to wait 10 seconds to create a session in case of really high load the users can either choose to compute the expensive nonce or come back later the hosting company also can decide whether an expansion of the infrastructure is required based on the difficulty chart over time  is this a sound plan to protect against ddos on websocket and similar public protocols? i would like to implement this on my server,0.025798525798525797,21,so,ddos|hash|networking|security|websocket,use mining to prevent ddos attacks on websocket is this a viable solution,3,ddos|attacks|protection,0.31154870986938477
24661,"yes and no brute-force attack is very primitive type of attack  no one will try to brute-force a hashing algorithm with current performance borders  1,000 session guesses per second i believe is also far from reality any brute-force attack is limited by bandwidth and most firewalls will block such amount of requests per second  if you are concerned about session security you need to think about  secure data transmission  between client and server because most of attacks will try to steal session key instead of trying to brute-force it  wiki link   don t worry about session brute forcing as you stated it will take alot of time to brute force a session id you have to worry about session hijacking  see more information about session hijacking here: http://en.wikipedia.org/wiki/session_hijacking",2014-06-17 11:20:09.09 UTC,227,24262419,in the article  https://www.owasp.org/index.php/insufficient_session-id_length you can read about session guessing attacks  how long would an attacker need to find a valid/active session?      most php default settings use md5 hashing 128 bits.when i use 128 for b 1000 for a and 500 for s i get the result of   seconds or roughly   years  seems pretty ok i have never seen any session on any website having less than 128 bit  question  or is there something wrong with my assumptions?  are the default values  safe  ?  using this now paranoia mode   generating sessions ids like,0.04405286343612335,10,so,entropy|php|session,understanding session entropy,3,owasp|hijack|attacks,0.31154385209083557
53131,timing safe comparison requires knowing which array is coming from the user which determines the time it will take and which array is your secret which you don t want to give away the secret of how long it is   it s an ingenious technique that i first saw  here,2015-12-19 01:28:55.03 UTC,93,34366319,using the windows cryptography api how do i compare two byte arrays for equality in constant time?  edit the length of the secret is fixed and is public knowledge,0.010752688172043012,1,so,cng|cryptography|timing-attack|windows,compare 2 secrets in constant time using windows crypto api,1,timing attack,0.31141775846481323
16368,you are calling   from   when   that s the root cause of your infinite loop i don t know why it works on api 15 probably that you never reach this line in api 15   try something like this,2014-06-30 09:54:42.253 UTC,120,24487408,am using time picker through dialog fragment.in that time picker am set the minimum and maximum time.i got the stack-overflow error when i click the minusnow the time in minimum value in minutes button minutes in time picker in api 10 but in api 15 its working fine.am not able to get the solution for this issue.can any one know help me to solve this issue   time picker dialog fragment class     log-cat error,0.03333333333333333,4,so,android|android-dialogfragment|android-timepicker,stackoverflow error in timepicker in android,3,infinite loop|overflow error|stack overflow,0.31098729372024536
9201,password compromise doesn t require control of the database  what if i break into your car and steal a stack of dvds with database dumps?  you  do  back up your database right?  as mentioned people use the same password for multiple sites  hbgary fell victim to this when they were hacked by anonymous recently  one server with an sql injection vulnerability turned into a much larger compromise  if i have access to your database for five minutes and get the hash i now have access to your account until you change the password  salt is cheap    you should use a key derivation function anyway not a salt   most of the time data-theft from databases succeed through injection sometimes even blind injection  an attacker who has found a database injection exploit in one of your scripts doesn t gather any control over the rest of the system until he is able to retrieve some kind of higher credential - which could be the  admin s password  if you being the admin have your password stored as a simple md5 hash together with the rest of the users and the attacker manages to retrieve it - he could eventually overtake your system by using a rainbow table to look it up  if i understand them correctly rainbow tables remove the computational burden of calculating the hashes which is deliberately high so attacking is faster  yes  many people use the same password for everything compromising the original password as opposed to simply changing it to something you know can often give an attacker access to someone s accounts on other services  rainbow tables are also much less computationally intensive simple lookup than a dictionary attack which requires hashing or brute force which requires a lot more hashing  use strong unique passwords,2011-05-12 19:25:00.04 UTC,448,5983356,let s just assume a simple non salted hash function just a plain old    premises   the password hashing all takes place server side and the hashes are stored in the database the client has no way to see these  a rainbow table attack must have knowledge of the hashes in order to retrieve passwords   given premise 2 that would mean that the hacker already has control of the database in which point you have a much bigger problem on your hand  so is the point of trying to foil a rainbow table attack simply to protect the retrieval of passwords from an  already compromised database/system  is it that simple or is there something else that i am missing  i m already familiar with password hashing techniques but am just wondering why there is so much hype about rainbow tables thanks,0.029017857142857144,13,so,rainbowattack|rainbowtable|security,are rainbow tables attacks even a threat,5,attacks|exploit|protection|sql injection|vulnerability,0.31066420674324036
6585,phishing dictionary and social engineering based approaches are far more frequent than database leaks  database leaks result in far more passwords being released than via the above approaches because a large scale leak can result in hundreds of millions of passwords being released  however - why are you asking? for example if concerned about your own credentials assuming secure practices are used by the service a user can protect themselves from database leaks and dictionary attacks by using a secure unique password i.e your username hash pair can leak without the underlying password being revealed sophisticated phishing and social engineering can be harder to protect against at the individual level,2018-01-09 14:28:19,134,177164,list of user:hash published? same password for web and local os...? phishing,0.06716417910447761,9,sse,account-security|passwords,what are the most frequent ways someone s password leaks,3,leak|attacks|protection,0.30967992544174194
25367,a  tiny  fraction of reasons include   infinite loops in general  low memory in general  race conditions  dead locks  starvation  spawning too many threads  forking too much  super low process priority  infinite recursion  algorithms of bad algorithmic complexity  really slow operations on numbers  something is repeated too often e.g a variable that is better calculated outside a loop which the compiler was unable to move out  cache-unfriendliness  using sleep-like functions  invoking slow functions  running the daemon on a slow machine  your are being dos-attacked  your machine is running out of electricity and tries to slow down  your cpu has a bug  your cpu has a hardware defect  your cpu is running at too low voltage   these list items are not exclusive to each other we really can t tell something more specific without more information  a a bug  b a large job  are you thinking of a particular daemon?  slow non-responsive systems are more normally caused by  i/o  contention than cpu usage by the way install   to see what s hogging your disk  you can also   or   programs to free up the system a bit,2011-07-11 09:25:07.173 UTC,224,6648126,what are the possible condition might occur for daemon to hog the cpu and makes the system very slow or moved to non-responsive state?  daemons have few threads as well,0.026785714285714284,6,so,c|daemon|daemons|debugging|linux,when daemon will hog cpu,6,attacks|dead lock|infinite loop|race condition|denial of service|infinite recursion,0.309639573097229
32370,finally found out what is the problem.my application has one thread which calls 3rd party c/c++ dlls.if i remove this thread and execute the same code jn sequence then it works on 32 bit.it seems 32 bit machine has different threading model as compare to 64 bit in .net.finally we managed to fix it thanks everyone,2016-05-30 06:50:44.087 UTC,192,37519301,"we have 32 bit .net application which is calling some 3rd party dlls[c/c++].on 32 bit os with 2gb ram we continuously get attempted to read or write protected memory error after calling dll but on 64 bit os with 4gb ram success rate is 50% on 32 bit os with 4gb ram success rate is 100% hence it seems issue is due to memory is insufficient    but we observed that while application is running on 2gb memory,and memory is available still we are getting attempted to read or write protected memory can anyone suggest what is going wrong even if memory is available ?  is it address space issue ?   we have tried largeaddressaware but getting same issue",0.015625,3,so,.net|32bit-64bit|memory|memory-leaks|ram,32-bit application is failing on 32-bit os but working on 64 bit machine,2,protection|memory leaks,0.30938613414764404
3426,content-length repres the length of the entity-bodi in byte but where doe the count start? how was 13 derived? 0 is the first charact count in your first exampl but newlin in http are usual crlf \r\n which is two charact accordingli,2020-04-27 09:34:50,130,230609,i am tri to understand more about http request smuggl and i have been read thi articl https://portswigger.net/web-security/request-smuggl content-length repres the length of the entity-bodi in byte https://developer.mozilla.org/en-us/docs/web/http/headers/content-length but where doe the count start? portswigg exampl cl.te how was 13 derived? if i count the newlin befor 0 and between 0 and smuggl the total is 12? do i count the newlin after transfer-encod chunk as well? portswigg exampl te.cl similarli where do i start the counting? any help is appreci,0.023076923076923078,3,sse,http|penetration-test,calcul content-length http request smuggl,2,penetration test|http request smuggling,0.3083976209163666
30458,it s pretty simple to create your own dynamic rate limiting with  guice  and  sitebricks   using method interceptors  you could count the number of requests per ip address per servlet those counters can be stored in  memcache  and used to fail requests fast based on your own rules those can be completely application specific   you would still have to pay for the cpu time used to detect the dos attempt but as long as your algorithm is aggressive enough those costs will be minimal e.g one memcache get and checking a condition this is what i would do until gae provides their own dynamic dos protection,2013-02-09 07:24:01.493 UTC,206,14785921,there were a lot of dos attempts to my gae app which i couldn t reduce/combine them into less than 100 subnets to fit in the restriction is there a way i can block more than 100 subnets?  if any chance google app engine team is reading this i d like to say i really love gae but the way of blocking ips now is inefficient there should be features to help app owners block ips dynamically in terms of request rate or something smarter,0.024271844660194174,5,so,ddos|google-app-engine,how to block more than 100 dos attempt subnets in google app engine,2,protection|denial of service,0.3073723316192627
1585,by storing a staled hash of the password and then using this same hash as an authentication token  an application is more vulnerable to attack because this application is now storing authentication credentials in plain-text   an attacker can use sql injection to read this hash and then authenticate without having to crack the hash to obtain the plaintext password   the owasp wiki entry refereed in this question was misleading and was removed     here is how the salted md5 technique works the database stores a md5  hash of the password md5 hash is a cryptographic technique in which  the actual value can never be recovered. when a client requests for  the login page the server generates a random number the salt and  sends it to the client along with the page a javascript code on the  client computes the md5 hash of the password entered by the user it  then concatenates the salt to the hash and re-computes the md5 hash.  this result is then sent to the server the server picks the hash of  the password from its database concatenates the salt and computes the  md5 hash if the user entered the correct password these two hashes  should match the server compares the two and if they match the user  is authenticated   md5 is not appropriate for passwords  this is a broken primitive and a violation of  cwe-916    even if this was describing a secure challenge response https is a more appropriate tool and is required to protect the session id,2013-11-11 01:55:41,317,45254,the  owasp application security faq  recommends the use of javascript to produce a salted hash of the password client-side with javascript prior to sending it to the server   is this something truly advocated?  should i go this route how should the page behave should javascript be disabled? i imagine that signing in will be disallowed,0.03470031545741325,11,sse,authentication|html|javascript|owasp|passwords,owasp recommendation on client-side password hashing,7,cwe|owasp|attacks|protection|vulnerability|sql injection|plaintext password,0.30688750743865967
37295,,2014-07-15 12:55:19.177 UTC,85,24758757,how can i add more than 3-4 millions entries to a database table in django?  i tried using   but this is giving a memory leak the script uses more than 500mb and takes very long to execute  when i do it using flask+sqlalchemy i use   and all entries are created in 2-3 minutes  when i do it using symfony+doctrine orm all entries are created in 3-4 minutes  how i can do it in django,0.03529411764705882,3,so,django|memory-leaks|python,how to add many rows without memory leak,1,memory leaks,0.3068036735057831
595,system.random is not cryptographically random see  http://msdn.microsoft.com/en-us/library/system.random.aspx  it s predictable enough to weaken your crypto considerably  see tie-fighter s answer below for the keyspace calculation which i got hopelessly wrong by using 61^8 instead of 61^8!  i d be more concerned with how that password is stored surely your application needs to decrypt the zipfiles on the potentially-compromised computer in order to use them? is the password to do this stored on the same machine?  also to throw a spanner in the works - surely your  worst case scenario  is that an attacker gets access to the client s computer and watches the screen while the legit user does work with sensitive data?  the correct math is that with 26+26+10 characters and a password length of 8 you have a keyspace of 62^8 which is 218.340.105.584.896 or ~2^48  breaking this is feasible how long it would take is a complicated question depending on the attackers budget the implementation with a good gpu one can probably crack it within a day.you should use at least 12 characters probably more this depends on who your enemies are how much money they have how long the data should be protected etc...  also as the op suggested password storage might be a weak spot/problem  and using a crypthographic rng is always a good idea  p.s you can read more at    http://en.wikipedia.org/wiki/password_strength    http://en.wikipedia.org/wiki/brute-force_attack#theoretical_limits    http://www.wolframalpha.com/input/?i=password+of+8+characters    you may want to consider using different passwords on each file yes it s a pain but more secure   or you may want to consider using rar instead of zip since rar v3 does take longer to crack also both 7-zip format and rar support aes256 again it all comes down to how valuable the data is  if you are super paranoid encrypt using rar and then 7-zip the file with a seperate password  fyi crark supports gpu clustering so if you have a house full of laptops an desktops with gpu s you could theoretically crack your passwords within a reasonable ammount of time depending on the length of the pw again if super sensitive data use 2 passwords and 2 different formats with completely different passwords,2012-07-02 16:30:07,503,16747,how long would it take to brute force decrypt an aes 128 encrypted zip file if the password is 8 characters long in the range of a-z a-z 1-9?  does it make any difference if .net s   object was used to generate the password or if   was used?  i m writing an application and i want to make sure i get the security right in the worst-case scenario if an attacker got access to the client s computer he would have a few dozen zip files to attack but they all have the same password i want to ensure that the application is secure against this kind of offline brute force attack the attacker would not have access to any other randomly generated passwords or zip files encrypted with a different randomly generated password,0.02584493041749503,13,sse,.net|aes|brute-force,securing aes 128 encrypted zips against brute force attacks,4,attacks|weakness|protection|sensitive data,0.30450358986854553
16663,hash  is a python  built-in function  and use it to calculate a hash value for  object  not for string or num  you can see the detail in this page  https://docs.python.org/3.3/library/functions.html#hash   and hash values comes from the object s __hash__ method.the doc says the followings     by default the  hash  values of str bytes and datetime objects are “salted” with an unpredictable random value although they remain constant within an individual python process they are not predictable between repeated invocations of python   that s why your have diffent hash value for the same string in different console  what you implement is not a good way  when you want to calculate a string hash value just use  hashlib   hash is aim to get a object hash value not a stirng  python uses a random hash seed to prevent attackers from tar-pitting your application by sending you keys designed to collide see the  original vulnerability disclosure  by offsetting the hash with a random seed set once at startup attackers can no longer predict what keys will collide  you can set a fixed seed or disable the feature by setting the    environment variable  the default is   but you can set it to a fixed positive integer value with   disabling the feature altogether  python versions 2.7 and 3.2 have the feature disabled by default use the   switch or set   to enable it it is enabled by default in python 3.3 and up  if you were relying on the order of keys in a python dictionary or set then don t python uses a hash table to implement these types and their order  depends on the insertion and deletion history  as well as the random hash seed  also see the    special method documentation       note  by default the   values of str bytes and datetime objects are “salted” with an unpredictable random value although they remain constant within an individual python process they are not predictable between repeated invocations of python   this is intended to provide protection against a denial-of-service caused by carefully-chosen inputs that exploit the worst case performance of a dict insertion on^2 complexity see  http://www.ocert.org/advisories/ocert-2011-003.html  for details   changing hash values affects the iteration order of dicts sets and other mappings python has never made guarantees about this ordering and it typically varies between 32-bit and 64-bit builds   see also     if you need a stable hash implementation you probably want to look at the    module  this implements cryptographic hash functions the  pybloom project uses this approach   since the offset consists of a prefix and a suffix start value and final xored value respectively  you cannot just store the offset unfortunately on the plus side this does mean that attackers cannot easily determine the offset with timing attacks either      hash randomisation is  turned on by default in python 3  this is a security feature     hash randomization is intended to provide protection against a denial-of-service caused by carefully-chosen inputs that exploit the worst case performance of a dict construction   in previous versions from 2.6.8 you could switch it on at the command line with -r or the  pythonhashseed  environment option  you can switch it off by setting   to zero,2014-12-17 09:48:17.853 UTC,588,27522626,i ve implemented a bloomfilter in python 3.3 and got different results every session drilling down this weird behavior got me to the internal hash function - it returns different hash values for the same string every session  example   ----- opening a new python console -----   why is this happening?why is this useful,0.01870748299319728,11,so,hash|hash-collision|python|python-3.3|security,hash function in python 3.3 returns different results between sessions,5,attacks|exploit|protection|vulnerability|denial of service,0.30395984649658203
45722,you are leaking the interval timers you need to call   before initializing it again,2015-05-20 12:39:59.767 UTC,232,30349930,i have a digital clock function that takes the hours minutes and seconds from a json result parses them as integers does math on them add 1 to the seconds every time it s looped through if the seconds is 0 add 1 to minutes etc. after that math i parse these three variables as strings so i can then pad them with leading 0 s for a result that looks like  10:05:34hours minutes seconds  i use this method rather than datetimes because js will always parse datetimes in local time but the three variables are based on server time   which is called by this function that sets it at an interval     is initialized globally before so i can clear that interval on a window focus event.when i take this function out of my page i have a memory use of around ~20kb that stays fairly certain with this function included memory use starts around 40kb and increases every second which i think indicates a memory leak  .   is called on a nonstandard interval around every 45 minutes by the success callback of the ajax call that gets    and   do i have a scope problem? am i redifining variables needlessly when i have   on an interval,0.017241379310344827,4,so,javascript|jquery|memory-leaks|performance,why does my digital clock function leak memory,1,memory leaks,0.30293378233909607
27411,there are no security drawbacks to using base 64 since it is simply a different representation for binary data  as long as  you don t consider the base 64 as adding a layer of security  since base 64 is easily decoded by anyone since it uses no key it is  not  a security layer but rather just a different format  i emphasize this because i ve seen a lot of vulnerabilities resulting from the usage of base 64 to obfuscate sensitive data  the data was easily recovered by the attacker me in this case  don t worry this was in academia for a security class ;-   all sensitive data should be encrypted with a secret key from the server  i prefer to use base 64 myself since it can be processed easily as a string  i always convert binary data to base 64 after it s encrypted    as explunit mentioned in comment it will slow down your algorithm a bit  however the slowing down of the algorithm by spending cycles converting to base 64 is actually a plus for security since it makes a brute-force cryptographic attack less feasible for the attackers,2013-03-19 11:42:39.18 UTC,269,15498543,i have a hashed password to store in my sql srever database  not sure whether to convert it to a base 64 string before storage or whether to store it as    if it was base 64 it would be easier to deal with in my c# but i am not sure if there are drawback to that approach,0.022304832713754646,6,so,c#|passwords|security|sql-server-2008,hashed password - store as base 64 string in sql server vs store as varbinary,3,attacks|vulnerability|sensitive data,0.30221912264823914
17888,"that code contains multiple versions the last version with a random salt  getting randomized encryption result with salt  is almost ok   as you can see the iv is generated from pbkdf2 which means that the  same iv will be produced when the same salt and password are used  this is an undesirable effect because it means that the same plaintext will create the same ciphertext an attacker might deduce that you sent the same message again by only observing ciphertexts this is of course not semantically secure which is why some randomness is necessary  a way to solve that would be to use a random salt   another problem with this code is the  low iteration count of 1000  nowadays an iteration count of at least 60,000 up to a couple of million should be used otherwise it gets easy for an attacker to try a lot of popular passwords and find the one password you used so increasing the iteration count would also severely limit the throughput for an offline attacker  another important problem with the code is that there is  no authentication of the ciphertext  depending on your system architecture an attacker might launch a padding oracle attack and decrypt any ciphertext that you have sent with multiple online queries the way to protect against that would be to use an authenticated mode of operation like gcm or eax or employ an  encrypt-then-mac  scheme with a strong mac like hmac-sha256  the last problem is that pbkdf2 is used to derive the key  and  the iv from the password  this is not a good idea    i ve found  this code  which doesn t seem to have any of the problems discussed above",2015-11-16 14:57:29.303 UTC,345,33738292,i m checking this aes encryption/decryption project  http://www.codeproject.com/articles/769741/csharp-aes-bits-encryption-library-with-salt   but i read that it has vulnerabilities with iv and salting how can these vulnerabilities occur?   i know aes is immune to known plaintext attacks but if this implementation is wrong is there a easy way to break it and get the key or plaintext,0.028985507246376812,10,so,aes|c#|encryption,vulnerable iv and salt in aes encryption,3,attacks|protection|vulnerability,0.3012382388114929
13126,,2017-03-01 21:57:02.37 UTC,121,42542875,context  training with the gbtregressor on spark 1.6.3 + scala on a feature set of about 2000 and with ~1 million rows  using 10 ec2 instances with 32 cores each and we end up with 30 executors each with 10 cores  hyperparameters/parameters- depth:3 iterations:100 stepsize 0.1 cachenodeids true checkpointinterval:10  partition size is 5000  problem   about 50% of the time we get a stackoverflow error always around stage 1000 other times the trainer finishes fine our gbtregressor is the first   in the    sample exceptions though the error has been occuring at varying depths inside     another   we submit with also tried upping the thread stack size with xss but to no avail,0.024793388429752067,3,so,apache-spark|apache-spark-ml|apache-spark-mllib,apache spark nondeterministic stackoverflowerror when training with gbtregressor,3,overflowerror|overflow error|cross site scripting,0.3011041283607483
407,"what you could do is to force the request to sleep for an exponential time on every failed login   for the first login hold for one second second failed 2 seconds third failed 4 seconds etc or longer or shorter depending on what your server can handle it doesn t hurt normal users who mistype their password but it makes anyone attempting a brute force wait a very long time after only a few failures  reducing the number of guesses per minute is the way to go but implementing it as a   in your code will still consume precious resources and lead to a denial-of-service  instead you want the client to wait not the server so log the time the attempt was made add   seconds to that time and don t attempt further authentications from that user/ip until after that time has expired  edit added   that is if an authentication attempt is received reject it with an error message without even checking the password   you ll want to put some corresponding javascript in the login page so that the user won t submit a new login attempt until that timeout has expired plus a safety margin that keeps users from seeing unnecessary error messages and getting confused  increasing the timeout with each attempt is also a good idea this helps to keep from inconveniencing normal users while making a brute-force attack practically impossible  my general view on dos is all existing systems are fundamentally vulnerable to dos  it is just a question of degree  if you have a high-profile site that is at high risk for dos it may be worth worrying about these vectors  if you just run an ordinary everyday site i wouldn t bother  in most sectors dos attacks are rare so they re probably not worth spending time and energy to defend against  there are of course exceptions but i m talking about most sites here.  the great thing about dos attacks is that if they occur they are readily detectable   edit  2/25 it sounds like you really want to do what you can against computational dos attacks despite the limitations i ve articulated  given that please see my  other   answers  for some ways to make computational dos attacks harder  they re far from perfect and personally i m sticking with my bottom line recommendation namely implementing these defenses most likely isn t the best use of your time but there they are in case you feel differently  one possibility is to consider using  client puzzles   the basic idea is to force the client to do a significant amount of work and prove it has done so before you will accept a username/password pair and try to validate it  here is an overview of the approach  the server generates a random puzzle and sends the puzzle to the client  the server generates the puzzle in a way that it can predict reasonably well how much effort will be required to solve the puzzle e.g 100ms of computation  the client solves the puzzle and then sends the solution along with the user s username and password  in a web setting this would probably be implemented with javascript the login page would contain the javascript code to solve the puzzle and the puzzle description  a legitimate user s web browser would run the javascript on the page which would solve the puzzle and include the solution in the form along with the username and password  this makes it harder for an attacker to dos the server  for instance if it takes the server 100ms of computation to check the validity of the user s password and if the server chooses a puzzle that takes 100ms to solve then an attacker will have to do at least as much work as the server  if you turn up the dial to select puzzles that will take 200ms to solve then now the attacker has to do twice as much work as you to dos you  thus this does not prevent dos but it makes it harder and requires the attacker to invest more resources -- which may reduce the risk somewhat  here s a very simple example of how to construct a puzzle  the description of the puzzle is a random 128-bit number  r  chosen by the server and sent to the client  the goal is to find a number  n  such that sha256 r   n  is zero in its low 20 bits  the client can loop over values of  n  test whether  n =0 is a solution test whether  n =1 is a solution test whether  n =2 is a solution ... until the client finds a solution  on average you should expect this to take about 2 20  trials  if the client can perform about ten million hashes per second then 2 20  trials should take about one-tenth of a second 100ms  once the client finds a solution the client can send  n  to the server  note that the server can quickly verify that  n  is a valid solution to the puzzle it takes only one hash not a million hashes  this creates the required assymetry where the server can very quickly generate new puzzles and verify claimed puzzle solutions but where it takes longer to solve the puzzle and where the server can control how long puzzle-solving takes with a fair degree of accuracy  this is just one example of a puzzle construction there has been a great deal of research on this problem and there are  better   constructions   you might be especially interested in  kapow  which implements this idea for the web context  the main limitation is that this is not going to be secure against dedicated attack  a serious attacker can just acquire a botnet of a few hundred machines this is not that expensive and dos you and now you re completely hosed  client puzzles can t stop that kind of attack they can only stop a relatively low-grade attack  however stopping dos is in general very difficult  a secondary limitation has to do with the broad variability of computational power of clients that might access your website  a puzzle that a high-end desktop can solve in 100ms might take a mobile device 1000ms to solve i m making up numbers here  if you choose parameters that make the puzzle relatively easy to solve on the lowest-power device that is ever likely to access your website then you may find that security against high-end desktops is degraded  also it seems plausible that a c program run by an attacker might be able to solve the puzzle much faster than the javascript run by legitimate users which would further exacerbate the gap between how fast an attacker can solve puzzles vs how fast the slowest legitimate client can solve them  this gap degrades security  a tertiary limitation is that client puzzles require the client to use javascript to log in  however this might be acceptable  a variant of this idea is for the server to monitor itself and check whether it is under heavy load  under normal conditions skip the puzzle  however if the server seems to be under dos attack then start requiring login attempts to come with a puzzle solution  this mitigates some of the disadvantages of the client puzzle approach but still is far from perfect and still cannot withstand a serious dos attack  i describe this potential solution in case you interested but i don t really recommend it for most sites  for most sites the risk of dos is relatively low and if it does occur you can deal with it at the time  for that reason my guess is that it probably isn t worth your time or energy to implement these kinds of defenses  if you ve got that much extra time and energy to spare you might be better off spending it building new features or making your site better  another approach you could consider is to use captchas to make this kind of dos attack harder  the basic idea is to require the user to solve a captcha when they want to log in and require them to present a correct solution to the captcha before you will validate their password  this is unfriendly to users so if you want to consider this i would recommend a variant  have your server monitor its own load so it can detect when it is under attack  for instance measure how much of your cpu time you are spending on bcrypt-hashing passwords or just measure the load on the server.  under normal conditions do nothing special users just need to present a username and password as usual with no captcha anywhere  when you detect you might be under attack enter self-protection mode  in this mode the server should send a captcha in the login page asking users to enter in their username enter their password and enter the solution to the captcha  when the user clicks submit the server should first validate the solution to the captcha before using bcrypt to verify the user s password  if the user didn t solve the captcha properly don t even try to verify their password  this raises the bar against dos attacks because now an attacker cannot simply send 10 invalid username/password pairs per second the attacker will have to also solve 10 captchas per second to dos the server  as a result the trivial dos attack no longer succeeds  also this provides dos resistance while ensuring that in most situations users never need to see a captcha users only need to solve captchas when the server is under attack  of course you can combine this scheme with rate-limiting by ip address and by username if you wish  that said the security of this scheme is far from perfect  there are underground markets used by attackers that exist to automate the process of solving captchas for you at a fee  these services provide an api that their customers can use to submit a captcha and get back the solution  the  going rate  is something like a few dollars per thousand captchas solved  based upon this we can compute what it would cost to mount a successful dos attack against this scheme by using these captcha-solving services  an attacker would need to solve 864,000 captchas per day 10 per second to take down your server which would cost a few thousand dollars per day of downtime using existing markets  this is an upper bound on the cost of mounting a successful dos attack  however it almost certainly greatly over-estimates the cost of dosing your server as there will almost certainly be other far more economical ways of mounting a dos attack on your server  anyway i share this potential solution based upon use of captchas but i don t really recommend it for most sites for most sites the risk of dos is relatively low and if it does occur you can deal with it at the time for that reason my guess is that it probably isn t worth your time or energy to implement these kinds of defenses if you ve got that much extra time and energy to spare you might be better off spending it building new features or making your site better  currently ddos is the simplest and cheapest way to take down your server  i have seen estimates for the cost of ddos attack in the range of  $100 per day  though it may cost  significantly more  to defeat a larger better-resourced target  protecting your server against dos attack is  not easy  the economics of it are not working in your favor  perhaps one of the best things you can do is arrange for your site to have a lot of excess capacity and to make sure it can scale up as traffic grows perhaps using a cloud service with vms and network capacity on demand  the advantage of this approach is that it is beneficial even if you never come under dos attack because it will help you avoid getting overwhelmed by a flood of legitimate users -- e.g if you get slashdotted  basically the first step to make a risk evaluation   what are the potential attack scenarios ?  how much resources are available to the attacker ?  is a dos the worst case scenario ?   what is the probability versus business cost of each scenario ?   depending on the answers above the measures can vary - for example   a simple scheme implemented on the html level won t defend against anything but the most basic form of attack  a simple increase the time to next password validation with every failed attempt and return always false in the meantime this slows down any attack considerably but won t hold up against a saturated network  handle the password validation on some other machines which can be easily scaled up on case-by-case-basis this help keep the site responsive by outsourcing the heavy lifting  implement some defenses on the network level usually done one or more hops before the request reaches your server this can help in cases where the attacker has lots of bandwidth capable of satuarating your network..   one thing to be kept in mind  even the best defenses will not hold when confronted with an attacker with unilimited bandwidth and ips..  my simple recommendation and what i have done to mitigate dos attacks on slow password hashing is to use a password strength calculation score as primary validation method  so for a user login example i get the user record by email then first check if the strength score match against the plain text password which is obviously fast just a couple functions and only start password hashing functions with a score match else bail with a failed request  and to avoid exposing the strength of the password in the database you can find some sort of basic one way calculation algorithm to mask the real password strength score",2012-02-23 23:12:57,2722,12101,i ve been thinking about bcrypt recently and what i wonder is if there s a way to deal with the inherent ddos problems with slow hashing functions namely if i set up bcrypt so my machine takes 100 ms to hash a password then it only takes 10 password attempts per second to make my server unresponsive i could reduce the difficulty but the faster i make it for me the faster i make it for an attacker i could ban anyone who tries too many passwords but botnets are everywhere these days  are there any common solutions to this problem?  i was thinking there s probably strategies where you make the client do something complicated but easily verifiable and check the result before checking the password but it has a couple problems   is there a way to do it in plain html without javascript  what would the something complicated be? rsa encryption gave me the idea of generating two primes multiplying them together and asking the client what the original primes were but i suspect the generating two primes step wouldn t be particularly fast either   another thing i had considered was caching bcrypt results in memory so attacks against one user don t cause problems but what if i have too many users to reasonably do that?  i m mainly thinking of using this in the context of a username/password login box on a website but any method of dealing with this could likely be done in javascript if there s nothing better  edit  the idea of somehow rate limiting requests is useful but it s not the kind of solution i had in mind it makes ddos attacks more expensive because you need more ip addresses but it doesn t solve the fundamental problem that the server is doing significantly more work than the attackers  an example of what i m thinking of is how using a salted password makes rainbow tables  worse  than a straightforward brute-force attack so if you re using salt you don t have to worry about rainbow tables at all since there s better attacks what i want is a way to make attacking my login page no better or worse than just hitting random urls,0.0319617927994122,87,sse,authentication|cryptography|denial-of-service|password-management,prevent denial of service attacks against slow hashing functions,5,ddos|attacks|protection|vulnerability|denial of service,0.30020859837532043
15470,brute force is much harder than you think if a person is using a bad password it s their problem even a weak password 8 characters would require 2 years of brute forcing if the attackers can do million attempts every second you have many options   limit the number of login attempts per username per five minutes this requires a table in your database where you keep the requests and their time for the last 5 minutes more or less.this has the unfortunate side effect of allowing someone to ddos one of your users  limit the number of global login attempts.. this is easier but can make ddos of your entire server easy i wouldn t do it  limit the number of attempts from an ip or from an ip/user that wouldn t allow an easy ddos but it won t stop a distributed brute force attack so don t bother   i d do the following require passwords of 10 characters or more scream if they aren t strong enough or are based on dictionary words but still allow them log when there are too many login requests and investigate personally as soon as possible  all of this happens on the server side and has nothing to do with ajax     i m thinking of making it refuse access unless it is from my own site s form...but how would you go about doing that   it is impossible to achieve this reliably you could use some sort of  captcha  throttle requests configure your firewall to drop multiple successive requests from the same ip which will make the attacker job a bit harder  after 3 requests from the same ip simply delay each response from your server by an incremental 2 second  don t use sessions or any client side mechanism just use a temporary table for login request who store ip and number or failed auth that you use for increment your time after 15 min without auth attempt from an ip flush its entry  with that brut force can be a little tricky for the bad guy maybe the have some years for attempt to access to your site an it preserve usability for dyslexic like me who can need to re-type his password 4 or 5 time fore the good one without error ^,2010-11-27 10:17:35.047 UTC,479,4291127,suppose i have the file      takes get or post parameters username and password and return true if login successful and false otherwise this way other pages can access it via ajax  my question is since this method is vulnerable to brute force attacks how should i secure this i m thinking of making it refuse access unless it is from my own site s form but how would you go about doing that?  i use jquery to make ajax calls,0.020876826722338204,10,so,ajax|brute-force|login-attempts|security,how to prevent brute force attacks on ajax form submission,4,ddos|attacks|vulnerability|weak password,0.299927681684494
47799,,2015-07-28 22:12:02.933 UTC,87,31688021,i ve got this weird behavior sometimes for no reason on random  post  requests csrfviewmiddleware.process_view hangs for ages than the entire website becomes unresponsive than everything gets back to normality  this server is behind a proxy squid i believe and i m positive it s something to do with this i m in the process of moving the admin out of the proxy but this has been buggin  me for a long time..          traceback,0.04597701149425287,4,so,django|django-csrf|python,why django.middleware.csrf csrfviewmiddleware.process_view sometimes hangs and crashes after 5 minutes,2,django csrf|cross site request forgery,0.29902029037475586
10891,did you execute this code in different sessions of python? if so the hash code for the string keys likely changed to defend against denial of service attacks caused by intentionally submitting keys with colliding hashes python salts hashes for     and other strings of data types   per the docs      note      by default the   values of str bytes and datetime objects are “salted” with an unpredictable random value although they remain constant within an individual python process they are not predictable between repeated invocations of python      this is intended to provide protection against a denial-of-service caused by carefully-chosen inputs that exploit the worst case performance of a dict insertion on^2 complexity see  http://www.ocert.org/advisories/ocert-2011-003.html  for details      changing hash values affects the iteration order of dicts sets and other mappings python has never made guarantees about this ordering and it typically varies between 32-bit and 64-bit builds      see also  pythonhashseed    if you require repeatable ordering based on insertion order use,2016-03-09 04:50:50.747 UTC,223,35883140,i created a function that counts the number of times a string has appeared in a list and returns a dictionary i got the program to work but interestingly enough when i changed the value of a key value d[i] the order of the names changed  why is python doing this?   before     input    result       after    input     result,0.02242152466367713,5,so,dictionary|python,python dictionary result changes after adding 1,4,attacks|exploit|protection|denial of service,0.29769933223724365
23169,20 top factors that impact website response time 1 complexity 2 interdependencies 3 configuration and communication of components 4 latency 5 demand peaks 6 web page size 7 responsive web design 8 javascript 9 web content and code 10 n+1 queries 11 the back-end server 12 the database 13 third party services 14 the network 15 virtualization 16 it infrastructure changes 17 altered code 18 distributed denial of service attacks ddos 19 information architecture,2015-09-25 03:00:22.233 UTC,135,32774091,i m working on some performance tests on web pages that use the https protocol and for me it is still unclear which factors interfere in the page response time what makes a web page to respond faster or slower a request from a client,0.022222222222222223,3,so,http|https|performance|testing,what are the factors that affect the response time of a webpage,3,ddos|attacks|denial of service,0.29751530289649963
25351,sha-256 is not a great hashing method use bcrypt pbkdf2 or scrypt  make your salts the same length and put them first  peppering hmac-ing passwords is useful if you think you have the chance a sql injection vulnerability down the road and a lower chance of an application server vulnerability  correct peppering means your application server holds the key and it is  not stored in your database    if you accidentally have a sql injection down the road a hacker might steal all your peppered passwords but wouldn t be able to steal the key  it sounds like you re going to be doing hashing on every request which is going to painfully slow you should hash once and provide a token,2017-05-05 15:43:31.67 UTC,524,43808961,i m getting a bit confused about hashing passwords with a salt and the way of hmac an password or using as a signature and so on i ve read a lot of articles about it but it seems that i don t got the point of using this or that  i got this examples of using that ways questions marked by a number  hashed passwords  password from user is hashed with sha-256 and a unique salt for each user stored in the database password and salt is just concatenated like this   and run multiple rounds of hashing   that may be vulnerable for an length extension attack right?   would changing the order of values make any prevention of lengthextension attacks? so changing   to  ?   a better way of hashing the user password seems to be hmac in this case using      but is this really better and more secure to use hmac here?    some people said it makes no sense to use a salt as a password for hmac    but the salt is not visible for the user because it is just stored    in the database for me it s nothing other than a password   api authentication/validation  for an api all users got a unique and random   and an  .the   is sent in all requests by an header to identify the user that should be a kind of stateless authentication  the   will be used in backend to generate a   using     the   is a random value that is send in another header as plain text and as well as a hashed value to verify that the   itself won t be manipulated well not really needed because it will change the whole  .. so a kind of nonce-signature     but is this really more secure than just make something like   and leave the   without any nonce-signature?  is there any security issue by providing multiple hmac-hashes at the same time that are hashed with the same  ?  any other toughts about that in point of security?    it s hard for me to understand what the more or less secure ways of creating a secure hash are some people said ok this is secure.. you should do this and then other people said oh you forgot this or that attack or vulnerable so i try to understand how and why in a simple and i-am-a-dummie-way,0.01717557251908397,9,so,hash|hmac|restful-authentication|security|sha256,hashing passwords and signature with hmac any security advantages,3,attacks|vulnerability|sql injection,0.296661376953125
9926,"i agree this is an arbitrary programming decision  putting the delay to one second instead of three doesn t really hurt the crackability of the password but makes it more user-friendly   technically this  deliberate delay  is to prevent attacks like the  linearization attack   there are other attacks and reasons as well      to illustrate the attack  consider a program without this  deliberate delay which checks an entered serial to see whether it  matches the correct serial which in this case happens to be   xyba   for efficiency the programmer decided to check one  character at a time and to exit as soon as an incorrect character is  found before beginning the lengths are also checked       the correct serial length will take longer to process than an incorrect serial length even better for attacker a serial number  that has the first character correct will take longer than any that  has an incorrect first character the successive steps in waiting time  is because each time there s one more loop comparison to go through  on correct input           so attacker can select a  four-character string  and that the string beginning with  x  takes the most time  by guess work     attacker can then fix character as  x  and vary the second character    in which case they will find that  y  takes the longest    attacker can then fix the first two characters as  xy  and vary the third character in which case they will find that  b  takes the  longest    attacker can then fix the first three character as  xyb  and vary the fourth character,in which case they will find that  a  takes the  longest         hence the attackers can recover the serial one character at a time     linearization.java      linearization.docx sample output       the serial number is four characters long ans each character has 128  possible values  then there are 128 4  = 2 28  =  268,435,456 possible serials  if attacker must randomly guess  complete serial numbers she would guess the serial number in about   2 27  = 134,217,728 tries which is an enormous amount of work  on the other hand by using the linearization attack above an  average of only 128/2 = 64 guesses are required for each letter for a  total  expected work of about 4 * 64 = 2 8  = 256 guesses   which is a trivial amount of work    much of the written martial is adapted from  this  taken from mark stamp s information security principles and practice also the calculations above do not take into account the amount of guesswork needed to to figure out the correct serial length   what i tried before appeared to work but actually did not if you care you must review the wiki edit history..  what  does  work for me is to  both  lower the value of pam_faildelay.so delay=x in  /etc/pam.d/login  i lowered it to 500000 half a second  and also  add nodelay preceded by a space to the end of the line in  common-auth  as described by gabriel in his answer     at least for me debian sid only making one of these changes will not shorten the delay appreciably below the default 3 seconds although it is possible to lengthen the delay by only changing the value in /etc/pam.d/login  this kind of crap is enough to make a grown man cry!  on ubuntu 9.10 and i think new versions too the file you re looking for is located on      /etc/pam.d/login   edit the line     auth       optional   pam_faildelay.so  delay=3000000   changing the number 3 with another you may want  note that to have a  nodelay  authentication i think you should edit the file      /etc/pam.d/common-auth   too on the line     auth    [success=1 default=ignore]      pam_unix.so nullok_secure   add  nodelay  to the final without quotes.but this final explanation about the  nodelay  is what i think  i would like to add a note from a developers perspective though this wouldn t be obvious to the naked eye a smart developer would break out of a match query when the match is found in witness a successful match would complete faster than a failed match because the matching function would compare the credentials to all known accounts until it finds the correct match in other words let s say there are 1,000,000 user accounts in order by ids 001 002 003 and so on your id is 43,001 so when you put in a correct username and password the scan stops at 43,001 and logs you in if your credentials are incorrect then it scans all 1,000,000 records the difference in processing time on a dual core server might be in the milliseconds on windows vista with 5 user accounts it would be in the nanoseconds  i don t see that it can be as simple as the responses suggest  if response to a correct password is some value of immediate don t you only have to wait until longer than that value to know the password is wrong? at least know probabilistically which is fine for cracking purposes and anyway you d be running this attack in parallel.. is this all one big dos welcome mat?  failed authentification delays are there to reduce the rate of login attempt the idea that if somebody is trying a dictionary or a brute force attack against one or may user accounts that attacker will be required to wait the fail delay and thus forcing him to take more time and giving you more chance to detect it  you might also be interested in knowing that depending on what you are using as a login shell there is usually a way to configure this delay   in gdm the delay is set in the gdm.conf file usually in /etc/gdm/gdm.conf you need to set retrydelay=x where x is a value in seconds  most linux distribution these day also support having fail_delay defined in /etc/login.defs allowing you to set a wait time after a failed login attempt  finally pam also allows you to set a nodelay attribute on your auth line to bypass the fail delay  here s an article on pam and linux   it s a very simple virtually effortless way to greatly increase security  consider    system   has no delay  an attacker has a program that creates username/password combinations  at a rate of thousands of attempts per minute it takes only a few hours to try every combination and record all successful logins    system   generates a 5-second delay after each incorrect guess  the attacker s efficiency has been reduced to 12 attempts per minute effectively crippling the brute-force attack  instead of hours it can take months to find a valid login  if  hackers were that patient they d go legit  :-    basically to mitigate against brute force and dictionary attacks  from  the linux-pam application developer s guide        planning for delays       this function is offered by linux-pam  to facilitate time delays following a  failed call to pam_authenticate and  before control is returned to the  application when using this function  the application programmer should  check if it is available with       generally an application requests  that a user is authenticated by  linux-pam through a call to  pam_authenticate or pam_chauthtok.  these functions call each of the  stacked authentication modules listed  in the relevant linux-pam  configuration file as directed by  this file one of more of the modules  may fail causing the pam_... call to  return an error it is desirable for  there to also be a pause before the  application continues the principal  reason for such a delay is security a  delay acts to discourage brute force  dictionary attacks primarily but also  helps hinder timed covert channel  attacks   i am not sure but it is quite common to integrate a delay after entering a wrong password to make attacks harder this makes a attack practicaly infeasible because it will take you a long time to check only a few passwords  even trying a few passwords - birthdates the name of the cat and things like that - is turned into no fun  this makes it take longer to guess passwords   it s actually to prevent brute force attacks from trying millions of passwords per second the idea is to limit how fast passwords can be checked and there are a number of rules that should be followed   a successful user/password pair should succeed immediately  there should be  no  discernible difference in reasons for failure that can be detected   that last one is particularly important it means no helpful messages like   or   not even a time difference in response between the invalid user and password and valid user but invalid password failure reasons   every  failure should deliver exactly the same information textual and otherwise  some systems take it even further increasing the delay with each failure or only allowing three failures then having a massive delay before allowing a retry",2009-04-03 02:24:21.15 UTC,1526,712339,this question has always troubled me  on linux when asked for a password if your input is the correct one it checks right away with almost no delay but on the other hand if you type the wrong password it takes longer to check why is that?  i observed this in all  linux distributions  i ve ever tried,0.01703800786369594,26,so,authentication|passwords|security,why should checking a wrong password take longer than checking the right one,3,bypass|attacks|denial of service,0.29568231105804443
48806,,2014-12-28 14:38:37.28 UTC,435,27677391,"i am attempting an r randomforest analysis in r on a wide genetic dataset​662 x 35350 all variables except the outcome are numeric and 99% of them are binary 0/1 i am quite familiar with r randomforest but have only worked with datasets with 5000-10000 variables previously the next planned phase of analyses will be on an exceptionally large dataset with millions of variables so i am motivated to find a solution to this problem   my understanding is that r randomforest has no inherent limits on number of variables and i know i ve read published work with variable numbers in the 100,000s  when i attempt the analysis on the full dataset setting ntree=100 i get:error protect protection stack overflow  this is true whether the dataset is a dataframe as it was originally provided or when i convert it to a matrix when i submit the run to a cluster for parallel processing i see that all of my cores are working as soon as i execute the code i also see that at no point does my ram usage approach the machine s limit 48 gb at most it hits about 16% of ram during the execution attempt  i also had the same problem on my 512 gb ram machine at the office where it never used more than about 5%  i have tried several solutions found online including one in a previous stackoverflow post  increasing or decreasing the memory available to r processes  i tried the instructions provided by bobbyshaftoe in 2009 adding --max-mem-size=49000m and --max-vsize=49000m within the properties of the shortcut tab but this prevented r from opening properly i also tried running these commands in command line but these generated  --max-ppsize / --max-vsize=5000m is not recognized as an internal or external command operable program or batch file  i have also read the suggestions made in this post  how to improve randomforest performance?   i can t reduce the number of features until i have at least one run with the full feature set plus i am not sure the problem is ram per se.  i m on windows 7 running revolution r 7.2 64-bit.my memory limit is set at 49807 mb but i m not sure if the memory.limit is specifially addressing the protection stack size allowed   breaking the dataset into smaller chunks of variables which does solve the stack overflow problem does not solve the analytic problem are there any suggestions as to r settings that may permit the analysis on the full dataset?   see sessioninfo",0.011494252873563218,5,so,r|random-forest|stack-overflow,r stack overflow error with randomforest on large dataset 48-512 gb ram,3,protection|stack overflow|overflow error,0.29424190521240234
29888,it just behaves as if your cpu is lamer or as if your cpu takes some milliseconds off every one second because some cpu cycles are sunk in the contention monitering process,2017-04-19 09:58:39.9 UTC,77,43492374,in java docs  i found that if we enable the thread contention monitoring  then overhead will be there in targeted application    this is overhead affects in response time or how it will affect the performance ,0.012987012987012988,1,so,java|memory|memory-leaks,enabling thread contention montioring increases performance overhead,1,memory leaks,0.2939944863319397
55065,the issue that allowed account takeover was the rate limit on the endpoint validating the 6 digit code was ip based  this means that even though the endpoint had a rate limit of ~250 requests per ip address with access to a large enough pool of ip addresses you could try all 1 million codes,2019-07-19 22:46:01.123 UTC,187,57120340,"1 suppose i go to instagram forgot password page i enter a persons mobile number whose account i want to hack   2 it sends a 6 digit code to that person  3 i want to brute force it - around a million combinations   4 i get 5000 ips and send 200 requests per ip and brute force it i,e sending requests concurrently from all the ips there is a race condition here right so wouldn t i be able to break in?   as the requests are going concurrently how can you keep a count and block the user after few failed attempts? that is what this guy did l https://www.forbes.com/sites/leemathews/2019/07/15/hacker-discovers-a-simple-way-to-hijack-any-instagram-account/#56e3c87e7b64  can someone explain how",0.016042780748663103,3,so,hacker-news|race-condition|security,can i brute force a 2 factor authentication to reset password using multiple ip s,2,hijack|race condition,0.2936480641365051
6915,,2018-04-22 08:27:34,124,184288,i read this article  https://www.sjoerdlangkemper.nl/2016/02/11/cracking-php-rand/  about cracking php rand function and i involved with some problems on testing methods mentioned in article i used php 5.5.16windows x64 version for testing  t1.php   test.php   i generated some csrf tokens in vulnerable application as follows   i took 308axol3zs and used it in wincrackseed.c file i edited that file a little bit here my edited file   https://pastebin.com/ks8s8ps5   my problem is when i tried to crack seed using this attack attack was active about 2-3 hours but nothing matched ! in my laptop its checked over 34 million seeds in that period of time.but in article sjoerd langkemper said     this will take about 10 minutes for the windows version,0.03225806451612903,4,sse,php|random,problem in cracking php rand seed on windows,3,attacks|vulnerability|cross site request forgery,0.29255732893943787
2044,,2020-04-13 22:21:11,204,61197767,i have a situat where i have a veri larg panda datafram and much of the process is done with data type float 16 due to memori constraint at a point in the process i save the current result to a cach and the parquet format requir float 32 -- so i upcast at thi point at thi point my aggreg data can fit into memori for each float column i do df[col] = df[col].astyp float32 the issu i m have is that after i ve convert all of the column to float32 the amount of memori that has appear to leak is exactli the size of the origin float16 datafram i can t figur out how to releas it in my toy around i did discov a way to produc a similar issu though not exactli like mine if i omit df.values.flag the first time the memori leak doesn t show up if i insert it the memori leak remain until i call it a second time i m hope know why thi is happen will help me with my problem sinc it s veri similar,0.024509803921568627,5,so,memory-leaks|pandas|python-3.x,why is there a panda datafram leak until i call valu on the datafram,1,memory leaks,0.2888599932193756
5887,"you ask about a scenario where an attacker is able to create a hash collision with an arbitrary file the attacker does not control in this case the valid download this is called a  preimage attack  and is generally harder than a simple collision attack but your scenario involves a preimage attack rather than a simple collision under that scenario the following assertion does  not  hold     it would be extremely improbable for an attacker to be able to generate a collision that would be a runnable program and could somehow infect your system   assuming you have determined how to produce any file y that has a hash collision with valid file x it is likely   just as easy   to instead produce a working binary z with the same hash value binaries have enormous degrees of freedom you can modify embedded resources string tables and icons and so on metadata program name author compilation date etc. and of course just stick the garbage used to produce the hash collision at the end of the file after the real malicious program  when people produce a hash collision they aren t working from a pristine state in which there is no hash carefully crafting data that bit by bit produces the desired hash at least not if they re using any even vaguely-modern hash algorithm all possible inputs  including the empty string  have a valid hash digest the goal of the collision-seeker is to find an input that produces the desired output but there s no reason the input can t be partially a blob of fixed data such as a malware program yes that blob will have its own initial hash value digest.. but so does the empty string!  so yes if arbitrary blob a by itself produces the desired collision blob a by itself as a file is extremely unlikely to be a meaningful much less malicious program however the amount of work it takes to find blob a  is the same amount  as it takes to find blob b that when concatenated on the end of fixed malicious program m the digest of m+b produces the collision   now with that said the collisions found thus far on sha1 are not preimages that is the security researchers found two arbitrary blobs a and b that have the same sha1 digest but did  not  demonstrate the ability to for a  specified  file x or specified digest d where presumably d = sha1x for some x produce a file y that has the same sha1 digest d as x they produced a collision but not a preimage attack  collision resistance is a characteristic of a secure hash function so finding any collision has cast doubt on the overall security of sha1 however we re some ways yet from being able to  produce preimage attacks against either arbitrary digests or arbitrary files against sha1   even though it proved to be possible to create two files with the same hash collision it is not feasible to create a file that matches some predetermined hash preimage attack therefore the scenario you describe is not particularly vulnerable to a hash collision  however keep in mind that if the attacker can inject another executable he may also be able to inject another hash value  sha1 collisions are particularly a problem when signing documents such as certificate requests with the ability to create a hash collision an attacker can create two certificate requests one for legit.com and one for evilattacker.com with the same hash the certificate authority will then sign legit.com and the attacker can use the signature to create a valid certificate for evilattacker.com and vice versa  its been a while that sha-1 collision was successfully achieved by g oogle researchers  as proof of concept the research presents two pdf files  [pdf1   pdf2]  that have the same sha1 hash but display totally different content   you can learn more in  this  academic paper  its also worth to mention that this cryptographic hash function is 22-year old but as far as i m concerned we still far from seeing real world attack conducted especially in the scenario you described except if you are targeted by a government or a a wealthy criminal enterprise     a practical collision attack against sha-1 would cost $700,000 in  2015 and $143,000 in 2018 he surmised at that cost attacks,  especially if they were carried out by a wealthy criminal enterprise  or government entity could be feasible  bruce schneier    now to be more practical a sha-1 collision may affect the microsoft kernel-mode code signing policy for instance this attack relied on signature verification for loading only signed kernel-mode drivers you can find more  here  but for now i m not sure you need to worry about a mitm attack delivering an altered file",2017-05-17 07:04:28,955,159694,say you go onto a website and are downloading a program next to the file there is a sha-1 checksum of the file you download the program verify the checksum and find that it is the same as the one on the website - perfect!however you soon find that the sha-1 checksum is not the same because the program is the same but a man in the middle appears to have delivered you a collision of the program and it s not what you thought it was at all.my question is what s the problem with that? it would be extremely improbable for an attacker to be able to generate a collision that would be a runnable program and could somehow infect your system the biggest inconvenience i can see is that you would have to download it again  so what harm could actually be caused by a collision,0.025130890052356022,24,sse,hash|sha,what s wrong with sha-1 having collisions,4,attacks|malware|vulnerability|man in the middle,0.2879515290260315
36603,google appengine imposes a limit on how long a request may take before it is terminated   there is a  roughly 30 second limit  imposed on how long a request must take before it is terminated however shortly before the process is terminated a   exception will be thrown  additionally there are  per-minute quotas  to prevent the application from consuming too much quota in a short period of time it is very unlikely that your application will exceed the per-minute quota under normal conditions but if needed an increase to this quota  can be requested   while jon handled the low level case of an infinite loop there could also be a situation where one of your handlers is called repeatedly an excessive amount of times - perhaps you accidentally configure something to back up your entire datastore every second instead of once a day  theoretically you could use up a lot of resources even in 30 second chunks  however you would still not be in danger of racking up a huge amount of charges  you have the option of setting a limit on how much you want to spend per day  if you have no quota left your app will return an error not put you into debtor s prison  i m a google employee but have little experience with appengine please don t consider this an official response.  i m guessing you re using the java servlet api - if not please specify  from the  appengine servlet docs      a request handler has a limited amount  of time to generate and return a  response to a request typically  around 30 seconds once the deadline  has been reached the request handler  is interrupted   i don t know how/whether this occurs in a tight-loop which wouldn t allow the vm to interrupt it in normal java,2010-06-01 06:00:18.307 UTC,377,2947551,i am not a google app engine user however i understand you re billed for cpu time and other resources what are the consequences if you happen to create an infinite loop? will google ever terminate it or will you have to do it yourself manually somehow?  i m a hobbyist developer worried about a small error that might end up costing hundreds,0.010610079575596816,4,so,google-app-engine|infinite-loop|infinite-sequence,consequences of an infinite loop on google app engine,1,infinite loop,0.2869081497192383
22377,it is normal to find 2 different files/strings/data with same crc32 there are only 32 bits use md5/sha1-512 to be more protected from duplication  yes they can be the same but that will occur accidentally with a very low probability of 2 -32   as jon noted you can construct strings with the same crc deliberately   my spoof code  automates that  here is an example of another string with the same crc as those presented in the problem but with limited differences from the first string   generated using spoof  yes that s what crcs are like they re  not  unique ids they re  likely  to be different for different inputs but they don t  have  to be after all you re providing more than 32 bits of input so you can t expect to have more than 2 32  different inputs to all produce different crcs  a longer cryptographic hash e.g sha-256 is far  more likely  to give different outputs for different inputs but it s still not impossible and can t be due to the amount of input data vs output data the big difference between a crc and a cryptographic hash is that a crc is relatively easy to steer if you want to - it s not terribly hard to find collisions and it s used to protect against  accidental  data corruption cryptographic hashes are designed to protect against  deliberate  data corruption by some attacker - so it s hard to deliberately create a value targeting a specific hash  as an aside your use of   without specifying a charset is problematic - it uses the platform-default encoding so if you run the same code on two machines with the same input you can get different results i would strongly encourage you to use a fixed encoding e.g utf-8,2014-12-04 10:04:43.73 UTC,354,27291131,my application use crc32 to check two contents or two files are same or not.but when i try it use to generate unique id i see the problem with the two different string the crc32 can be same here is my java code thanks in advance,0.01694915254237288,6,so,crc32|java,different text but same crc checksum,3,attacks|spoofing|protection,0.286724716424942
35478,this is due to 1mb limit on max entity size in my case temp_o.list is a textproperty see  cloud.google.com/appengine/docs/python/datastore  scroll to the bottom of the page,2015-09-08 18:14:23.073 UTC,90,32464568,i am getting the error   for the following code   when i get the size of temp_o.list using the following code   the result i get is   since sys.getsizeof returns value in bytes so this is only 4.2 mb why would saving only 4.2 mb data lead to requesttoolargeerror in google app engine,0.011111111111111112,1,so,google-app-engine|memory-leaks|python-2.7,requesttoolargeerror the request to api call datastore_v3.put was too large,1,memory leaks,0.2864128649234772
25802,i assume this means there is no restriction on the size of message you can send    basically maxreceivedmessagesize is there to protect your server from dos attacks using long messages  there is not the same need for a limit to the size of the response message as that s under your control  a client may of course want to set maxreceivedmessagesize to avoid excessively long message from a rogue server,2009-06-22 15:30:30.437 UTC,138,1027857,"i am surprised to see that no wcf max response message size setting in wshttpbinding? my question is whether there is max response message size setting in wshttpbinding? i am using .net 3.0 + c# + vsts 2008  btw i have found max request message size setting and tested it works  thanks in advance,george",0.021739130434782608,3,so,.net|c#|response|wcf|wshttpbinding,no wcf max response message size setting in wshttpbinding,3,attacks|protection|denial of service,0.2863757610321045
65409,if you use an appropriate cryptographic password hashing function e.g bcrypt scrypt or pbkdf2 the value added by this approach is negligible so really the solution is simply to responsibly hash your passwords in the first place  this is security by obscurity imo the extra step of hashing the user id to produce a password is trivial to recreate once it s known because the usernames aren t stored in hashed form in your user table and because something somewhere has to have read access to both dbs if an attacker can obtain a dump of your user table and get usernames he can obtain a dump of the password table as well and connect the dots the only unknown is the hashing algorithm used and because there are a small number of secure hash functions suitable for the purpose it s trivial to just try them all and don t think for a minute that i m advocating creating your own hash or using an obscure one that s just more security by obscurity  if the hashing method is secure and the password has non-trivial entropy  the top 10 most common passwords  and simple derivations of same have effectively zero entropy as they re the first thing any cracker will try you could spray-paint your password hash on your car and drive it into the middle of def con and you d be perfectly fine that s the point the ideal hash s most efficient attack should be the birthday attack which should still be infeasible with the proper combination of input entropy digest size and computational cost  if you are salting and hashing your passwords properly there is little to no value added by the system you propose  if an attacker can get one database they can probably get both  an added hash on the foreign key does not provide any appreciable increase in security beyond the hashing of your passwords  additionally depending on how you implement such a system it could cause substantial performance losses  take a look at  my answer here  talking about segregating password information combined with a different permissions set that doesn t allow reading but does permit comparison by means of a stored procedure separation of password information would indeed be useful  you can t effectively go about encrypting or hashing key values because the output would be random you d either need a large enough space to avoid collisions and then you d run into painful amounts of database fragmentation where insertions are happening all over the place and pages being split key values should remain incrementing integers for everybody s sanity     i suppose that it might be easy to detect an attack that is using a list of passwords that do exist for users in the system and block or otherwise shutdown the attack   no that would involve checking against the full list of possible hashes which would be expensive if you ve used proper salting and hashing methods  we hash passwords for a specific reason in case an attacker could grab a copy of your server tables we hash passwords to prevent the attacker from  escalating  a partial read-only access into a read-write access see  this blog post  for a more detailed discussion password hashing is already a second layer of defence  your proposal is about adding a third layer it has value only if the attack scenario is that the attacker could obtain a read-only access on  one  table but not on both this scenario is not very realistic though because such read-only access often come from sql injection attacks and server elements which have the permission to read the table of users often have the permission to read the hashed password as well in particular the login system which must by definition use both  so you have to balance the extra safety against the extra complexity because complexity is known to be the arch-enemy of security in that case i don t find the extra complexity to be worth it since the scenario for which the table splitting would add some safety is implausible,2012-12-14 15:15:23,911,25449,the usual method for simple sites is to store a hash of a user s password right in their user record  what if the password field is removed from the user table and a password table is created? the password table would have the same password hash but instead of the user id the key to the table is another hash - a hash of the userid and some secret key  the idea being that if you get a copy of the user table you don t get the password hashes if you get the user and the password table you cannot connect a password to any particular user account  i suppose if you could crack the password hash you d have a handful of passwords you know are in use on the system you could try each password on each account until it works so perhaps i answered my own question nevertheless is there something i m missing? this feels like a good idea but i couldn t find any thing about it  i suppose that it might be easy to detect an attack that is using a list of passwords that do exist for users in the system and block or otherwise shutdown  the attack,0.01756311745334797,16,sse,hash|passwords|sql-injection,is there value in storing passwords in their own table with encrypted or hashed keys,3,attacks|sanitization|sql injection,0.28627991676330566
38909,"it s fine as basic shield from ddos or handling external requests for your api methods that can go-out-of limit.but if you want to prevent real ddos attacks your should check debouncing and event throttling also think about per-machine custom firewall configurations;  dig a bit more into docs of this module ;     burst burst is the number or amount of allowable burst requests before  the client starts being penalized when the client is penalized the  expiration is increased by twice the previous expiration   bursts = base request counter for 1 unit of time defined by default as 1 second or a custom set up  limit     limit is the number of maximum counts allowed do not confuse that  with maxcount count increments with each request if the count  exceeds the limit then the request is denied recommended limit is to  use a multiple of the number of bursts   requests received => check for the limit if limit achieved requester gets a penalty.when you see a lot of requestsmultiple bursts detected.that s real detection for excide of request limit so 5 bursts set 20 as limit when burst detected as 5 it will flag 20 request counter like a fully recognized limitation   maxexpiry     maxexpiry is the seconds of maximum amount of expiration time in  order for the user to use whatever service you are providing again,  they have to wait through the expiration time   and that s it just dive into testing this stuff",2019-05-30 05:50:06.41 UTC,350,56372356,i implemented the  express-rate-limit  npm module on my code nodejsi saw the  ddos module   anyone who have good expertise on nodejs please suggest me that wheher i have to use ddos module or not   i installed the module but this will block the request i read about express rate-limit also this module is also working same as ddos  someone suggest me that use ddos i told that i already used express-rate-limit but he said that use this also  i am confused now please give me the proper input regarding this any help is really appreciate,0.025714285714285714,9,so,ddos|express|rate-limiting,could we used ddos if express-rate-limit is used on code,2,ddos|attacks,0.2859369218349457
38660,this is about event counts the ddos device will log intrusion events which count as  actual negatives  of these further inspection will reveal  false negatives  now you can calculate the  fp rate  as  fp  /  an  and the  tn rate  as  tn / an  or  an-fp / an     what you cannot rate is the number of intrusion events in relation to background traffic as the latter is event-less but this rate is commonly not asked for rather the absolute number of events  an,2015-04-02 15:33:02.077 UTC,160,29416835,"false positive（fp,true negativetn,actual negativefp+tn .we can calculate the false positive rate by  fp/fp+tn ;in ddos detection experiments the false positive happens when we misjudge the legitimate traffic as malicious traffic however the legitimate traffic is usually as background traffic it does not have a specific number how to get the number of  fp+tn how to calculate the false positive  rate of ddos detection  algorithm",0.03125,5,so,ddos|false-positive,how to calculate the false positive rate of ddos detection algorithm,1,ddos,0.2856660485267639
44807,,2016-11-18 10:12:48.273 UTC,83,40674374,i have got a fairly simple function for a live clock on a website used as a dashboard i.e the site should be able to run for weeks without refresh  my code   this function fills up my system memory at a rate of about 1mb per second why is this happening and why is the garbage collector collecting?  my ram is full and the system completely unresponsive after about 30 minutes is this fixable,0.024096385542168676,2,so,javascript|memory-leaks|settimeout,settimeout function leaks memory in chromium,1,memory leaks,0.28473955392837524
480,hashes are irreversible   your password was hashed and  not  encrypted  you should now that laravel uses  one way  hash functions     a one-way hash function is a mathematical function which takes a  plain text input string and converts it into a fixed-length  binary sequence furthermore a one-way hash function is designed in  such a way that it is hard to reverse the process that is to find a  string that hashes to a given value hence the name one-way. a good  hash function also makes it hard to find two strings that would  produce the same hash value   besides     showing a user s password on screen especially in a web app is  likely a security vulnerability and may render the system vulnerable  to script injection screen reader or man in the middle attacks    read more,2020-02-19 17:42:42,209,60306283,i m saving a password value on a site model using the password::make field   i would then like to display this password decrypted when a user with the correct privileges which i already have working presses an inline button show password on the detail view or index view  i ve tried using    but unfortunately this gives me an incorrect payload error  any ideas,0.019138755980861243,4,so,laravel|laravel-nova,how can i show encrypted field value in laravel nova,3,attacks|vulnerability|man in the middle,0.281755656003952
44339,,2015-02-03 23:25:23.93 UTC,135,28310721,on win2k12 running iis8 with 12gb of ram how much memory will a single w3wp process use with no other pressures  i am attempting to isolate where i have an issue it seems like there is a leak somewhere since it grows to about 10gb and then crashes and starts up again it usually hovers between 800mb-2gb then all of the sudden it starts increasing over a 2-5 minute period and it crashes  as i begin to isolate i was wondering all things being equal how much would the process use up with no other pressure - just one site on this server similar to the way sql server will use up all the ram if you let it will w3wp do the same,0.014814814814814815,2,so,iis|memory|memory-leaks|w3wp|w3wp.exe,how much memory can/will w3wp use,1,memory leaks,0.28101861476898193
2555,"nothing in this world is invulnerable to  unlimited computer power  good to know that does not exist such a system    on a real attack scenario the attack on a otp protected system depends on how much attempts the attacker can make in the timeframe of the otp token generally one minute if you employ rate limiting on your authentication scheme employing a fixed delay on every authentication session you are pretty safe    there are no algorythm immune to offline attack there are very resistent ones taking very long times to be broken billions of years but nothing is unbreakable if the attacker can bruteforce your password offline with enough computing power the security depends on your password any data encrypted with a very secure algorythm and protected by a trivial key will be easily decrypted      yes assuming the offline bruteforcing is successful and they have discovered your password they still cannot log into the system without the one-time auth token they will not have the opportunity to bruteforce the one-time auth token    n/a because of the   if not      you have two different questions to answer your first question..     does a one-time-password like google authenticator or yubikey protect against brute force attacks with unlimited computer power?   when talking about brute force attacks you have to differentiate between online and offline attacks  when considering online brute force attacks i refer you to rfc 4226 section 7.3     truncating the hmac-sha-1 value to a shorter value makes a brute     force attack possible  therefore the authentication server needs to     detect and stop brute force attacks      we recommend setting a throttling parameter t which defines the     maximum number of possible attempts for one-time password validation.     the validation server manages individual counters per hotp device in     order to take note of any failed attempt  we recommend t not to be     too large particularly if the resynchronization method used on the     server is window-based and the window size is large  t should be     set as low as possible while still ensuring that usability is not     significantly impacted      another option would be to implement a delay scheme to avoid a brute     force attack  after each failed attempt a the authentication server     would wait for an increased t*a number of seconds e.g say t = 5,     then after 1 attempt the server waits for 5 seconds at the second     failed attempt it waits for 5*2 = 10 seconds etc      the delay or lockout schemes must be across login sessions to prevent     attacks based on multiple parallel guessing techniques   good implementations of hotp and totp which are the algorithms used in google authenticator  should  throttle login attempts thwarting online bruteforce attacks  when it comes to  offline  brute force attacks the attacker has to guess the shared secret used  rfc 4226 section 4 has this to say about the secret     r6 - the algorithm must use a strong shared secret  the length of     the shared secret must be at least 128 bits  this document     recommends a shared secret length of 160 bits   brute force attacks against keys of that length  is considered to be impossible   moving on to your second question..     if not what algorithems are immune to bruteforce attacks against static data on a hhd or usb-stick? consider the data is stored on an usb-storage and you loose the usb-storage to the attacker who has unlimited computer power available to brute force the password   ordinary drives do not encrypt their content in any way loosing the drive means revealing all the data on it you  must  encrypt the data on the drive if you are to have any hopes of it remaining secure some drives have encryption  built in  while you can use software like truecrypt to encrypt others   assuming that the encryption is sound aes with a proper mode of operation and all that... it comes down to the password you use to derive the key from in that situation the encryption is only as strong as your password choosing a long random password makes it  very  unlikely that an attacker will be able to guess it  this is of course assuming that the attacker is  computationally bounded  which is what you will face in reality against attackers with unlimited computing power you will need a very special class of algorithms known to be  information theoretic secure  some examples of such algorithms are the one time pad and shamir s secret sharing  i assume in my answer that the op talks about online attacks since its not quite possible to use otps for offline encryption or harddrives  for the first question it depends on if you use time-based event-based or challenge-based otps.im going to talk about the standarized method of 6 digits otp hotp totp ocra variants of authentication now  time-based they are secure against bruteforcing since the attacker only have 30 seconds to enumerate all codes provided you do not have any synchronization window and provided you have your totp tokens set to change code each 30 second.even adding a delay on 0.5 second on each login attempt regardless of if successful or not which is basically unnoticable to a normal user will only give the attacker a ability to guess the code with a propability of 1/16666 which is far more harder than guessing a atm pin with 3 retries which has a success chance of 1/3333.after 30 seconds the totp code is changed forcing the attacker to begin from the beginning again i would still recommend a captcha to prevent bruteforcing since the attacker could strike the 1/16666 by luck  event-based they are not secure against bruteforcing since the same static code is used all the time until a correct code is presented the only way to secure a event-based login is to either lock the account and require 2 successive codes to unlock or use a captcha a way could be to use both.the 2 successive code system would not be noticable to the end user since the user would on the attempt to login to his locked account just simply get invalid otp code try again with the next code like users who really typed their otp wrong get user would naturally press the button again to generate the next code after his enter this second code the account would have been presented with 2 successive hotp codes and be unlocked without the user noticing the account was locked and without a attacker noticing he entered the correct hotp since the account would be locked on first try thus the attacker would need to have enter 2 successive code but the attacker could never find out when he entered the first code correctly.requiring 2 successive codes would immediately reduce the attacker to having to guess something out ot 1/1000000000000 chance which is equvalient of a 6 character password with a-z a-z and 0-9 also if the attacker enters the first code successfully but the second incorrectly he would have to begin on the beginning again of all those 1000000000000 codes since the first code is invalidated  challenge-based here its dependant on the design since the response is proportional to the challenge its important here to make challenges one-use and expiring if you program your challenge system to only allow one valid challenge for a account at one time make sure each challenge is one-use eg the challenge is invalidated each retry regardless of correct otp specified or not and make sure the challenge is expired when its not used for a time then the system will basically be infinitely secure against bruteforcing since each attempt would cause the system to change the access code however the attacker could strike a 1/1000000 out of luck simply like winning joker on  a exact 6-digit lotto number why you should have captcha on such logins too  google authenticator when used with google or facebook is a totp token eg time-based otp but can also be loaded with hotp profiles event-based   when talking about yubikey using their internal 44-char key or disabling the public identifier giving a 32 char key the key length even if its event-based makes it practically impossible to brute-force a yubikey-protected system since bruteforcing a yubikey response would be equvalient of bruteforcing a 128 bit 32 hex chars aes key so regardless of if you bruteforce the response or bruteforce the internal yubikey storage key your work will be as hard as both a attacker would want to bruteforce the internal key instead since it would gain a greater success  on your question 2 i do not quite understand what you mean since its technically impossible to use otps if the authenticator oracle is not completely secure this means that the process or item that verifies otps must be trusted and using otps in a offline scenario just means the attacker could modify the offline process to allow reuse of old otps and unlimited attempts",2014-10-13 14:27:51,1550,70571,does a one-time-password like google authenticator or yubikey protect against brute force attacks with unlimited computer power?    if not what algorithems are  immune  to bruteforce attacks against static data on a hhd or usb-stick? consider the data is stored on an usb-storage and you loose the usb-storage to the attacker who has unlimited computer power available to brute force the password,0.02903225806451613,45,sse,brute-force|one-time-password,one-time-passwords resist against bruteforce attacks? immune alternative,3,attacks|protection|vulnerability,0.28056973218917847
66264,depending on the os it may be trivial for the keylogger to distinguish different input sources and filter out your random characters and even if that s not the case there are many places aside from the keyboard where a decent trojan will be looking for passwords like ram contents and screenshots  aside from that your method will only work if you introduce lots of random characters let s assume the keylogger has captured a sequence of   characters corresponding to   characters of the password and   random characters added by your method assuming   is known there are   possible passwords for the attacker to try for example if the keylogger has captured 16 characters and the password is known to be 8 character long the attacker will discover the right password in   if the password length is not known the attacker could simply iterate through different possible values for example if in the example above the password is known to be between 8 and 16 characters the attacker will have to try out 39203 possible passwords  only when you add about 250 random characters to your 8 character password the difficulty of cracking the keylogger data becomes equal to brute forcing an alphanumeric password at this point you should make sure you have a good random characters generator because these 250 characters could be enough to crack the trivial ones,2017-03-10 09:53:35,349,153445,i am making a pc application with authentication it is using a salted encryption and works well safety wise still my main concern now is with keyloggers so i thought that maybe i could solve in a very primitive way this problem  i thought that the best way to do so would be to make the application simulate itself as the keyboard and type random characters alongside the user s password i know it is a long shot but i just can t see why it wouldn t work i am talking about software keyloggers not hardware.  can i fool a keylogger by simulating key presses with an app,0.02005730659025788,7,sse,antimalware|keyloggers|malware,can i fool keyloggers by simulating fake key strokes,3,trojan|attacks|antimalware,0.28025734424591064
40221,,2019-01-02 17:53:36.727 UTC,117,54010946,i am not the first one asking about this issue however my problem seems a bit different from what i saw basically i was trying to merge a dataframe   and   and i get a memory error with a   i have 5gb of ram available for the computation.  to try to circunvent it i tried to do it iteratively like below but after the 6 first iterations where the memory computation time increases slightly from one to the other it explodes at the 7th one   any idea ?i am reaching the point where i am going to write those 2 dataframes to a database to do the join there,0.017094017094017096,2,so,memory-leaks|merge|pandas,memory leak merging 2 pandas dataframe,1,memory leaks,0.27796339988708496
19410,there appear to be at least three attacks you might want to worry about   targeted attack at a particular user you want to make logging in  more difficult  for the attackee but not too much more difficult a captcha is sufficient but don t make the user type in the password again if it wasn t displayed on the login page  large-scale attack on many users locking out individual users is a bit pointless since the attacker can just try say 3 passwords and then move on to a different account a captcha per ip might be sufficient but you may also want to rate-limit per-ip or x-forwarded-for for a list of whitelisted proxies this depends on the size of your attacker s botnet a large enough botnet can distribute attacks over multiple bots/sites such that each site gets a low rate from each ip  offline attack on the password database in this case you need at least about 50 bits of entropy even with a good hash ntlm uses a single call of md4 which is  not  a good hash which you can t get in a relatively normal 8-character password 8 log 2 94 is only 52.4   you could store tries-per-ip into a tree where you group dense parts of the tree together then just bucketize it construct a new tree every 10 minutes keep the old tree around for 10 more minutes this has the possibly mistaken assumption that neighbouring ips are likely to exhibit similar behaviour but downgrades gracefully into just clustering the ipv4 into say /24 s  if you re feeling particularly generous you can store a separate cookie on login that s not cleared on logout and save a copy in the database a 128-bit random value should be good enough on a login attempt be nicer to the browser if it presents the correct cookie e.g allow 3 attempts on that cookie without counting per-ip or per-user failure rate this means that the last machine used to access the account isn t presented with a captcha even when the user s account is being bruteforced  in general it s more useful to talk about password entropy than password length and types of characters &mdash i m pretty sure nearly everyone just makes the first letter capital and sticks a 1 on the end i ve also yet to see any human-friendly password generators that also state password entropy  asp.net has a built-in mechanism to prevent brute force attacks against login passwords.refer to the  maxinvalidpasswordattempts membership property   imho 7 character passwords are perfectly adequate for most web applications my bank allows 7 char passwords provided security best practices are followed such as securely hashing passwords and blocking brute force attacks  once you get beyond 7 or 8 character passwords you are really saying my app needs to be super secure in which case you ought to consider individual client ssl certificates requiring more characters in a password has diminishing returns how many of your users can remember complex 8 or 9 character passwords? they end up writing them down personally i get turned away by any site that requires me to create some super-complex password  asp.net membership does most of the hard work around security for you as long as it is setup properly  however there are some things asp.net membership cannot do for you such as   ensuring https is used  preventing csrf and similar attacks  ensuring all web requests are routed to asp.net to prevent static content being served up by iis and bypassing asp.net authentication  checking that the user is a human captcha   for more on security best practices i d look at  owasp   many brute force attacks occur offline that s why failed-attempt lock-outs are no substitute for requiring complex passwords using proper salt and key-strengthening  ip address isn t really a secure method of identifying the user you could try storing the last time a login attempt was submitted in a cookie but if the browser doesn t accept them it ll be of limited use session variables also require cookies so they re out  some sites yahoo comes to mind start showing a captcha form after the third or so attempt you have to correctly answer the captcha in addition to your login details   another option would be to disable an account after x failed attempts which can be tracked in your database but i personally dislike this as it tends to force me to call someone to get my password reset whenever i forget one,2010-08-19 18:47:43.257 UTC,839,3525134,i just read an article saying that passwords with 7 characters are no longer safe however if the server increases the time to retry a login attempt after each login attempt then brute force attacks are useless how do you create such logic in asp.net? somehow i guess the server side code needs to remember the ip-address that tried to login and should increase the response time with each new try,0.02026221692491061,17,so,asp.net|brute-force|iis-7|security,brute force attack failsafe login in asp.net,4,owasp|bypass|attacks|cross site request forgery,0.2772841155529022
65144,i would say no  here  is an interesting article on a similar topic  i would suggest   using a hash function this does not add virtually any security on its own but makes improvements mentioned below much more efficient e.g    adding a long random secret constant this adds security  while the secret constant remains secret  - this value may be very easy read in shared-hosting server e.g    reading some bytes from   if this is available and adding them to value being hashed  this adds real security  without use of php extensions mentioned in question post e.g    add client ip and port this improves security a bit especially if client ip address is not known to attacker e.g    this does not improve security but makes the generation algorythm more understandable - remove uniqid stuff and replace it with pure   e.g    added later add http   request header to input of hash function this has dual effect on security - good part is that csrf token depends also on session cookie so attacker must know/guess victim s session cookie to calculate csrf token - in most cases session cookie is more critical bad part is that attacker may try to guess session cookies based on any csrf tokens saved in browser history to make this guess computationally harder at the expense of server resources   header can be hashed alone before adding it e.g     p.s notice that variable-length digit strings are concatenated using non-digit separators to avoid some non-equivalent string sets resulting in equivalent concatenated values  p.s.2 looks like there is a bracket   missing in question post code  p.s.3 more on meaning of cryptographic hash function here hash function does the following    adds  no entropy  - has functions are deterministic hash function will always produce the same output from each particular input it is important to understand that no entropy is added even if length of string is increased sha-256 from string a will be 32 bytes long but it will still be a well known value you can just google this hash in hexadecimal to find out how popular it actually is  mixes and obfuscates this time obfuscate does not mean something avoidable and insecure the input - the only way attacker can determine the input is by guessing it basically hash function introduces an all-or-nothing principle - either attacker knows entire input and can verify it by computing the hash or attacker does not know which parts of the input he has predicted are correct as a result hash function allows to increase security by adding a long random secret constant step 2 mentioned above without use of the hash function attacker would see the constant in all observed csrf tokens thus he would know what the constant is and that it must be added   very closely related to 2. makes attacker to spend some computational resources with each guess of the input of hash function in article mentioned above the attacker uses gpu video card because he must do large number of guesses and that would take very long time on cpu the more complex less predictable input of the hash function is the more computational resources attacker must use to guess the input if the attacker has enough computational resources for guessing the input of the hash function used to generate csrf token as said before amount of computational resources needed depends on complexity/predictability of the input he can sample the input and start to analyze how it is composed identify constant parts etc    for further reading on application of cryptographic hash functions the following wikipedia articles are relevant  cryptographic hash function   hash-based message authentication code   p.s.4 partial off-topic as mentioned earlier it can be easy for an attacker to extract the constant and generation code by attacking shared hosting environment  don t try to reinvent the wheel and don t try to reinvent a solution for unique random numbers if your language or framework provide a function to generate a random uuid a go with that you may know what you are doing but you have just you debugging and supporting the language or framework has a company or organization behind it to make sure that they get it right   no  i m not a php developer but i found this on the uniqid function you re using   uniqid manual page   warning   this function does not create random nor unpredictable strings   this function must not be used for security purposes  use a cryptographically secure random function/generator and cryptographically secure hash functions to create unpredictable secure ids  please refer to  this thread  on generating secure random numbers in php as it s beyond my scope and largely this is what your question boils down to  if you re getting 128 bits of entropy the encoding or putting it through further hash functions is largely unimportant  the accepted answer seems to be providing snake oil alongside good advice  for the generation of csrf tokens specifically there are only a few requirements   unpredictable  some resistance to brute force  uses printable characters that are compatible with web technology   all you need to do to achieve this is generate enough  cryptographically secure  randomness to prevent a modest brute force attempt compared to say offline password cracking and then base64 encode it  you can see what is used widely by the clojure community  https://github.com/ring-clojure/ring-anti-forgery/blob/master/src/ring/middleware/anti_forgery.clj#l11  as part of the anti-forgery middleware prevents csrf     the key is that the   function being called here uses the cryptographically secure java   class under the hood the   refers to 60 bytes of random data to be base64 encoded  that s it  you can achieve the same outcome by sourcing random data from    here are problems with the other suggestions for the token generation algorithm   usage of   - don t use this at all it s not fit for purpose here and is  exactly  what the linked article in the accepted answer warns against but then the accepted answer goes on to use   in code examples...  usage of   - does not create unpredictable or random strings as per the php docs  using a hash function - does reduce the character set but doesn t add any security base64 encoding is clearer about the purpose and doesn t imply security functionality that simply isn t there  using a secret key - in other contexts secret keys are critical in this context it simply provides you with something you have to keep secret not an easy thing to do adding complexity and difficulty with no extra security benefit  adding client ip/port/etc - doesn t make the token more secure than crypto-random data but does make the algorithm more complicated and less portable  usage of microtime - doesn t add any extra security the current time is predictable enough to be brute forced if we consider   insecure then surely a timestamp is too   essentially anything you do that adds complexity without meeting the fundamental requirements just makes the algorithm   less maintainable  harder to understand  more likely to have a critical bug  less secure,2014-12-24 21:56:26,1315,76809,i am working on enhancing the security of an existing web application which currently has not implemented the use of anti-csrf tokens so it is up to me to generate one add the hidden fields and checks etc a limitation right now is that neither the openssl or mcrypt modules are currently configured on the server  here is my token-generating code   i have a feeling that this would provide an adequate token but not a particularly strong one but that s my own personal instinct does anybody have any suggestions as to how i can strengthen the security of this token without the use of mcrypt or openssl functions? thank you!  edit this is currently how i am verifying the token after the form is submitted where $token is the token passed along with the request,0.018250950570342206,24,sse,cryptography|csrf|php,would this generated csrf token be considered cryptographically strong,2,attacks|cross site request forgery,0.2770472466945648
27306,why would you store the salt within the databse? let s assume someone gets access to your database - he will have the passwords and salts if you had your salt hardcoded or somewhere else he would just have the hashed passwords if the attacker gets full access to your systems this doesn t matter whatsoever  also will you use https for the login precedure?  assuming you have more than one database connection and they have different encodings - you should pass the correct link to your sanitizer function by the way what s the benefit of wrapping mysql_real_escape_string with another function that doesn t add a thing but actually limits the actual function?i d encourage you to use pdo  apologies i cannot comment on m02ph3u5 s post so i have to post another answer storing salts in the database is completely standard the reason that salts exist is to prevent brute forcing with rainbow tables rainbow tables are precomputed hashes which would allow all the passwords in the database to be cracked quickly with salts even ones known to the attacker a rainbow table cannot be precomputed this means people have to brute force each password this is good because if you use an algorithm like blowfish then cracking hundreds of passwords becomes impractical  it isn t a way to make passwords impossible to crack it is a way to make it impractical to crack all the passwords  edit if they gain access to your db they will have only your hashed password and salts they still require the plain text password  edit2 so the idea is they have access to the site so the only thing we want to do at that point is protect the password usually people re-use passwords hashing is really just to protect those kinds of people  i am by no means an expert on this but first off switch to pdo for connecting to mysql it seems a little more difficult but protects you 100% from anyone trying to do sql injections if used properly for encryption i used this to generate a random salt per user   if you feed it a string it spits out a given length salt from the characters in that string then i believe i used blowfish due to the fact that it is not easy to crack  did you consider xss vulnerabilities? i have not yet and need to get on that make sure users aren t injecting html or javascript code into your database that might be displayed somewhere on the page later  good luck  edit:here is what i use for hashing   implemented a similar prototype few months ago in java  from what i see it looks quite similar to what i had some things that i have done or at least planneed to do which i don t see from you explanation   obviously secure the http channel with tls otherwise you would be sending your password in plaintext  use some key streching functions such as  http://en.wikipedia.org/wiki/pbkdf2  so that brute force attacks would be even more computation intensive  i applied the hash several times i used 1024 iterations again to protect against brute force attacks if i would have to do it again i would rather spend time implementing 2  sessions should expire on a last-accessed-time basis   hope that gives you some additional ideas,2012-11-12 18:16:44.233 UTC,998,13349197,so i am looking to design a high security php user system i have decided to begin at the login/session end i would like someone to take a look at my plan and inform me of loopholes and security flaws   this is my first time attempting to create a system on such a level if you find a problem could you send me resources to help me fix it   connecting to the database*i will be using mysql so it s your standard database connect stored in a file outside of the sites root   registration *the registration will be a simple form i am using a function stored in a external file to sanitize the input and protect from sql injection   is there anything i need on this function to secure it up any more?  the password is the to be hashed using either sha-512 or bcrypyt i have done a little research and have seen a lot of people saying md5 is no longer secure enough i want to ensure the best security possible so what should i use to hash the passwords   i have also added a randomly generated salt using the following snippet which this appended to the front of the hashed password and then rehashed the salt is then stored in the database in plain text format should i be thinking of encrypting this for extra security?  when a user goes to log in their entered password will be hashed the salt from the database added and rehashed and then compared to the password stored in the database   if the password matches a session will be created that will then be locked to a single ip address and user agent the session will then expire after a short time the session will only be used to store a login token the username and password will of course not be store in the session   when the user logs in the token will be stored into the database with a username assigned to it so that we can safely pull the users details from the database using the token without the risk storing it in the session  if the password is incorrect the user will not be logged in.. duh i will enforce a brute force protection on this too after three attempts the users ip and user agent will be locked out for 15 mins and after ten failed attempts blocked in the firewall   i feel like i am forgetting something but i can t put my finger directly upon it   any help or advice would be much appreciated,0.018036072144288578,18,so,login|php|system,desiging a php user login system,8,flaws|attacks|hardcoded|protection|sanitization|sql injection|vulnerability|cross site scripting,0.2764618992805481
4564,this attack is similar to the flush+reload one however yours uses cache filling instead of flushes to force cache eviction  refer to part 3 and 4 of this paper : https://eprint.iacr.org/2013/448.pdf   basically because of how modular exponentiation works by knowing the time between each access in memory to the private key you can deduce the key  a long delay between two accesses means a multiplication which is a 1  a short delay between two accesses means a binary shift which is a 0  then you add up all of them and you have a pretty close representation of the key try multiple time and add some statistical magic and you end up with the key  if anyone want to further vulgarize the paper feel free to edit this answer  apparently this topic arouse great interest to many of you after having done some more research i presented the cachebleed attack to a group of scientists last month now i would like to share my results with you and to actually answer my own question  the above three steps of the generic prime+probe attack are correct however the decisive step is missing i.e how to deduce the secret key the attacker is  not able  to perform a dma and he is  not able  to read the data saved in the cache lines this is quite important to understand because otherwise the entire attack would be much easier  the attacker only knows  what cache line  was accessed by measuring time delays futhermore he knows how rsa is implemted and what algorithms are in use openssl uses a fixed-window exponentiation algorithm to compute the message     we need to understand how this exponentiation algorithm works the  handbook of applied cryptography  by menezes van oorschot and vanstone suggests the following pseudo code       please do not confuse the secret key   and the exponent   in the above algorithm as we are only interested in the decryption step   is not the public key but the secret thus   in our case the interesting point is the multiplication in   it uses precomputed multiplier   which actually speed up the exponentiation however  their selection depends  on the secret key   if the attacker knows the index of the current multiplier i.e what multiplier is used he knows some bits of the secret key the  value  of the multiplier is  not of interest    openssl uses the so-called  scatter-gather technique  to avoid cache attacks on cache line granularity it is predictable where the multiplier are stored in total there are 32 multipliers for that reason each needs 5 bits to be identified uniquely the two most significant bits select the cache line while the three least significant bits identify the bin each cache line consists of eight bins   the attacker are able to deduce what bin was accessed during a decryption operation this reveals three bits of the index of the used multiplier and thus partly the private key the missing two bits can be computed due to redundancies in rsa keys  to sum it up no dma is performed the attacker does not read data from the cache the crucial factor is that the cache position partly reveals the secret key this is due to secret-dependent memory accesses similar attacks such as on aes make use of the cache position as well the actual data is not of interest but the position reveals sensible data,2016-06-05 08:07:59,793,126149,there are many scientific publications that deal with cache attacks most recently the cachebleed attack was published which exploits cache bank conflicts on the intel sandy bridge architecture most timing attacks use a similar approach   the attacker fills the cache with same random data he controls  the attacker waits while his victim is doing some computation e.g encryption     the attacker continues execution and measures the time to load each set of his data that he s written to cache in step 1 if the victim has accessed some cache sets it will have evicted some of the attacker s lines which the attacker observes as increased memory access latency for those lines   by doing so the attacker can find out what cache set or even what cache line were accessed by the victim  most papers i ve read automatically conclude that the attacker then has the possibility to deduce the data e.g a secure private key that was written to these cache locations this is what i don t understand  if i know that victim v accessed a certain cache position how do i get his data? many memory addresses map to the same cache position and even if i had the full memory address i doubt that the attacker could perform a dma  as a reference you can take the recently published  cachebleed  attack,0.03278688524590164,26,sse,caching|side-channel|timing-attack,principles of cache attacks,3,attacks|exploit|timing attack,0.276204913854599
6393,as with databases of password hashes the traditional approach to prevent dictionary based attacks is to make computing hashes or in this case verifiers from passwords costly enough  in the case of srp the obvious way to do this is to choose a group which is large enough in any case it must be large enough to resist cryptanalysis as with rsa or dh.  obviously increasing the size of the group will also slow down both clients and servers  another possibility is to replace sha-1 with another hash function as is suggested in section 3.2 of rfc 2945 the salted sha-1 construct used in srp-sha1 can be replaced by another message digest or by hmac or by a password hashing function  in my opinion using a good password hashing function such as argon2 or bcrypt or pbkdf2 instead of sha-1 makes sense doing this increase the cost of dictionary attacks without having any impact on server performance client performance is impacted though. on the other hand this does not improve resistance to cryptanalysis  srp is designed to protect passwords in a threat model which assumes an attacker can gain access to the memory of the server thus srp is designed to be secure in a context where an attacker would have access to your master password  this above also means srp actually competes with password managers the later being a more widely deployed solution srp is mostly useless if your users already use password managers with a unique password for each site  encrypting stored verifiers is essentially the same as peppering a password database this adds quite a lot of complexity whether this improves the security of the system is not obvious  this only makes sense if you assume the attacker has access to the database but not to the server while using srp only makes sense if you assume the attacker has access to the server your master password has good entropy and your encryption code is properly written ivs are properly generated...  regarding your usage of argons2 you should make the parameters fixed use as much memory and processing power as you can afford do not make the parameters depend on the password,2017-10-29 19:27:50,550,172435,we working on a new website we would like to use srp protocol so the salt and verifier are sent to the server and in theory the server store them like that in a database.if the db is leaked attacker could surely brute-force a user password and check with the verifier/salt even with a good kdf like argon2.we would like to prevent this there is a recommended solution please?  my current solution  on restful service start a master password is manually inputted then hashed with argon2 multiple time xor each password s byte to get the number up to 255 this master password hash is used to encrypt any sensitive information with aes-gcm before storing them in the db salt srp verifier  with this method if the db is leaked even with source code or harddrive the attacker can t exploit srp verifier the only attack i have in mind would be to install a rootkit and to dump memory to found the master password,0.02909090909090909,16,sse,account-security|rootkits|srp,there is a recommanded solution to protect srp verifier to be used if the db is leaked,5,leak|attacks|exploit|protection|sensitive information,0.2754683792591095
14549,the login process needs reduce its speed for both successful and unsuccessful login  the login attempt itself should never be faster than about 1 second  if it is brute force uses the delay to know that the attempt failed because success is shorter than failure  then more combinations can be evaluated per second    the number of simultaneous login attempts per machine needs to be limited by the load balancer  finally you just need to track if the same user or password is re-used by more than one user/password login attempt  humans cannot type faster than about 200 words per minite  so successive or simultaneous login attempts faster than 200 words per minite are from a set of machines  these can thus be piped to a black list safely as it is not your customer  black list times per host do not need to be greater than about 1 second  this will never inconvenience a human but plays havoc with a brute force attempt whether in serial or parallel  2 * 10^19 combinations at one combination per second run in parallel on 4 billion separate ip addresses will take 158 years to exhaust as a search space   to last one day per user against 4 billion attackers you need a fully random alphanumeric password 9 places long at a minimum  consider training users in pass phrases at least 13 places long 1.7 * 10^20 combinations  this delay will motivate the attacker to steal your password hash file rather than brute force your site use approved named hashing techniques  banning the entire population of internet ip for one second will limit the effect of parallel attacks without a dealy a human would appreciate  finally if your system allows more than 1000 failed logon attempts in one second without some response to ban systems then your security plans have bigger problems to work on  fix that automated response first of all  as per discussion above sessions cookies and ip addresses are not effective - all can be manipulated by the attacker  if you want to prevent brute force attacks then the only practical solution is to base the number of attempts on the username provided however note that this allows the attacker to dos the site by blocking valid users from logging in  e.g   note that as written this procedure leaks security information - i.e it will be possible for someone attacking the system to see when a user logs in the response time for the attackers attempt will drop to 0 you might also tune the algorithm so that the delay is calculated based on the previous delay and the timestamp on the file  hth  c  cookies or session-based methods are of course useless in this case the application has to check the ip address or timestamps or both of previous login attempts  an ip check can be bypassed if the attacker has more than one ip to start his/her requests from and can be troublesome if multiple users connect to your server from the same ip in the latter case someone failing login for several times would prevent everyone who shares the same ip from logging in with that username for a certain period of time  a timestamp check has the same problem as above everyone can prevent everyone else from logging in a particular account just by trying multiple times using a captcha instead of a long wait for the last attempt is probably a good workaround  the only extra things the login system should prevent are race conditions on the attempt checking function for example in the following pseudocode   what happens if an attacker sends  simultaneous  requests to the login page? probably all the requests would run at the same priority and chances are that no request gets to the increment_attempt_number instruction before the others are past the 2nd line so every request gets the same $time and $attempts value and is executed preventing this kind of security issues can be difficult for complex applications and involves locking and unlocking some tables/rows of the database of course slowing the application down  imho defense against dos attacks is better dealt with at the web server level or maybe even in the network hardware not in your php code  cballuo provided an excellent answer i just wanted to return the favor by providing an updated version that supports mysqli i slightly changed up the table/field columns in the sqls and other small things but it should help anyone looking for the mysqli equivalent   the short answer is do not do this you will not protect yourself from brute forcing you could even make your situation worse  none of the proposed solutions would work if you use the ip as any parameter for throttling the attacker will just span the attack across a huge number of ips if you use the sessioncookie the attacker will just drop any cookies the sum of all you can think of is that there is absolutely nothing a brute forcing attacker could not overcome  there is one thing though - you just rely on the username that tried to log in so not looking at all the other parameters you track how often a user tried to log in and throttle but an attacker wants to harm you if he recognizes this he will just also brute force user names  this will result in almost all of your users being throttled to your maximum value when they try to log in your website will be useless attacker success  you could delay the password check in general for around 200ms - the website user will almost not notice that but a brute-forcer will again he could span across ips however nothing of all this will protect you from brute forcing or ddos - as you can not programatically  the only way to do this is using the infrastructure  you should use bcrypt instead of md5 or sha-x to hash your passwords this will make decrypting your passwords a lot harder if someone steals your database because i guess you are on a shared or managed host  sorry for disappointing you but all the solutions here have a weakness and there is no way to overcome them inside the back-end logic  i generally create login history and login attempt tables the attempt table would log username password ip address etc query against the table to see if you need to delay i would recommend blocking completely for attempts greater than 20 in a given time an hour for example  you can use sessions anytime the user fails a login you increase the value storing the number of attempts you can figure the required delay from the number of attempts or you can set the actual time the user is allowed to try again in the session as well  a more reliable method would be to store the attempts and new-try-time in the database for that particular ipaddress  you have three basic approaches store session information store cookie information or store ip information  if you use session information the end user attacker could forcibly invoke new sessions bypass your tactic and then login again with no delay  sessions are pretty simple to implement simply store the last known login time of the user in a session variable match it against the current time and make sure the delay has been long enough  if you use cookies the attacker can simply reject the cookies all in all this really isn t something viable  if you track ip addresses you ll need to store login attempts from an ip address somehow preferably in a database  when a user attempts to log on simply update your recorded list of ips  you should purge this table at a reasonable interval dumping ip addresses that haven t been active in some time  the pitfall there s always a pitfall is that some users may end up sharing an ip address and in boundary conditions your delays may affect users inadvertantly  since you re tracking failed logins and only failed logins this shouldn t cause too much pain   or as suggested by cyro   it s a bit rough but the basic components are there if you refresh this page each time you refresh the delay will get longer  you could also keep the counts in a database where you check the number of failed attempts by ip by using it based on ip and keeping the data on your side you prevent the user from being able to clear their cookies to stop the delay  basically the beginning code would be   store fail attempts in the database by ip since you have a login system i assume you know well how to do this.  obviously sessions is a tempting method but someone really dedicated can quite easily realize that they can simply delete their session cookie on failed attempts in order to circumvent the throttle entirely  on attempt to log in fetch how many recent say last 15 minutes login attempts there were and the time of the latest attempt    you cannot simply prevent dos attacks by chaining throttling down to a single ip or username  hell you can t even really prevent rapid-fire login attempts using this method       why?    because the attack can span multiple ips and user accounts for the sake of bypassing your throttling attempts   i have seen posted elsewhere that ideally you should be tracking all failed login attempts across the site and associating them to a timestamp perhaps   a quick note on the ip_address field you can store the data and retrieve the data respectively with inet_aton and inet_ntoa which essentially equate to converting an ip address to and from an unsigned integer   decide on certain delay thresholds based on the  overall  number of failed logins in a given amount of time 15 minutes in this example  you should base this on statistical data pulled from your   table as it will  change over time  based on the number of users and how many of them can recall and type their password     query the table on every failed login attempt to find the number of failed logins for a given period of time say 15 minutes     if the number of attempts over the given period of time is over your limit either enforce throttling or force all user s to use a captcha i.e recaptcha until the number of failed attempts over the given time period is less than the threshold         using recaptcha at a certain threshold would ensure that an attack from multiple fronts would be stopped and normal site users would not experience a significant delay for legitimate failed login attempts,2010-01-19 03:42:08.367 UTC,1899,2090910,i was just reading this post  the definitive guide to form-based website authentication  on preventing rapid-fire login attempts    best practice #1 a short time delay that increases with the number of failed attempts   like  1 failed attempt = no delay 2 failed attempts = 2 sec delay 3 failed attempts = 4 sec delay 4 failed attempts = 8 sec delay 5 failed attempts = 16 sec delay etc    dos attacking this scheme would be very impractical but on the other hand potentially devastating since the delay increases exponentially   i am curious how i could implement something like this for my login system in php,0.018957345971563982,36,so,honeypot|security|throttling,how can i throttle user login attempts in php,8,leak|ddos|bypass|attacks|weakness|protection|race condition|denial of service,0.27489450573921204
65905,if the machine is compromised - there is nothing that you can introduce to make it harder to capture - they own the box the memory and the os    from what little i understand of ruby as well the choice of random is wrong for anything to be secured as it is only a pseudo-random number generator and instead you should be using securerandom  with securerandom you won t need to try and find other sources of entropy,2018-01-31 23:54:02,181,178845,suppose we are on a compromised computer for example infested with trojans logging key presses or taking screenshots and for some reason we have to generate a number that s supposed to be secret for the purpose of using in a script   using ruby when i do something like   the random number generator is seeded with the current time process id and a sequence number considering the characteristics of these kinds of malware can you think of any better sources of entropy we can introduce to the seed which are harder to capture,0.016574585635359115,3,sse,keyloggers|random|trojan,generating a secret on a compromised computer,2,trojan|malware,0.2747843563556671
47355,,2018-02-17 11:43:45.67 UTC,184,48840889,i am using   to prepare features for sequence labeling task the most important feature is an array containing ids of characters per input token for example    since it is a multidimensional array i used varlenfeature and tf.sparse_tensor_to_dense for parsing in   as suggested in  https://stackoverflow.com/a/43672956/9373240   i feed this dense vector to trainable tf.nn.embedding_lookup and then feed bidirectional dynamic gru with this embeddings  during training memory usage increases with batch size goes up to 30gb for batch size of 256 also with small batches training craches with big step number i am guessing there is a memory leak but i cannot find the cause/reason runmetadata shows a huge memory usage by gradients on the bi-gru  also i am getting userwarning converting sparse indexedslices to a dense tensor of unknown shape this may consume a large amount of memory but i am not sure whether is connected with the problem i am guessing it is caused by mask used later in the code to account for padding  any help would be greatly appreciated,0.016304347826086956,3,so,memory-leaks|python|rnn|tensorflow,tensorflow tfrecorddataset + embedding_lookup + dynamic rnn -> memory leak,1,memory leaks,0.2740318775177002
17437,sha-256 suitable for password encryption?   by itself no when used as a primitive with additional controls yes     sha-256 encryption vulnerable to known-plaintext attack? for instance if they know what password gets hashed into they d just have to do a simple string match to know which accounts use that particular password    that s a different security property : its called the prp-notion of security and a hash lacks it dan bohen has a good slide deck on it at  prps and prfs   a hash is a prf pseudo random function when using a hash an attacker  can  distinguish between an oracle s real output and random output as you observed  a hmac is also a prf pseudo random function and it holds the notion of prp security because an attacker cannot distinguish between a real answer and a random answer when he/she queries the same oracle the hmac s key ensure it  so your first task is to use an hmac based on sha-256 and not sha-256 directly :  see the  secure password storage cheat sheet  and  secure password storage paper  john steven wrote for owasp it takes you through the entire threat model and explains why things are done in particular ways,2014-01-08 23:53:44.947 UTC,315,21009072,i am implementing a user profile registration on my server where i have to store user accounts and passwords i m trying to store the password encrypted as a security measure and someone recommended that i use sha-256   i m a bit of a beginner when it comes to cryptography but in a scenario where a hacker accesses my database and steals a whole bunch of encrypted passwords isn t sha-256 encryption vulnerable to known-plaintext attack? for instance if they know what password gets hashed into they d just have to do a simple string match to know which accounts use that particular password,0.022222222222222223,7,so,cryptography|encryption|sha256,sha-256 suitable for password encryption,3,owasp|attacks|vulnerability,0.27277976274490356
35163,,2019-05-30 17:03:45.2 UTC,77,56382602,i have integrated hazelcast 3.11 as distributed cluster and i have used it for the cache the json files i haven t configured any maximum values so it is just default settings problem is when i feed 1m data to this cache it fails with jvm crashes on the nodes in the cluster  this works fine when we restart the nodes of course,0.025974025974025976,2,so,hazelcast|java|memory|memory-leaks,hazelcast cluster getting jvm out of memory leak with cache,1,memory leaks,0.27151769399642944
15950,i generate by this code   this is a dovecot configuration issue dovecot knows two hash encodings the traditional hex encoding ie   and base64-encoding ie   the latter is more space-efficient when stored as strings and default in dovecot an example for generating the hash with     and   encodings     use   if you create hex-encoded passwords hashes in java the better solution would be to use dovecot s   encoding instead of setting the   though doing so you can easily change/upgrade the hash method later without invalidating all user s passwords at once an example for the hash you used in this scheme     finally plain hashing of passwords is  never  save also not when using large sha512 hashes never store unsalted password hashes you re vulnerable to rainbow table attacks if the database leaks,2016-09-12 07:15:35.77 UTC,175,39445102,centos6.6 postfix dovecot 2.0.9 and mysql 5.1.73  dovecot configuration     mysql database   the password is generated by   java code   now i wrote some java-code to authenticate   but it fails with following error   how can i fix the authentication,0.017142857142857144,3,so,dovecot|java|mysql|postfix,sasl login authentication failed ugfzc3dvcmq6,3,leak|attacks|vulnerability,0.2711242735385895
49255,"you could try something like this   slicing a query set does not load all the objects in memory only to get a subset but adds limit and offset to the sql query before hitting the database  you could try to iterate the queryset in batches see the     method see if that improves anything   here is a related  answer  i found but it is a few years old   try breaking it into small blocks since you have only 4gb of ram   when its necessary i usually use a character or number id enting in 1,2,3,4 etc",2019-04-04 18:05:05.437 UTC,185,55522402,i work in a company that has a large database and i want to perform some update queries on it but it seems to cause a huge memory leak the query is as follow     i wrote this in the interactive shell of django   i even tried to use    but it didn t work do you have any idea how can i detect the source of   the dataset i am working on is about 27 million  fixed_date is a calculated property,0.016216216216216217,3,so,django|memory-leaks|python,simple query causes memory leak in django,1,memory leaks,0.27028119564056396
59149,,2016-01-21 14:47:11.11 UTC,219,34926845,i m investigating what looks like a memory leak under ie the amount of memory consumed by ie keeps growing and growing up to 1 gb and more..  to solve the problem i started having a look at the memory panel in the ie 11 developer tools there i see that the browser s total memory grows from ~300 mbs to ~1.3 gb over ~20 minutes   however the heap snapshots that i take are only ~15 mbs to ~17 mbs each:      so i don t even feel inclined to have a look at them because 15 mbs is tiny compared to 1 gb therefore i don t know where to find the cause of the problem  frankly my conclusion is that  either it s a memory leak of the browser and not the page itself or i m completely misinterpreting what the developer tools are telling me  actually in the official  developer tools documentation  the described dt use case also shows a big difference between total memory and heap snapshot size how to investigate bugs in the non-heap area if ie does not seem to show what it s used for?  p.s by the way with add-ons disabled the behaviour is exactly the same,0.0136986301369863,3,so,ie-developer-tools|internet-explorer|memory-leaks,why is ie11 total memory so much larger than heap snapshot size,1,memory leaks,0.2691936790943146
2139,there are mani thing wrong with your code you never dispos your command that s a nativ handl to an odbc driver wait for the garbag collector to dispos it is veri bad practic you shouldn t be send those command individu anyway either send them all at onc in one command or use transact to group them togeth thi one is the reason why it s get slower over time will read the same file over and over and ignor a grow amount of line everi attempt and so grow slower and slower as the number of line ignor but process get larger and larger to fix #3 simpli creat the enumer onc and iter it in batch in pure c# you can do someth like or if you re use morelinq which i recommend someth like thi,2020-06-19 16:48:46,264,62474983,i m run a veri simpl function that read line from a text file in batch each line contain an sql queri so the function grab a specifi number of queri execut them against the sql databas then grab the next batch of queri until the entir file is read the problem is that over time with veri larg file the process start to slow consider i m guess there is a memori leak somewher in the function but can t determin where it may be there is noth els go on while thi function is run my program skill are crude at best so pleas go easi on me,0.015151515151515152,4,so,c#|memory-leaks|memory-management|sql,possibl memori leak in simpl batch file process function in c,2,memory leaks|bad practices,0.269022136926651
43341,you should convert the large number to   before doing the division i.e  note the end of brackets,2017-11-18 10:30:31.867 UTC,139,47365022,i m trying to store first 1000 bernoulli numbers in a dictionary in python at first i just stored the numbers as it is so i got an overflow error now after going through previous answers i thought of using decimal module  so here it is   the 260th bernouli number i was able to store all the previous ones in the dictionary  this is the sample code i ve written   this is the error snap shot       is there any better way to handle such huge numbers ? please tell me if there is some thing that can be done to store these numbers,0.02158273381294964,3,so,bernoulli-numbers|decimal|integer|integer-overflow|python,storing bernoulli number is giving overflow error in python even after using decimal module,2,overflow error|integer overflow,0.26891323924064636
28615,,2015-11-16 17:36:22.107 UTC,163,33741335,i m using d3.js to plot a grouped bar chart where data for each series bars at position n in each group comes from a separate server request via d3.json  in the callback for each server call i process the data and set x/y axis domains to newly calculated maxes  however somewhere in here i have a race condition that causes the bars not always get rendered against the correct maxes and it can take a few page reloads to get the chart show up correctly  i don t really understand how javascript d3 and asynchronous callbacks work together and how parallel processing really works i m using chrome for dev any pointers to relevant literature are appreciated  should i abandon one request per series strategy and combine the data on the server side?  i was hoping to make this more dynamic so that in the future i could add/remove series on the fly,0.018404907975460124,3,so,d3.js|javascript|race-condition,race condition plotting series with d3.js,1,race condition,0.2682301998138428
56188,,2017-12-16 15:17:39.05 UTC,176,47847080,i was processing a huge csv in chunks and noticed that it is gradually increasing memory after a lot of print quit and profiling i think it is because of how pandas is creating new dataframes without deleting the previous one when reading from csv  i used   to profile  below i just read from csv 4 times iteration and it shows me the size of   keeps on increasing    this gives me below stats   i am processing 6 files each over 10 gb in 6 process through shell and multiple subprocesses through multiprocessing pool according to my calculation each chunk i read is almost 2gb so for lets say 12 process it should be 24 gb and add overhead 30 35gb. but my  script is gradually increasing in memory and on random intervals it drops huge percent too but when it reaches 60gb which is my server memory the whole script get killed  this looks like there is some cleanup going on internally which is not linear,0.011363636363636364,2,so,memory-leaks|pandas|python,python pandas memory leak with read csv,1,memory leaks,0.26600903272628784
37695,from what i can tell this is should be expected  the file citylots.json is a 167 mb file consisting of a single object the aesonparse program is building the entire object in memory and that explains the memory ramp in the profile  by contrast the files companies.json and enron.json at  http://jsonstudio.com/resources/   are line-oriented json files - each line is a json object and there are no commas between the objects when you run aesonparse on either of these files it is only reading the first line,2015-11-15 19:29:22.613 UTC,190,33723999,i ve been playing around with the  aeson  parser s benchmark suite and got some surprising results comparing their strict parser and the lazy one    on all datasets that come with the  benchmark  the strict parser always takes more time this is expected but never saves any space   running the     benchmark on     takes dramatically more time and space than other datasets of comparable size    to get the benchmark running    here are some profiles i got with   turned on   from the profiles the leak traced back to   in   anyone knows what s going on,0.015789473684210527,3,so,aeson|haskell|json|memory-leaks,aeson benchmark space leak ? on citylots.json,1,memory leaks,0.2657727599143982
12016,,2016-05-08 13:09:29.463 UTC,316,37100209,there is rsa signature check which has following properties   rsa modulus n which is 2048-bit  rsa exponent equal 41 0x29  signature uses emsa-pkcs1-v1_5 padding of sha1 hash however the decoder only compares 20 least significant bytes which are the sha1 hash   this means that all but the 20 least significant bytes after rising forged signature to 41st power modulo n can be garbage  i have checked following    bleichenbacher s rsa signature forgery based on implementation error  is not applicable here as it deals with most significant bytes least significant bytes are garbage and as i tested it cannot forge signature that would contain 20 bytes of arbitrary chosen prefix when e=41  yutaka oiwa kazukuni kobara hajime watanabe a new variant for an attack against rsa signature verification using parameter field - rsa modulus is too small for this method to be effective here this is the attack that intel used in  berserk    is there any computationally feasible algorithm that could be used to forge signature that verifier which only checks the 20 least significant bytes would accept?  edit:is the  poncho comment from another rsa attack question  feasible here? if so how to implement  x=md5message find a 128 bit number y with   which will exist if x is odd and can be found in 128 steps  the changes in this case would be   41 instead of 3  sha1 instead of md5  180 instead of 128   edit2:i think the poncho comments mentions exactly the same what is implemented in   presented in  berserk vulnerability part 1   this function however as noted previously gives too big values is the result of   the lowest possible number fullfilling the goal?  edit3 forge_suffix_odd returns the lowest possible number fullfilling the goal  edit4  greg martin answered my question at mathematics  and provided very useful solution for sha1 checksums whose least significant bits equal 0,0.0189873417721519,6,so,cryptography|rsa,rsa signature forgery with e=41 and broken padding,3,forgery|attacks|vulnerability,0.2645023763179779
3377,"yes ​ yes  b interacts with a and the challenger as follows b forwards all queries from a to the challenger, and gives the same output as a for each response from the challenger b truncates the response and sends the truncated response to a a s view in that interaction is identical to a s view in the interaction for your scenario with the same key so if a finds the secret key then so does b  b is a constructive reduction from finding the secret key for hmac to finding the secret key for truncated hmac ​ therefore if an attacker cannot find the secret key for hmac then an attacker cannot find the secret key even though the hash is truncated  note that you ll need something to defend against reuse of old balance-tag pairs      truncating a well designed cryptographic  prf  should not result any security weaknesses other than a reduction in the  codomain  s size as prfs  by definition  are pseudo-random across their domain  all other things equal using a more modern hash algorithm sha-256 is safer than using an older one sha-1.   say you want 2^16 instead of 2^256 this is a  phenomenal  drop in tag space size at 2^16 or 2^32 a random tag will have a 1/2^16 or 1/2^32 respectively chance of being accepted whilst qr codes are less responsive at longer sizes there is a tradeoff of tag length versus forgery probability.    details matter  as @ricky points out showing truncated hmac values won t help the attacker finding the secret key compared to a similar situation where the hmac values would not be truncated this makes sense the truncation only removes information   however  the attacker is not  ultimately  after the secret key what the attacker really wants is to make  forgeries  i.e compute some hmac values that the computer will accept as its own finding out the secret key allows making forgeries but the ability to make forgeries does not necessarily entail knowledge of the private key   fortunately  hmac tends to behave like a  prf  so there are two known methods to make a forgery    perform a brute force enumeration of possible keys until the right one is found one that matches a few known value+hmac pairs this attack is defeated by choosing the key is in space large enough to make the enumeration unrealistic i.e you generate a 128-bit key with a cryptographically strong prng    luck the attacker sends a random sequence of bits as hmac value and hopes that it will work this attack is defeated by not truncating hmac values to too small a length and/or applying other mitigations such as allowing only one try and irrevocably rejecting devices that once showed even a single bogus hmac value as you do    the luck attack works with probability 2 - n   where  n  is the size in bits of your truncated hmac output i.e with a 32-bit output then a luck-based forgery has probability 1 in 4 billions or so to go undetected this is the best that can be done with a 32-bit output in all generality that hmac behaves as a prf means that it is optimal in that respect if you truncate to 8 bits then the attack success probability is 1 in 256 which is probably too much for comfort but this is your decision to make based on your usage context  also again as @ricky points out beware of  replay attacks  a valid mac assuming that there was no forgery only demonstrates to the computer that it saw and approved the message contents at some point -- but not that it was the  last  message that it saw old message-hmac pairs can be sent again by a fake device to defeat replay attacks you must maintain some state on the computer the computer must remember something about the device e.g a message sequence number that is part of the data over which the mac is computed and incremented for each message",2015-07-08 20:29:23,859,93445,i have data stored on a device which i must guarantee integrity for ease of explanation the data is like numerical field representing  credit balance available   the device only ever interacts with a single computer i therefore wish to verify integrity of the balance using a hmac the computer reads the balance and mac verifies it using its private key updates the balance and computes a new hmac and writes this back to the device   the limitations i want the hash to only be 32bits so i will truncate it the key can be arbitrarily long and i can use sha1 etc   am i safe to assume that an attacker cannot find the secret key even though the hash is 32 bits? what if the hash was 8 bits?  in this system the computer will block a device if it provides an invalid mac thus an attacker has only one attempt to provide a valid message/mac combination   note i ve read  this security.se question  but i am confused by the answer and the  3rd comment on the answer by @lateralfractal  otherwise it is a similar question,0.025611175785797437,22,sse,hash|hmac,determining strength of truncated hmac,3,attacks|forgery|weakness,0.26401928067207336
50977,,2015-02-16 17:58:45.977 UTC,81,28547527,given this scenario   memory consumption has a rapid peak reaching about 300mb and at the end of the process is not been released if i then force a gc.collect command it drops returning to starting level this is an issue because when i add more commands inside the loop the memory consumption grows even larger obviously causing out of memory in a 32bit process any reason why this is happening,0.012345679012345678,1,so,memory-leaks|stackexchange.redis,firing multiple commands in stackexchange.redis causes large memory consumption,1,memory leaks,0.26364168524742126
65921,can someone explain how this was possible    it was possible due to a lack of rate-limiting on the action/request as you said it could be brute-forced hence  instagram has now patched this issue i haven t seen any official publication on the mitigation measures but a quick visit to  instagram s password reset page  reveals the [intended] user is now sent a link      and how to mitigate it?   you could rate-limit the action/request - not only by ip address but actually upon the action itself however this would pose a potential dosdenial of service attack - hence why instagram probably did not limit it as such in the first place for one given it is a 6 digit code given to the user you could significantly reduce the rate-limiting threshold per ip address i.e from 200 to 20 to increase significantly the cost of the attack  but put simply the 6 digit code itself is flawed although the dos attack may be viewed as  unlikely  it carries relatively small cost given the potential gains/motives for the attacker so it should be mitigated.hence as instagram have - the easiest solution is to verify the user via more secure means e.g  send a link to the user via external previously established communication channels and then get them to input the code at this unique link  make the code 10 digits alphanumeric and set a 30 minute expiration &amp rate-limit the number of codes that can be issued repeatedly   etc,2019-07-19 23:26:42,409,213818,"i am new to the field of security i thought a lot about the following scenario and could not come up with a good solution  1 suppose i go to instagram forgot password page i enter a persons mobile number whose account i want to hack  2 it sends a 6 digit code to that person  3 i want to brute force it - around a million combinations  4 i get 5000 ips and send 200 requests per ip and brute force it i,e sending requests concurrently from all the ips there is a race condition here right so wouldn t i be able to break in?  as the requests are going concurrently how can you keep a count and block the user after few failed attempts? that is what this guy did l  https://www.forbes.com/sites/leemathews/2019/07/15/hacker-discovers-a-simple-way-to-hijack-any-instagram-account/#56e3c87e7b64  can someone explain how this was possible and how to mitigate it",0.02444987775061125,10,sse,authentication|multi-factor|race-condition,how i could have hacked any instagram account - the zero hack,5,flaws|hijack|attacks|race condition|denial of service,0.26207903027534485
21294,it s a good question and yes you should consider taking steps to ensure that the time taken to return a  logon denied  response for an invalid username is comparable to that for a valid username with incorrect password  timing attacks have been used in the case where the username is known in order to discover the hash of and eventually the password itself by analyzing the amount of time it takes the server to do the string comparison hence the use of a  slow_equals  function to compare the hashes and which does not exit early on finding a mismatch  upon a logon failure you should never report back whether it s the username or password that was incorrect as doing so would aid an attacker to discover valid usernames though you should probably inform the user why you won t tell them whether it s the username or password they got wrong as it tends to infuriate them otherwise  a random attacker needs to discover valid usernames and then the passwords for them  if you help them discover valid usernames they are half way there  so if they can discover valid usernames according to the time it takes your logon script to return a failure you would be helping them  let s consider the process:1 database query to retrieve hash and salt;2 hash password with salt;3 compare retrieved hash with generated hash from supplied password    does the query exit early when it finds the matching record?  it may do if we specified unique key for the username column so we probably shouldn t do that and instead have other mechanisms in place to avoid duplication when we modify the db    do we perform these subsequent steps if the db query found no record?  does your hashing process take a little bit of time?  it thus follows that a  default  hash and salt should be used if the username is not matched in order to complete the process in constant time [and this is the reason for the exception handler code in the q]    use  slow_equals  here    an answerer here suggested that network latency variance makes this a non-issue  i contend that an attacker does not need a 100% success rate and that statistical analysis will be revealing  you can t be too careful  the original timing attack the  tenex  attack had nothing to do with hashing -- it worked by positioning the password so that it crossed page boundaries leading to a virtual memory cache miss  the attacker could try a series passwords positioned so that only the first character was in the first page and it would know that the first character matched when verification took long enough for a cache miss to have happened  the attacker could repeat that for each character in the password  unless the attacker has control over fine-grained positioning of inputs in memory no timing attacks on passwords are not an issue but any secret checking algo that is super-linear w.r.t the length of the secret >= olength of secret might leak information about the password length  if you are careful to compare all characters in the password regardless of success then you also defeat the attack   you should have tests that make sure that compiler optimizations don t put timing vulnerabilities back into your code  note the term timing attack is also used in other contexts and those can affect web applications  for example when a system clock is used to construct a  covert channel  between two processes that should not be able to conspire -- javascript loaded in two different origins could establish a low bandwidth channel by using an interval to check the time and looping to consume processor or not to communicate a bit  might not be a definitive answer but it seems like you might be able to ensure that all user name checking operations took the same amount of time make them go into a wait state after they re finished until a predetermined amount of time has passed,2011-11-06 20:08:21.767 UTC,794,8030017,it seems that  for a while  the login utility on unix systems only calculated a hash when a valid username existed this opened a security flaw which allowed for a timing attack as the user could tell when a username was found by the amount of time it required to generate hashed key for comparison   this makes sense for desktop applications but would it make sense for web applications too? i d lean toward doing it but is this kind of fix necessary?   for example in a django auth module   would the additional hash computation make sense for this scenario? if i employ rate-limiting on auth calls does this decrease the possibility of a timing attack like this,0.022670025188916875,18,so,security,preventing timing attacks,4,leak|flaws|attacks|vulnerability,0.261771559715271
40195,,2017-07-06 14:42:01.1 UTC,139,44952105,i have an application where i am collecting linux system information periodically  every 1~2 minute using a system command when i check on my process list using   the memory usage percentage keep increasing when the program start the percentage used was 0.5% and the next day when i check it was 1.4% in a server having 8gb of ram i can understand it is abnormal  to illustrate the behavior i created the following snippet with fastest looping time and only the involved pieces of code when i run i can see memory usage percentage is going up very fast 0.5% to 1.0% in less than 15 minutes   is it a memory leak or something else? is there anything i am missing out here please guide me guys,0.014388489208633094,2,so,java-8|memory-leaks|runtime.exec,memory usage keep increasing on continues looping with runtime.exec,1,memory leaks,0.26157620549201965
201,"the performance of your pc isn t really an issue here your modulus  n  has 179 digits 594 bits which would take an  &nbsp;e&nbsp;x&nbsp;t&nbsp;r&nbsp;e&nbsp;m&nbsp;e&nbsp;l&nbsp;y&nbsp  long time to factor on a single desktop pc in 2005 it took  15.2 cpu years  to factor a 176-digit number by comparison  the question you linked to  only has a 256-bit modulus which can be cracked in a few minutes using software like  msieve   the only way you stand a chance of solving this problem is by finding a short cut perhaps the problem contains other clues that you overlooked or perhaps it uses a flawed implementation of rsa for a general overview of vulnerabilities in rsa i would suggest reading  twenty years of attacks on the rsa cryptosystem  by dan boneh  thanks for your help! i kown the difficulty to factor a big integer but this big number appeared in a ctf question.so i think there must be some method to crack it in the limited time and finally i found the method.the n c e mentioned above is just a half information and another half of info i got by sql-injection and they are n c1 e1 so the final method to crack it is common modulus attack.so i can get flag by given n e1 e2 c1 c2 without the d1,d2n=d1*d2...thank you guys anyway!   i tried with latest  rsactftool  version and it works    spoiler i m the dev of rsactftool",2020-03-28 03:42:11,290,60896984,giving n e c as follows   i have tried several methods to decrypt it like  factordb  yafu and even found a similar  question  which was solved by  rsactftool .maybe the performance of my pc is suck...could anyone help? thanks a lot,0.017241379310344827,5,so,cryptography|ctf|encryption|rsa,ctf rsa decrypt using n c e,4,flaws|attacks|vulnerability|sql injection,0.26080432534217834
12156,rsa encryption is based on the rsa  trapdoor  permutation a transformation which is easy to perform in one direction but infeasible in the other direction unless you know a “magic” value the private key turning a trapdoor function into an encryption scheme is not easy for a start it s vital that the encryption is  non-deterministic  otherwise if someone could guess what a message might be they would be able to check their guess by performing the encryption with the public key and comparing with the ciphertext they want to break to avoid this encryption schemes always include some random part called a  nonce   the rsa trapdoor operation takes input which has a fixed size determined by the key basically with an  n -bit key you need an  n -1-bit input string so to encrypt something you need to build a string of the required length from the original message the operation to assemble the payload to encrypt and the nonce and format them in a certain way is called  padding  there are two standard padding mechanisms for rsa encryption pkcs#1v1.5 and oaep  including a random nonce is necessary but not sufficient to make rsa encryption secure the mathematical operation at the root of rsa has some “nice” mathematical properties in many real-world situations an attacker who wants to decrypt a ciphertext can  submit modified ciphertexts for decryption  and even if those ciphertexts are rejected as invalid exactly how they re rejected error messages timing … can be enough to reconstruct the plaintext this is a problem with pkcs#1v1.5 padding which is vulnerable to oracle attacks such as  bleichenbacher s attack  and  manger s attack  where the attacker generates modified ciphertexts and obtains information about their validity which eventually allows reconstructing the plaintext  oaep is designed to make it impossible for the attacker to generate useful ciphertexts in a nutshell it works by masking with xor the payload with a pseudorandom string the standard way to generate this pseudorandom string is with a construction called mgf1 which starts from a seed and hashes it repeatedly this masking breaks the mathematical relationships induced by the rsa exponentiation operation modifying a ciphertext results in plaintexts that are wholly unrelated after unmasking this way if an attacker tries submitting modified ciphertexts they ll just get decryption errors which don t provide any useful information to reconstruct the plaintext  oaep involves a second hash function to include a  label  in the encryption the label is included in the mask calculation in such a way that attempting to decrypt with the wrong label results in a decryption error in a way that doesn t reveal the correct label the mask calculation uses the hash of the label in practice this feature is rarely used and the label defaults to an empty string furthermore in practice everyone uses the same hash algorithm for the label and for the mask generation but some libraries allow specifying a different hash  if you look at the  optimal asymmetric encryption padding   oaep.pdf  design you will see that it needs two hash functions   and   with different properties      g and h are typically some cryptographic hash functions fixed by the protocol    these two are required for two different properties    h hash function reduces the input into fixed size output   g is  mask generation function mgf  which uses a hash function to expand the input into a desired sized output is is defined in  rfc8017,2019-03-14 09:26:28.92 UTC,614,55158942,i m using python cryptography library and performing asymmetric encryption  i m referring to  this example    i understand what padding does however i can t relate to  why sha256 is used,0.017915309446254073,11,so,cryptography|encryption-asymmetric|python,use of hashing algorithm in asymmetric cryptographic padding,3,attacks|trapdoor|vulnerability,0.26043766736984253
67157,"the weakness here the lack of entropy in the password reset code  i mean that you choose a random 6 digit number338625 and try it against many users so the attacker has to guess a reset code from   possibilities.at some point it may work   calculation  since facebook has a large user base and receives large number password reset requests say 10000   in 3 minutes before they expire  which is comparable to     the probability that this password rest code is valid against some user is        is making the reset code much bigger will help resolving the problem,  or facebook will need a new reset technology?   facebook can simply fix it by increasing the entropy of password reset code.this can be done easily by    having alphabets  -  52 ,digits  - 10 ,special characters say 10   in the password reset code   having a good length of say  10 characters  answer by rоry mccune    calculation  for a reset code of length of 6,the attacker now have to guess a reset code among      the probability that this reset code randomly chosen by attacker matches a valid password reset code among the above   is   or   instead of doing this the attacker have better chances of guessing password itself and trying it against many users     how to defend from this attack should i reset it again after i  receive the reset message from the attacking process or should i  reset via mail?   i think efforts to secure against this attack should come from facebook  since users may not look in to the reset message/email as soon as it arrives from user point of view all i can think is of enabling two factor authentication",2016-09-08 08:30:01,407,136210,"based on  this  and  this  articles the writer claims to be able to compromise about  2 millions accounts  by using reset code feature and brute forcing the 6-digits reset code  i have to main questions about that regardless of the truth about the attack   is making the reset code much bigger will help resolving the problem or facebook will need a new reset technology?  how to defend from this attack should i reset it again after i receive the reset message from the attacking process or should i reset via mail?    edit   credit @dog eat cat world  just to clarify the problem,the attacker instead of bruteforcing users with a certain reset code he s bruteforcing the code to a large scale users",0.03194103194103194,13,sse,attacks|facebook,a new facebook password reset bug,2,attacks|weakness,0.25995349884033203
22260,if you’re storing passwords in a database you need to mix in something that varies per entry so that you cannot easily find matching passwords within the database you can use a random salt or you can just include the username if it s immutable or some other immutable unique field  if you use something like a username then you should also do something that’s unique to your implementation since there are probably other sites using the same algorithm and therefore there’s a chance of spotting matches strictly speaking it doesn’t have to be a salt that you include in the hash it could just be a difference in the way that your algorithm works but it’s usually simplest to just add a global salt if you’re using a per entry salt then there’s no need to add another global salt  with these kinds of issues it s usually simplest to think of the kinds of attacks that hackers might use remember you’re not actually trying to safeguard your data since your database has been compromised you’re actually trying to stop hackers from taking the credentials used on your site and using them on other sites  a different salt for each hash otherwise the hashed object becomes vulnerable to a brute force attack  the salt only makes sense if it is different for each hashed string  so yes create additional column and put  random  salt there   the only thing a salt that s stored in the database does is defend against rainbow tables   unless it s open source software it s more secure to have a single salt in the php file   many developers will disagree with me but for hackers it s a no brainer   if the salt is stored inside the php file the hacker will have to get file access to see the salt and it will still defend against rainbow tables..  in most cases subscribe to packet storm security if you don t believe me vulnerabilities are through sql injection / xss cross site scripting big attacks on wordpress from anonymous etc.. all sql injection and it s very rare that there is a vulnerability that will allow an attacker to see source code using an rfi remote file inclusion or something similar  think about it if someone see s your database through sql injection etc.. they have database access so they will also have your salt too so if it s random it will not matter,2012-06-07 03:57:07.693 UTC,468,10925465,here s my question in your experience is it safer to have a single salt for every hash stored in the php file doing the hashing or is it better to have a different salt for every hashed object and store it in the database with that object,0.027777777777777776,13,so,hash|mysql|php|salt,same salt or different salts,5,attacks|vulnerability|sql injection|cross site scripting|remote file inclusion,0.25969764590263367
20231,the idea of salting and hashing is to protect the passwords  in case  the database has been compromised whether it was by sql injection buffer overflow attacks or simply by going to the server room and pulling the disk out of your server salting won t protect you against password guesses but help in case the attacker gets to the data  yes salting is to protect against the passwords from ever being reversed into plaintext  it also stops someone from saying the encrypted password is the same on site a as on site b so the user has the same password in both places  this isn t just to protect users against hackers it s also to protect them against  you   yes the only defense against password guessing is to slow down or disallow repeated attempts  most captchas are breakable or broken and you can t impose a captcha or guess limit on someone who has a copy of the raw database  so keep even the encrypted data out of the hands of malicious individuals  don t let them at your .htpasswd or /etc/shadow file or your database  if you are not using salt generating a rainbow table in advance is much easier than guessing a very strong password directly  the key is that building the reverse mapping hash->password can be done once and the unsalted hash is broken forever to anyone possessing the rainbow table  the database could be hacked if your provider is compromised if there is an injection vulnerability in your code if your db user account password is guessed if your provider uses ebay to sell off the presumed wiped hard drive that had a three-year-old copy of your database on it..  it can happen many ways  the key here is that they re not trying to get the password of a user to use on  your  site though after you fix the hole that might be useful  it s so that if for whatever reason your site is compromised there isn t damage to you users  people have a tendency to duplicate passwords that would be bad  it might be foolish but many people use the same passwords for different sites if your database is compromised and the passwords are simply hashed then there are techniques e.g rainbow tables even a few web sites that a hacker could use to find a password value that results in the same hash that password could then be used to try and gain access to other sites where the user has an account  if the passwords in your database are salted then this becomes significantly harder.. and if other web sites also salt their passwords with different salts to yours then it becomes impractical for the hacker,2010-07-14 21:44:16.37 UTC,591,3250789,bear with me i have been only learning php for only a few weeks so example code may confuse me i think i finally understand salting! it s to protect passwords inside database if breached  what i don t understand is why would a hacker have to crack hashes if they are trying to figure out a user s password assuming that s their goal? wouldn t this be easier? is the only defense from password guessing is to implement a limit of password entry x amount of times a day or captcha?  how would a database get hacked in the first place? is it more password guessing or can hashes be obtained through mysql injection?  thanks,0.02030456852791878,12,so,brute-force|php|salt|security,how does a database get hacked? a question about salting etc,5,attacks|protection|sql injection|vulnerability|buffer overflow,0.2587984502315521
38673,one small optimisation is to filter the data using   before sorting it this helped me shave 2s off the final execution time another thing i ve done was to use sequences   instead of lists no noticeable difference :-  my version can be found  here   looking at the heap profile i don t think that there is a space leak in your program:   you re just building a fairly large hash table 377141 key-value pairs in memory and then discard it after some processing according to  johan s post  a hash table of this size takes approx 5*n + 4*n-1 words = 3394265*4 bytes ~= 13 mib which agrees with what the heap profile shows the remaining space is taken by the keys and values on my machine time spent in gc is around 40% which doesn t sound unreasonable given that you re constantly updating the hash table and the temporary stacks while not doing anything computation-heavy with the data since the only operation that you need the hash table for is   perhaps it s better to use a  mutable data structure ?   update   i ve rewritten your program using a mutable hash table  interestingly the speed difference is not large but memory usage is slightly better     as you can see the size of the block allocated for the hash table stays constant throughout the execution,2011-10-21 21:12:31.48 UTC,301,7855323,so after having squeezed the last bit of performance out of some haskell i am using to break tweet data into n-grams i m running up against a space leak problem when i profile the gc uses about 60-70% of the process and there are significant memory portions dedicated to drag hopefully some haskell guru will be able to suggest when i m going wrong,0.013289036544850499,4,so,haskell|memory-leaks|space,fixing a particularly obscure haskell space leak,1,memory leaks,0.25749707221984863
40011,,2015-05-14 11:09:35.9 UTC,71,30235846,i am running erp software which is allocating physical memory 300mb on startup but after generating some reports consumption of physical memory by that application increasing rapidly and consuming upto 700mb but after closing the same report its not getting free the same until i close the application  is there any way to free/ retrieve physical memory consumed by application through c# programming,0.014084507042253521,1,so,memory-leaks,how to free increased physical memory consumption,1,memory leaks,0.2553650736808777
64682,side channel attacks usually do not leave any traces of intrusion the attacked program will run without detecting any side effects with brute force attacks the attacked program is invoked by the attacker many number of times the attacked program can take counter measures as to check for the frequency of requests.e.g brute force would be to try every possible key for decryption of a cipher text side channel attack would be to detect/sniff the key by measuring the electric/electromagnetic/cache access characteristics for rsa encryption if square and multiply method is used it is possible to find out the key by checking the current usage of the device as each step in square and multiply require different power,2018-02-14 16:57:45,383,179801,i have been reading about cache side channel attacks and the flush+reload method came up a decent amount before putting forth my question this is what i understand about the flush+reload technique   i understand that if two users alice bob run the same program the typical os will create a copy of the page tables so that the program is not loaded twice these are read-only though so all is fine if attacker bob can determine which cache lines of the shared memory are in the l3 cache over a period of time he can possibly determine what cache lines alice s process accesses overtime this can be done with the flush reload by clearing out the cache and timing how long it takes to read the cache very fast = alice is accessing this piece of memory slow = alice is not accessing this piece of memory bob continues to do this overtime to develop a  baseline  of sorts hopefully the majority of my understanding is correct  i have read how this can be used for programs that have user input bob performs the attack on himself with many inputs and calculates the time based on refreshing the cache he then performs the attack on alice and basically compares the results to distinguish inputs high level but this isn t my question  the majority of my understanding is from  here   my question is how is this attack any different from a brute force attack or even a timing attack in the context of user input,0.04177545691906005,16,sse,caching|flush-reload|side-channel|vulnerability,side channel attack with flush+reload,2,attacks|vulnerability,0.2536208927631378
49099,,2016-09-30 09:49:05.657 UTC,75,39788565,below is the part code of a multi-threaded socket server java console application when the application receives normal dataevery 30 sec heap memory is stable but as soon as data incoming frequency is increasedper second the heap starts to grow and doesn t reduce even after the frequency decreases and throws a   after some time   please tell how to solve and help me keep the heap stable,0.02666666666666667,2,so,java|memory-leaks|out-of-memory|sockets,memory leak causing outofmemoryerror:javaheapspace,1,memory leaks,0.253370076417923
53911,how can i determine the different between two full memory dumps?   quite hard to do in windbg it s much easier with a memory analysis tool such as  jetbrains dotmemory  which  can import raw dumps  if you  took care using the right format      has anyone seen anything like this before?   yes     what could cause spikes in memory to grow each time?  there were millions of entity framework 6 entities of a certain type in memory despite there only being a few thousand in the database   if you have a on² loop like   then 1000 lines in the database may create 1.000.000 objects if you just add one more line in the database then there are 2001 more objects next time you run the same query     if it is a memory leak why would it ever return to baseline between spikes?   i would not call the spike behavior a memory leak it looks quite ok however you need to consider that at some point in time ram is no longer sufficient and swapping to hard disk occurs your application then becomes much slower perhaps you can change the algorithm  however note that the baseline is not constant       so you indeed have a memory leak but it s not related to the spikes instead of comparing a spike to a non-spike i would compare two baselines     the number of database records increases gradually but is only a few thousand and the memory issue re-sets if the process is restarted   that might be resolved if the basline leak is fixed     no operation seems to take more than a few seconds as per logging despite spikes lasting ~10 mins   no operation? what s an opertaion for you? a method call? how did you ensure that you measure all method calls? next time you might want to add a cpu% graph as well,2018-11-24 08:40:35.127 UTC,713,53456555,i ve got a real head-scratcher of a memory-leak-in-production-on-azure-app-service-webjob       this is a background worker process reading work from a queue and on a schedule  about every 10 minutes memory usage spikes before about 10 minutes later returning to a normal baseline  each spike is slightly higher every time  until eventually the spike hits high enough >80% say whereupon it takes longer and longer to return to baseline and finally it locks up  i have very detailed logged in place  there are no huge or large numbers of database queries and none of the processing operations takes longer than a few seconds  it s hard to get a very clear trace since there are maybe 15-30 different operations which happen on a 10-min cycle  anyway  a while ago while it was at the maxed-out phase i got a full memory dump plugged it into windbg  there were millions of entity framework 6 entities of a certain type in memory despite there only being a few thousand in the database  i haven t been able to repro locally  so i added some code to the constructor of this entity type - it keeps a   of the number of times a certain   is seen from the constructor   i m waiting for the maxing-out to happen but connecting remotely it looks pretty standard/normal at the moment  given that these objects /may/ be increasing over time that doesn t explain the increasing spikes and return to baseline though  does it?  i ve also just captured a full memory dump during a baseline and then a small spike only been running a few hours as per the image  i have rudimentary windbg skills   anyway my questions / causes of confusion   how can i determine the different between two full memory dumps?  has anyone seen anything like this before?  what could cause spikes in memory to grow each time?  if it is a memory leak why would it ever return to baseline between spikes?   i d like to think there s no magic going on but i simply can t find a thing that coincides with the spike   the number of database records increases gradually but is only a few thousand and the memory issue re-sets if the process is restarted  no operation seems to take more than a few seconds as per logging despite spikes lasting ~10 mins,0.011220196353436185,8,so,.net|memory-leaks|windbg,.net memory leak - increasing spikes then back to baseline,1,memory leaks,0.2518395483493805
17612,https://github.com/grails/grails-core/issues/668    and/or   config properties/settings  the following worked for megrails 3.3.9 in     in   domain   at the end of     the solution is to set   and   limits inside your   file be sure to have something like this   replace   with the  number of max bytes  you want for a file upload and for the entire request  grails 3 default is 128000 ~128kb     faq   1 how to convert from mb to bytes?    e.g  convert 5 mb in bytes     2 security   as quote from  owasp       limit the file size to a maximum value in order to prevent denial of service attacks   so these limits exist to prevent dos attacks and to enforce overall application performance bigger request size == more memory used from the server   3 what is the right value for   and  ?    it depends on the application type but it s generally a bad idea to keep them too high if you really need to upload big files probably you should develop or use a dedicated and separated service  try this https://github.com/zyro23/grails-core-668/blob/master/grails-app/conf/application.yml   upload:            maxfilesize 9999999999            maxrequestsize 9999999999  part of the problem is setting a max file size the other problem is handling that error gracefully  here is my bean definition   here is my implementation   the idea being to check for the request attribute in your controller or a filter to determine if the file was too large  i also had no luck trying to set new maximum values in application.properties or application.yml  what does work though is using a bean definition in conf/spring/resources.groovy,2015-04-24 11:08:47.887 UTC,296,29845943,i am trying to update the file upload maxfilesize limit in grails 3 and tried the configuration in     and   but it s still throwing the exception    any help on where and what to configure in a grails 3 application,0.016891891891891893,5,so,grails|grails-3.0|spring-boot,grails3 file upload maxfilesize limit,3,owasp|attacks|denial of service,0.25136417150497437
1099,you can never trust the client and using a cookie is just that - you are relying on the client possibly an attacker sending you accurate information for this to work properly you d need to store this server side  you could store the reset attempt count with the account data and just reset it to 0 on successful login - workable for smaller sites for larger sites you d want to store the data in a separate data store - memcached or similar probably using whatever mechanism you use for server side sessions data  you re right that a cookie is a bad idea but the approach itself is misguided  the problem with these kinds of hard limits is that they make it easy for an attacker to create a dos condition for the legitimate user simply by putting in 10 incorrect email addresses even if you time-delay the attempts the attacker can simply send a request every time the time limit lapses  the way i d implement it would be to time-limit the requests on a per-ip per-account basis with no delay after the first and second attempts but increasing delays on consecutive attempts up to a limit of 45 seconds another useful measure would be to enforce 45 second delays on any request coming from an ip address that has seen more than 10 incorrect attempts in the last 15 minutes across any number of accounts in order to protect against attacks that scan the entire list of usernames against a single known email address i might also consider requiring a captcha after a certain number of failed attempts to protect from automated attacks  this provides the following benefits   a legitimate client will always be able to use the feature i.e no dos condition can be reached as long as the attacker is using a different publicly-facing ip address  a reasonable number of failed attempts can be performed without the user being hindered by lock-out mechanisms  both targeted attacks and low-hanging fruit attacks are hindered with minimal negative effects on legitimate users  the worst-case scenario is that an attacker generates a dos condition when on the same network as a legitimate user not exactly critical or likely   this would involve storing failed attempts in a log on the server side perhaps in a database table or in-memory cache  you should lock down the account server side setting a cookie or temporarily banning an ip can all be trivially circumvented by the client i would suggest working a mechanism to disable the user from resetting after   failed attempts your php script should check your database if attempts exceed,2013-03-13 01:59:35,506,32510,in a setting where one has forgotten their password i d like to be able to limit the attempts of entering in email addresses to something like 10  my first thought was to use a cookie   my concern with only using a cookie is that cookies can easily be removed.what would be a better practice here,0.029644268774703556,15,sse,password-management|php|secure-coding,setting a limit on password recovery attempts,4,attacks|protection|secure coding|denial of service,0.2488269805908203
1631,if you are record data you don t need it in real time i think what your concern is that you don t want to loos any data enter buffer i see ~6mb buffer size use for the serial port interfac if you read data faster than that buffer fill up you can be confid no data was lost due to sleep and that is an empir thing and should be test you could go fanci and use clever back-out algorithm that tri to calcul least amount of sleep but if your data rate is near constant just find a sleep time where buffer is no more than 80% full when you start read it or less if you expect burst so as long as you drain data faster than it come noth should be lost and your code can sleep easi,2020-06-11 17:05:48,258,62329727,i am use follow script to continu log data from a sensor at 500hz rate which requir an infinit loop natur it keep the cpu busi at over 30% for window laptop and up to 100% with raspberri pi 4 thi problem is usual solv by implement sleep function but consid i am record live time-seri data i can t afford loos any data sampl dure the sleep time i would like to know how time-crit process are handl not to overload cpu and how i can possibl optim my code respect cpu load on window 10 pc cpu load on raspberri pi 4,0.011627906976744186,3,so,infinite-loop|python|time-critical|while-loop,cpu optim of time-crit infinit loop in python,1,infinite loop,0.2477870136499405
1837,dotnet can onli know the size of the file by upload it consum bandwidth in your exampl the solut to thi problem is to set a smaller limit for the size of upload file the way dotnet work it will stream the file process it littl by littl onc it hit the max upload size it will return an error you are right that thi doesn t prevent an attack from upload mani larg file - dotnet will still have to upload/stream them until the max size is reach the way to mitig attack like these is through other mean - rate limit ip block ddo protect at the cdn etc,2020-03-30 07:12:27,368,60925644,i creat an api recent that allow video and audio file to be upload and it was work fine until one custom upload a larg video and i got thi error fail to read the request form multipart bodi length limit 134217728 exceed from mani post it s evid that the philosophi behind request size limit is to reduc the chanc of ddo attack pleas correct me if i m wrong what secur hole do i open up if i do not limit the max request length for file upload to my webserver? https://manage.accuwebhosting.com/knowledgebase/2997/aspnet-error--maximum-request-length-exceeded.html howev i upload a larg video myself and watch the bandwidth both on my local pc and on the server i have a vps for thi app and i can watch bandwidth use task manag s perform tab by select ethernet and see the graph of send and receiv the point is i saw that a high traffic go on for some minut when i was upload the file and then i saw that error thi mean that the resourc of the server are be consum even when there asp.net core reject the request so i don t understand thi part how asp.net core diminish the chanc of dos or ddo attack by limit request size while bandwidth is taken in reality? a logic way would to drop the request from the begin for content type if the request payload/bodi was too huge and even do not consum the bandwidth,0.035326086956521736,13,so,asp.net-core-mvc|ddos,how asp.net core prevent ddo attack by limit the request size,4,ddos|attacks|protection|denial of service,0.2476668357849121
31795,,2012-07-06 16:10:20.46 UTC,441,11365901,i have noticed this behavior only twice  once with oss software elasticsearch and once with some custom-written software  the behavior is such that launching a java application segfaults with exit code 6 and the only thing that worked was a reboot  the system appears healthy plenty of memory swap disk etc  system logs report nothing unusual but java would crash each time we tried to restart the application  the application was not logging any information about any problem when we tried to restart it which makes me think it was a problem with the jvm or that the segfault was occurring just before the application logging logic kicked in  is the problem with the jvm itself or is it the application?    jun  8 15:13:48 node3 kernel java[7362] segfault at 00000000417f71a8 rip 00002aaaaaf67812 rsp 00000000417f71b0 error 6  rebooting the linux host corrects the problem which i found odd  as if a condition in the jvm or application was persistent between attempts to restart like a shared resource outside of the running java program was left in an orphaned/mangled state - semaphore shared memory message queue but the program in question does not appear to use ipcs..  in this case the java application was having problems and java garbage collection was running constantly and was consuming hefty portions of cpu so we shutdown the app  attempts to restart failed with the above errors  not being anywhere near proficient in java internals i m not sure what would cause the segfault nor why rebooting would fix the problem   i m not a fan of rebooting systems to correct problems as reboots usually do not correct the root cause of a problem  this is not a repeated problem but i am puzzled why a reboot would correct this  limits on the running process are pretty generous and i don t see anything in logs indicating hitting a limit:limit                     soft limit           hard limit           units max cpu time              unlimited            unlimited            seconds max file size             unlimited            unlimited            bytes max data size             unlimited            unlimited            bytes max stack size            10485760             unlimited            bytes max core file size        0                    0                    bytes max resident set          unlimited            unlimited            bytes max processes             790527               790527               processes max open files            131072               131072               files max locked memory         32768                32768                bytes max address space         unlimited            unlimited            bytes max file locks            unlimited            unlimited            locks max pending signals       790527               790527               signals max msgqueue size         819200               819200               bytes max nice priority         0                    0 max realtime priority     0                    0       java is jdk1.6.0_29 host os is oracle linux rhel 5.8  any clues or insight,0.013605442176870748,6,so,elasticsearch|java|jvm|reboot|segmentation-fault,why would a linux system reboot correct the inability to launch a java application segfault,1,segmentation fault,0.24678963422775269
5091,"perhaps this explains what you are looking for weaknesses in the key scheduling algorithm of rc4 scott fluhrer itsik mantin and adi shamir  i found a copy here: http://www.crypto.com/papers/others/rc4_ksaproc.pdf   the weak keys you are talking about are exploited by the  fms attack  as correctly pointed out by another user     it would be great if someone enlightened me about how they reveal secret wep keys?    it s not easy keep in mind this is a relatively complex cryptographic attack it s not a simple dictionary attack or something like that  wep is should we say  was ? based on a cryptographic algorithm called rc4 the way rc4 works is the following  1 starting from a key - in our case,a shared key concatenated with an iv - it computes an internal state s[i] which is a pseudorandom array of 256 8-bit values through a key scheduling algorithm ksa  2 this internal state is the input of a block prga - pseudo-random generation algorithm that generates one pseudo-random byte k[i]  3 the internal state is changed and fed again into the prga which produces another pseudo-random byte k[i]+1 the sequence of pseudo-random bytes is known as keystream  4 the message plaintext is xor d with the keystream because of the properties of the xor operator this is like xoring each pseudo-random byte of keystream with a byte of message indeed that s what happens  one thing you have to know about the xor it is always true that a ⊕ b ⊕ b = a    the fms attack  works as follows   because wep is used in wi-fi which for technical reasons not related to security uses a particular type of header the first byte of any message protected by wep is known and is 0xaa aa in hexadecimal applying the xor property and considering that every byte of the ciphertext is derived separately we can get the first byte of keystream let m1 = 0xaa be the first byte of plaintext and c1 the first byte of ciphertext the first byte of keystream is m1 ⊕ c1   remember that the fixed shared key is concatenated with the iv which changes at every packet but is always known to the attacker since it s transmitted in clear as the shared key is constant a key is weak if the iv is weak  let s focus now on the first three bytes of keystream k[0] k[1] k[2]  as the keystream starts with the iv k[0,1,2] are the only three bytes of the iv and as such are known  the attacker starts with these bytes and loads them into the ksa if the iv is weak the attacker can get a possible value of the fourth byte of the key by repeating this procedure on several packets the attacker can get a pretty good idea of the value of the 4th 5th bytes and so on     what are these weak ivs?   as we saw weak ivs are ivs which allow to get hints about the key one byte at a time these hints get more and more concrete as the attacker collects more and more packets  more specifically an iv is weak in the sense of the fms attack if it is of some particular kind such as for example a+3::ff:x where a is the byte of the key to be found ff is 255 in decimal and x is an arbitrary 8-bit value     can we somehow stop or slow down wep cracking by avoiding usage of weak ivs?   we can stop  this attack  by avoiding usage of weak ivs but we cannot prevent wep cracking at all wep is broken in many ways most of which would work even if the underlying cryptographic algorithm wasn t broken as rc4 is for example through the fms attack   do not use wep period  there s no reason to do that anymore",2016-10-25 08:56:10,748,140791,"most of us dealing with wireless security must have cracked a wep key and while doing that must have come across texts like this     weak ivs reveal more information about the secret part of the wep key than others--about 9,000 weak ivs out of 16,000,000 possible ones   what are these weak ivs? it would be great if someone enlightened me about how they reveal secret wep keys? can we somehow stop or slow down wep cracking by avoiding usage of weak ivs",0.03877005347593583,29,sse,wep|wifi|wireless,what are weak ivs? can we somehow stop or slow down wep cracking by avoiding usage of them,4,exploit|attacks|weakness|protection,0.24558331072330475
67703,a relatively user-friendly way of mitigating brute-force attacks is delaying the minimum time between attempts the first time your user enters wrong credentials you let him wait 1 second before he can try again the second time you let him wait 2 seconds the 3rd time you make him wait 4 seconds 4th time 8 seconds and so on you also base this on the username that is used to authenticate not any ip addresses if there hasn t been an attempt in the past 5 minutes or if the user authenticated successfully you reset the counter  the result is that a user that makes a typo in their password isn t affected the first few times but any brute forcers will very quickly reach a point where brute-forcing is effectively not viable anymore   aside from preventing your web application against brute-force attacks you should also ensure standard password protection practices like hashing &amp salting preferably with pbkdf2 or bcrypt secure password resetting and mitigation against username enumeration  but i assume that since you post on here you already know that  a good compromise between user experience and security would be to have ip-based captchas that trigger after a few failed logins from a particular ip regardless of username    this approach isn t vulnerable to dos attacks against a single user by bruteforcing his account until the backoff time reaches several hours/days and prevents the legitimate owner of the account from logging in    the number of people stuck behind a nat isn t that high and it s better to slightly degrade their experience with a captcha rather than prioviding attackers with the ability to completely dos an account and lock out their legitimate owners    it depends on what your app is for and how long the session lifetime is but if it s something like stack exchange then people usually don t log in and out frequently    as for your concerns about using sessions to track incorrect logins you are right that s pointless since it ll work for legitimate users but an attacker won t even bother storing the session cookies which means on each try he ll get a new session and new login attempts without captcha  for the ip changing yes that s a bit of an issue but an ip is still a cost to the attacker eventually he ll run out of proxies and/or compromised machines in his botnet and he will have to buy more you should also always require a captcha if the ip is in an open proxy database search for one on google or in the list of tor exit nodes that way an attacker won t be able to use these free solutions and will have to rent a botnet or some premium proxies that aren t yet in the blacklists  i ve been doing some research on mitigating brute force attacks and came across this post i recently implemented the following approach  in a 24 hour period on 20 or more failed authentication attempts for a single ip address we require captcha for each subsequent attempt at 100 failed attempts we blackhole requests but give no indication that the request was blackholed we just keep returning the standard error message  for us this is reasonable as we are not worried about large amounts of people logging in from the same ip and we believe our thresholds are high enough to where a normal user would not be affected its trivial to send an alert at 100 failures or if a username appears across multiple ip addresses  we log everything in a database we do concede that a botnet could use a large swath of ips however we log the username that an ip failed to authenticate with this table is generally checked once per day so the potentially exists for us to identify a large scale brute force attack targeting a single user  we also recently enhanced our password requirements to increase character length and complexity as well as not allowing passwords that are in various password dictionaries our login page is no-indexed in google not in robots.txt use the meta robots tag as attackers can check your robots.txt for interesting pages finally administrator accounts require two factor authentication  still we accept the risk that a brute force hack is possible,2014-11-18 07:53:32,897,73095,i want to cover the possible cases of attacking my application already has captcha and two-factor authentication but how can i avoid a tiny attack without annoying my users? the possible cases that i m thinking to cover are       show captcha after 3 failed login attempts based on session but the problem is that some related articles said it should not be based on asp.net session as it somehow could be cleared    showing the two factor authentication after the captcha but should i also show the captcha based on the failed count from the previous step? or i should count from the beginning?    also i m thinking of blocking the user s ip for a certain period but that might affect other users working from same ip! what if the hacker has a tool for changing the ip periodically?    could you please advise me with references if it is possible what is the best way to cover these security issues,0.020066889632107024,18,sse,captcha|penetration-test|web-application,best practice in web application security authentication to avoid bruteforce attack,5,attacks|protection|vulnerability|penetration test|denial of service,0.24506130814552307
51618,"read in the dictionary from file store it in a   set up your java application as a service that runs continuously since you said it gets called many times per second then your   will be cached in ram  i would load the file into a   at startup of the application and then use it as you describe  i would store the data in a database for faster load times  definitely do not have the application startup and shutdown every time it is called have it as a service that waits for io using asynchronous i/o such as  netty   the fastest is a hard coded map in memory.if u a have a huge file you can use a memory mapped file    this approach is a bit problematic though,in practice you will want to partition the fileon line breaks i.e read until the 100th line clean the buffer and read some more",2013-05-02 19:52:30.02 UTC,255,16346571,i have a java application which is started and stopped multiple times per second over hundreds of millions of items called from an external script   the purpose of this application is to look for a certain key in a never  ever ever  changing   ~30k keys and to return the value very easy      question what is more efficient when used multiple times per second          hard-coded dictionary in a      read an external file with a       ...amaze me with your other ideas       i know hard-coding is  evil  but sometimes you need to be evil to be efficient :,0.0196078431372549,5,so,coding-style|dictionary|hardcode|java|performance,dictionary hard-coded vs external file,2,hardcoded|hard coded,0.24474076926708221
64631,,2016-08-24 13:49:31,56,134828,nikto reports the following via the ssl-dh-params nse script   why is the dh group reported as weak dh group 1 when group 1 should have a modulus length of 768  according to sources such as this ? this as you can see is 1024 which should be group 2,0.03571428571428571,2,sse,diffie-hellman|logjam|vulnerability,clarification on dh groups,2,weakness|vulnerability,0.24463225901126862
58592,this helped in resolving the problem    thank you dan mašek! it was all about realising device context,2018-12-10 20:02:58.48 UTC,94,53712809,this script looks for a button and clicks it when possible   computer works slower and slower while running this script i guess some variables are being overwritten without deleting from memory i tried adding some sleep to the loop but it didn t solve the problem - it slows down slower but still regularly do you have any ideas what might cause it,0.02127659574468085,2,so,infinite-loop|performance|python|python-3.x|win32gui,simple script with infinite loop slows down the computer,1,infinite loop,0.24139437079429626
66966,if this were a denial of service attack you d be seeing those 15k messages covering less than an hour -- probably far less  this is just the botnet-based brute-force attack on ssh passwords that constitutes part of the background noise of the internet  make sure you re using strong passwords or better yet key-based authentication and that you ve disabled root login over ssh and don t worry about it,2014-11-10 05:43:46,123,72638,when i m checking my log i found like 15k events of  pam servicesshd ignoring max retries 6 > 3   i know this might be normal and considered as brute-force attack but can it be classified as dos attempt?  sample of the log,0.06504065040650407,8,sse,brute-force|denial-of-service|linux,can this be a dos attack,2,attacks|denial of service,0.2405756115913391
20767,here a batch bruteforcer 3 minutes to find your code lol   you can check the needed time in    i cannot see how this answer could do any harm because if you know the batch source code then you would probably know how to start the program anyway  so let s try and enter   this should throw an error but the batch continues and therefore you bypass the if clause  type   in your console that will bypass the password protection   scnr    anyway there is no injection hole in that code,2014-04-19 19:58:32.667 UTC,161,23174804,so i made a very simple very non-secure batch program which compares the password you enter to the password in the program now i want to see if i can somehow bypass it i´m personally thinking of something along the similar to sql injection the only restriction is you can´t read the source code so any suggestions?   this is batch for      edit you also can´t bruteforce it,0.043478260869565216,7,so,batch-file|password-protection,bypass non-secure password,3,bypass|protection|sql injection,0.24055418372154236
25296,you can never protect yourself fully against both dos attacks and unexpected timeouts  i suggest that you scale the allowed timeout with the amount of data already sent start with a shart timeout and increase it as you send more and more data  dos attackers and defunct clients will likely open many connections but won t bother reading from them you want to close this kind of connection fast  clients who have received tens or hundreds of mb of data should be allowed much greater timeouts  example   initial  500 ms  after 1 mb 2 s  after 100 mb 10 s,2017-12-14 12:37:58.613 UTC,278,47813577,i am transferring files from server to clients using nonblocking sockets some files may be quite big hundreds of megabytes a separate thread handles each client   sometimes   timeouts and the server closes the connection to prevent malicious clients from using up server resources i discovered this can happen even when the client is still trying to read the data in such case the client gets   when calling    i figured out that the server repeatedly sends some chunks of the file and at some point the socket may remain not ready for writing for some seconds i assume this is because the data hasn t been sent yet and the corresponding kernel buffer is full however this causes the timeout which leads the server to close the connection  when i increase the value of   the error happens less often what is the best way to make sure unexpected timeouts don t happen?  the server is supposed to run on multiple platforms so i cannot really use an os-specific solution,0.017985611510791366,5,so,c|sockets,preventing timeout while transferring files between 2 peers,3,attacks|protection|denial of service,0.24048365652561188
57834,,2014-08-14 11:17:24.133 UTC,152,25306743,today i was looking for a way to make our servers more robust against a very special way of dos attack  we use bcrypt with a strength of 12 to store user passwords.validation of a password on our environments may take up to 500ms  it s acceptable for a user on login.i think somebody may flood our servers with login request with invalid passwords to take it under heavy load  first i thought this problem can only occur with an existing user but then during debugging i found this code in the daoauthenticationprovider   in case of a usernamenotfoundexception it checks the provided password against a usernotfoundencodedpassword! also here the system needs the same calculation time cause the usernotfoundencodedpassword also has a strength of 12 where is it set? how can i avoid that calculation? do i have to write my own authenticationprovider,0.019736842105263157,3,so,authentication|denial-of-service|java|spring-security,spring authentication avoid unnecessary bcrypt operation,2,attacks|denial of service,0.24012908339500427
39891,,2015-09-29 03:02:15.31 UTC,185,32835017,"i have a simple program that grab data from the database and stored it in a dataset through dataadapter.fill  the problem i am facing is that the program s memory size keep increasing using process explorer i monitor the virtual size of the program the processes are run in a standalne buildnot in unity editor  2mins after the process is launch the program sits at 824,444k virtual size but leaving the program running for 30 mins the program s virtual size increased to 1,722,340k  can t upload screenshot  https://drive.google.com/open?id=0b0dwzunteqfkcdhhcxrmv2twuee   the program only consist of 2 simple script a helper class sqlcontroller which manage the loading from database to dataset and a monobehavior script dataproducer that poll the database at regular interval for update using sqlcontroller object   sqlcontroller    dataproducer   i am not very sure what exactly causes the memory leak but when i changed the loading from threaded   to running in the main thread   the process virtual size though still increment but at a slower rate from 828,312k at 2 min to 1,083,908k at 40mins",0.016216216216216217,3,so,c#|dataset|memory-leaks|multithreading|unity3d,memory leak with dataadapter.fill to dataset,1,memory leaks,0.23913343250751495
29978,check   is not,2013-05-08 18:57:07.82 UTC,46,16448167,i have the following problem when inserting database data during an offline process using management script   given links has a large number of entries the process takes more and more memory during the loop,0.043478260869565216,2,so,django|memory-leaks,django memory leak during offline database insertion,1,memory leaks,0.23859646916389465
26208,"here s some python code to generate those keys.. haven t tested it yet but would be interested to get feedback on it   java hashmap/hashtable can do the  resize  operation when the filled entry reach threshold it s hard to say that there have an fixed bucket hashmap waiting for you because of the operation for selecting bucket have two steps one is take hash value of specified key another primary step is remainder operation with total bucket sizethe size has being changed by  resize    tomcat version affected are apache tomcat &lt;= 5.5.34 &lt;= 6.0.34 &lt;= 7.0.22 as per the link you provided  the page lists apache tomcat >= 5.5.35 >= 6.0.35 >= 7.0.23 as fixed versions  the simplest solution is to upgrade to a fixed version of tomcat  however i suspect you want to know the details of what the tomcat people would need to change  this attack works by exploiting a common implementation detail of hash data structures - using linked lists to hold all the values whose hash is the same  adding values to this linked list is inefficient as the size of the list gets large  an attacker can create a list of values that are known to generate colliding hashes forcing this inefficient behavior  in order to protect against this you have a few options    prevent collisions - prevent the attacker from generating colliding values by having some pseudo random factor in your hash function  perl has done this for a long time    use something besides linked lists for your buckets - the attack works because inserting n items into a linked list has n^2 growth  if you use a balanced tree or some other structure that has n logn growth when inserting the problem should be mitigated  this may sacrifice some best/average case performance in order to limit how bad the worse case is    understanding attack vector  how hashmaps work  say a comment form on a blog accepts the parameters – first_name last_name comment – as post parameters internally tomcat stores these parameters as a hashmap  the  logical structure  of this hashmap is like this -   but the  physical structure  is different the keys are first converted into a hashcode and then the hashcode is converted into an array index  the  ideal physical structure  thus becomes -    but the possible keys are infinite so at some point two keys will have the same hash code this becomes a hash collision   with collisions the  physical structure  becomes     hash collisions and impact on performance  when you have hash collisions inserting a new entry means iterating over all the elements in a single hash bucket  sequentially  just to find out if it already exists in the map inserting one element can approach on complexity if all elements hash to the same value inserting n elements in this worst case makes it on*n complexity   in short  if you  insert thousands of keys that have the same hashcode  the server will require a lot of cpu cycles  how do you generate keys with the same hash?  in java aa and bb have the same hash code  because of a property called equivalent substrings we can generate several other strings with the same hashcode just by starting with these 2 strings  first iteration  aaaa aabb bbaa bbbb have the same hash code  now we have 4 strings with the same hash code we can permute them to generate 16 strings that will have the same hash code for example     all these 16 strings have the same hash code  you can now take these 16 strings and generate 256 strings that have the same hashcode   in short  it is very easy to generate a large set of strings that will have the exact hash code  how do you attack the server?   create thousands of string that have the same hash code see above  construct a post request like this - aaaa=&amp;aabb=&amp;bbaa=&amp;bbbb= ...   submit the form  repeat in a loop and create several threads so that all server resources are used up   because this is just a post request an attacker can also use innocent browsers to attack a server just find a website with a cross site scripting vulnerability embed code to make a post request and then use social engineering to spread the link to as many users as you can  prevention  in general the underlying platform cannot fix this this is considered to be a application framework problem in other words tomcat has to fix this not oracle/sun  possible fixes include       restrict the number of post parameters  - tomcat 6.0.35+ has a new parameter  maxparametercount  the default value is 10,000 the lower the better as long as it does not break your functionality      restrict the size of the post request  - for the attack to work the payload has to be huge the default post allowed by tomcat is 2mb reducing this to say 200kb will reduce the effectiveness of this attack the parameter in tomcat is  maxpostsize      web application firewall  - if you have a web application firewall you can configure it to block requests that look suspicious this is a reactive measure but is nice to have in case you cannot use one of the above solutions    fyi - tomcat s documentation is here -  http://tomcat.apache.org/tomcat-6.0-doc/config/http.html",2011-12-29 15:47:27.443 UTC,1034,8669946,below excerpt is from an  article  that explains possibility of denial of servicedos attack because of non random hash functions used in hash data structures      […] the condition can be leveraged by exploiting predictable collisions in the underlying hashing algorithms   in order to verify it i went through reference implementation of java hashmap from oracle and indeed found a static hash functions used    another  paper  on the topic tells     a tomcat 6.0.32 server parses a 2 mb string of colliding keys in about  44 minutes of i7 cpu time so an attacker with about 6 kbit/s can keep one i7 core constantly   busy if the attacker has a gigabit connection he can keep about 100.000 i7 cores busy   how can we safeguard against this vulnerability moreover so with so many of softwares we use being open source tomcat etc. which rely on this implementation,0.020309477756286266,21,so,ddos|java|security,application vulnerability due to non random hash functions,6,attacks|exploit|protection|vulnerability|denial of service|cross site scripting,0.23850327730178833
54314,as shown in the comments to my question the answer came from puciek  the solution was to close the pool of processes after it is finished i thought that it would be closed automatically because the   variable is local to   and would be deleted after   completed however python doesn t always work as expected  the fixed code is,2014-11-03 15:13:02.27 UTC,353,26717120,my code part of a genetic optimization algorithm runs a few processes in parallel waits for all of them to finish reads the output and then repeats with a different input everything was working fine when i tested with 60 repetitions since it worked i decided to use a more realistic number of repetitions 200 i received this error   here is a snippet of my code that uses pool   the runone function creates an object in class i created uses a computationally-heavy python package to solve a chemistry problem that takes about 30 seconds and returns the object with the output of the chemistry solver  so my code calls runmany in serial and runmany then calls runone in parallel in my testing i ve called runone using 10 processors the computer has 16 and a pool of 20 calls to runone in other words lenarg1*lenarg2*lenarg3=20 everything worked fine when my code called runmany 60 times but i ran out of memory when i called it 200 times   does this mean some process isn t correctly cleaning up after itself? do i have a memory leak? how can i determine if i have a memory leak and how do i find out the cause of the leak? the only item that is growing in my 200-repetition loop is a list of numbers that grows from 0 size to a length of 200 i have a dictionary of objects from a custom class i ve built but it is capped at a length of 50 entries - each time the loop executes it deletes an item from the dictionary and replaces it with another item   edit  here is a snippet of the code that calls runmany,0.0113314447592068,4,so,memory-leaks|memory-management|python|python-2.7|python-multiprocessing,python cannot allocate memory using multiprocessing.pool,1,memory leaks,0.23830096423625946
14344,,2015-02-05 07:33:38.467 UTC,143,28338493,i m using parse as a cloud-database in my project along with some server-type functions such as for data validation when my project is released it may be controversial since the parse burst rate-limit for queries is 40 requests per second i m worried someone will extract the api key and set up a computer to make 40 login or create account requests per second they could even launch 40 copies of the app per second in emulators  if this happens is there anything at all we can do to continue using parse? it seems like an almost-zero-effort dos method and everyone that uses parse is vulnerable if this occurs can we do anything like block ip ranges? do we have any recourse? buying a bigger plan does not prevent this attack,0.03496503496503497,5,so,parse-platform,can a parse account be protected from targeted rate-limit attack,4,attacks|protection|vulnerability|denial of service,0.2375769466161728
62331,i wrote small python script to do what i wanted i put the key under the name  ssl.key  and the word list in a file called  wl.lst   here s the complete code   this script is cross platform to increase or decrease the number of words used in a combination just add/remove appropriate code blocks   note  removing the display of status can considerably improve speed,2015-05-22 08:02:38.677 UTC,125,30391404,i have an rsa private key for my ssl certificate unfortunately i forgot the passphrase.here is the header info   i would like to perform a dictionary attack to try to crack it could anyone tell me how to do it? maybe using a tool like john the ripper,0.016,2,so,dictionary-attack|john-the-ripper|public-key-encryption|security|ssl-certificate,dictionary based bruteforce on a rsa private key,2,attacks|dictionary attack,0.23396866023540497
38051,turns out this is a glibc problem  the short answer for me was  export malloc_arena_max=1  this decreased process footprint virt in top by as much as 5x  back to the levels seen in centos 5  recent versions of glibc have a new feature per-thread memory pools   http://www.centos.org/docs/5/html/5.4/technical_notes/glibc.html   the last item in the 1.71.1 log section discusses it and refers to a non-public bug...,2012-05-13 21:19:26.31 UTC,281,10575342,i have 7 different java daemons that i run all 7 on 3 different servers  the java command line has -xmx2048m and -xss1024k  on these 3 servers all 21 processes show just under 2.5 gb for virt size in top and atop  res size varies from 300 to 1.9 gb according to which daemon it is  that is all as it should be  enter the new server  faster cpu more ram 16 gb instead of 8 gb slightly newer java 1.6.0_10-b33 on the old servers 1.6.0_31-b04 on the new server  both systems and jvms are 64bit  moved 2 of the daemons to the new server  on the new server given the same task the daemons are both consuming vastly more cpu about a core s worth and getting less done  moved from 5110 processors on the old systems to 5620s on the new one  pretty much a full extra core of cpu usage gc thread?? and reporting 5 gb virt and 2 gb res for one daemon and 10.5 gb virt and 2 gb res for the other daemon  any ideas what would cause java to ignore or appear to ignore if that is the case the memory limits,0.010676156583629894,3,so,java|jvm|linux|memory|xss,what would cause a java process to greatly exceed the xmx or xss limit,1,cross site scripting,0.2322169989347458
35352,well by the time i ve finished writing this question i ve found that its actually disk cache ram usage based on   and on information found here  https://www.centos.org/docs/5/html/5.1/deployment_guide/s2-proc-meminfo.html   mainly    while using   safe to assume that 957.56mb is being cached but not used and most likely will be reclaimed back when needed hopping it will help someone else that had this confusion :  cheers,2014-09-12 02:17:13.797 UTC,459,25799729,i have the below scenario that it looks to be causing what i think memory leak but haven t confirmed this if is disk cache instead ..  anyway i have one almost full ajax site that pulls out data from another server thus the data needs to be cached somehow i figured that once the page is rendered i could save the html code directly and on a second request if the cache file exists and matches the request to provide the user that specific cache file  the cache file is around 30kb to 100kb in size html code only however i have almost a 1m pages thus there are a lot of cached files total size at the moment is over 2.1gb disk space  i m using   to get the contents of the cache files and render them to the user  btw i m using symfony2 framework too just in case the info is useful and also with doctrine  at this point i have used    for avoiding doctrine s memory leak just in case symfony2 in dev only says its using around 20mb to 40mb of ram depending on the page and queries around 20mb of ram when serving cached file  using cpanel that has cloudlinux and physical ram is limited to 2gb i m currently using 1gb of ram and may increase ..  if this is disk cache which i m almost sure linux will eventually clear the cache or will drop the cache if ram is further needed well that s the safe case  i m asking if there s a way to check if there are memory leaks from my script what keeps on using ram i know that   is using   when reading the files so that would cause the ram to be used for disk cache most likely  i m using in most queries the   or   functions for querying the database i think i have only one custom query with    the page is really fast i don t see a problem with it loading but only concerned about its usage  i know that you guys will probably ask to provide code i would but can t really know what to include / which part i ve mentioned almost everything that seemed useful  thanks again and terribly sorry for the long question,0.010893246187363835,5,so,caching|doctrine|memory-leaks|php|symfony,guidance on about memory leak,1,memory leaks,0.23184160888195038
28308,,2018-09-10 17:00:29.597 UTC,77,52262399,we are running a spark app on an amazon emr cluster our code takes in json data converts it to dataframes carries out spark sql transformations on them and then writes the resulting dataframes to csv files at least it tries to however when executing the code on an emr cluster we get the following error    ...etc  can anyone help us?thank you very much in advance,0.03896103896103896,3,so,amazon-emr|apache-spark|apache-spark-sql|infinite-recursion,infinite recursion stackoverflowerror in amazon emr cluster,2,overflowerror|infinite recursion,0.228367418050766
67777,,2016-10-19 17:25:09,90,140235,i know its possible to use a tool like burp to test cookie/post/get values  but is it possible to use such tool to perform a brute force/dictionary attack fuzzy test on the values after # symbol   i know that the values after the # in a url are not sent to the server they are only interpreted by the browser this makes it more difficult to use tools such as burp,0.022222222222222223,2,sse,burp-suite|hash|penetration-test,use burp to fuzzy test the hash value the values after the # in the url,2,attacks|penetration test,0.2282869666814804
43091,,2015-03-17 13:29:14.823 UTC,153,29100363,it seems like we have a memory leak in the termsvcs process of our rds farm which occasionally occurs on a random machine in our farm we have 10 session hosts on windows server 2012 r2 with current patch level who all serve around 20 to 30 users  in this screenshot you can see the cpu and ram load notice the steady drop in memory within the last 24 hours http:// removed due to lack of reputation i.imgur.com/dz11fpz.png  here you can see the process in question with a very high value in private bytes  http://i.imgur.com/rh9sfzi.png  - and in comparison the same process on a healthy session host with the same configuration and roughly the same amount of users connected  http://i.imgur.com/fcmvcld.png   notice the difference the private bytes  does anyone else experience something similar?it seemingly started a week ago so maybe related to a patch day,0.0196078431372549,3,so,hyper-v|memory-leaks|rds|virtual-machine|windows-server-2012-r2,svchost termsvcs memory leak,1,memory leaks,0.22816893458366394
39938,this can not be done as easily as you think since you are running on one thread only you cannot have any checks if this thread is blocking it is blocking  you need to create some sort of multi-threaded environment where you run one worker thread for the execution of   to increase speed and take advantage of multi-core processors you could even spawn more worker threads and another thread that checks and kills the workers if they are taking too much time  taking what  @klaus said  into account you  would  be able to perform this check if you can edit the   function within the function there are likely either a number of calls to various subfunctions or a large number of   repeat loops you want to add a check somewhere in the functions or at the head of the   loops to see whether the function is taking too long to execute  simple pseudocode example,2011-09-23 18:45:57.32 UTC,254,7533485,i need to parse many html files using php   for some reasons file is too big function parse_html take very long time to run or has memory leak in it   i want to monitor function parse_html if the running time exceed a given time should continue to parse the next url and disregard the current one   for most of the time my codes runs great but there are some urls can not be parsed there is no error output and i guess it is memory leak,0.011811023622047244,3,so,memory-leaks|performance|php|time|while-loop,stop running php function if exceed a given time,1,memory leaks,0.22793082892894745
66971,if the page you are refreshing is sufficiently resource intensive in most cases it will not result in a denial of service as the latency in the network connection far exceeds the page load/cpu time if you found a page that did significant background work such as creating a database backup you could in theory repeatedly refresh the page and consume all available cpu/memory/etc  in principle yes it is possible to dos a site by repeatedly refreshing a website in a browser an example of accidental dos by browser loads is the  slashdot effect  in practice dos with a browser is not cost effective for an attacker because the cost of doing a page load and rendering the page in a browser usually exceeds the cost of the server for serving the page most dos attacker would use specially designed tool that can make a request without rendering the page this lowers the attackers  cost  generally dos is the most effective if the target server has some resources that incurs asymmetric resource usage i.e the request is a small payload but the response requires thousands of database requests and/or heavy cpu usage this can easily be abused by an attacker that makes a request then immediately disconnects if the server isn t well designed enough it may continue producing the page while the attacker is now free to do another request,2015-04-15 00:41:31,256,86046,is it possible to launch a dos by repeatedly requesting a webpage refreshing? like holding down f5 on your browser,0.0546875,14,sse,denial-of-service,requesting webpage as dos,2,attacks|denial of service,0.2274317890405655
25417,and another one     it is inefficient to iterate over the protected keys for each key in the hash as in your solution rather just iterate over the protected keys    it is inefficient to generate the array of protected keys each time the method is called define that array outside of the method    the following is better in these respects   perhaps something like   you can then call   and then   will modify the hash in place without duping per ruby standards,2013-11-12 19:14:18.95 UTC,139,19937909,this method takes a hash and returns a new hash without sensitive information it does not modify the hash passed in  is there a more ruby-like idiomatic way of doing it?   working in ruby 1.9.3 sinatra not rails and   not   using active record,0.03597122302158273,5,so,hashtable|ruby|ruby-1.9.3|sinatra,how to refactor this ruby sanitize hash method to make it more idiomatic,3,protection|sanitization|sensitive information,0.22655914723873138
48687,the answer was to add more ram however lowing php memory usage by updating the software running on the server may also have helped no issues since i ve taken these actions,2016-05-19 15:03:03.757 UTC,195,37327165,i have a site hosted on an ec2 instance which i am able to crash by simply clicking the home button very fast very quickly essentially a small scale dos attack the system log gives the following error   i ve gone through all the usual steps and the problem persists the steps i ve taken so far include   upgrading php from 5.3.29 to 5.4.45  setting up browser caching and server side caching  setting up gzip  increasing the hosting instance to 2gb of ram  reducing plugins and compressing all images on the site also compressing css and js   the php memory is set to 128mb and of that each session uses approximately 38mb which isn t overly high  i don t think it s anything to do with wp-config.php or a corrupt database the file wp-config seems fine and the site is only effected when handling lots of simultaneous requests,0.015384615384615385,3,so,database|denial-of-service|php|wordpress,high traffic in wordpress causing error establishing a database connection,2,attacks|denial of service,0.22548136115074158
38530,"if there really is a leak your script/scenario shouldn t be causing it but i would think that you could potentially cause it to appear to be a problem sooner depending on how you run it  for example let s say with 5 users and reasonable pacing and think times the server doesn t die for 16 hours but with 50 users it dies in 2 hours you haven t caused the problem just exposed it sooner  i hope its web server problem.pacing is nothing but a time gap in between iterations,it s not  effect actions or transactions in your script  to answer your last question no  pacing is explicitly used when a new iteration starts the iteration start is delayed according to pacing settings  speculation/conclusions  if the web-server really runs out of memory after 10 minutes and you only have 2 vu s you have a problem on the web-server side one could manually achieve this 2vu load and crash the web-server the pacing in the scripts or manual user speeds are irrelevant if the web-server can be crashed from remote it has bugs that need fixing  suggestion  try running the scenario with 4 users do you get out of memory on the web-server after 5 mins",2013-04-11 19:06:48.15 UTC,418,15956821,running a single script with only two users as a single scenario  without any pacing  just think time set to 3 seconds and random 50%-150% i experience that the web app server runs of of memory after 10 minutes every time i have run the test several times and it happens at the same time every time.first i thouhgt this was a memory leak in the application but after some thought i figured it might have to do with the scenario design the entire script having just one action including log in and log out within the only action block takes about 50 seconds to run and i have the default  as soon as the previous iteration ends  set not the  with delay after the previous iteration ends  or  fixed/random intervalls  set   could not using  fixed/random intervalls  cause this memory leak to happen? i guess non of the settings mentioned would actually start a new iteration before the one before ends this obvioulsy leading to accumulation of memory on the server resulting in this memory leak but with no pacing set is there a risk for this to happen?  and having no iterations in my script could i still be using pacing,0.011961722488038277,5,so,loadrunner|memory-leaks,loadrunner and the need for pacing,1,memory leaks,0.2251621037721634
15324,"i am not sure what the best practice is but when dealing with dos attacks a better strategy is to actually divert traffic  away  from your server setting timeouts won t actually help because you re still processing the request and running php  have you considered setting up another web server running a simpler stripped down version of your login page? when the user tries too many times e.g thousands of times send a message to configure your router and redirect this user to the second web server  it s like when websites get hit with the slashdot effect many of them just redirect traffic away until traffic is reduced  i made a class that takes care of brute force attack protection in php    https://github.com/ejfrancis/bruteforceblocker    it logs all failed logins site-wide in a db table and if the number of failed logins in the last 10 minutes or whatever time frame you choose is over a set limit it enforces a time delay and/or a captcha requirement before logging in again   example     //build throttle settings array # recent failed logins => response      $throttle_settings = [       ]      $bfbresponse = bruteforceblocker::getloginstatus$throttle_settings       //$throttle_settings is an optional parameter if it s not included,the default settings array in bruteforceblocker.php will be used      switch $bfbresponse[ status ]{       }   i would suggest if the user has tried unsuccessfully say more than five times and five minutes you start returning a   immediately for that ip address  when a login fails you could use memcache to get the current bad attempts for an ip and then increment the amount and save it back to memcache with a 5 minute expiry    you don t want to put a   in your php code as that will allow a single user to create lots of connections to your web server and potentially bring down other users  since the user hasn t logged in you don t have a session cookie and if the user is trying to brute force their way into an account they may not present a cookie at all  i have used something like this..    check username and password   1.1 if no match then record last failed login time for that combo and number of failed logins  1.2 each fail makes the wait between being able to login something like failscount * 30 seconds up to a maximum such as 10 minutes       this means a brute force attack will exponentially take longer and longer  it could lock a user out - but it will not count a failed login whilst trying to login during the lockout period this should minimise it   i ve developed this but not released it into the wild yet so any feedback would be appreciated  the problem is the balance between user accessibility and attacker model  first solution    user  could be blocked and they don t like to reset  attacker  blocked all users by trying to authenticate to all users especially if all logins are publicly available  second solution   the question is what is the value of  amount_of_time  ?   user  can be annoying to wait  amount_of_time  for each error  attacker  keep trying with lower test / seconds  third solution    user  less annoying for few password mistakes  attacker  block the user to connect by sending lot of incorrect password    fourth solution    user  need to resolve the captcha not too complex  attacker  need to resolve the captcha must be complex    good solution and used by a lot of sites but  be careful to our captcha  implementation  anyway there is a trick see next solution  fifth solution    user  user may be blocked because he cannot correctly remember his password  attacker  trying the same password with different user because blocking is based on number of login by user    final solution ?    user  user cannot be ip blocked but must remember its password  attacker  difficult to have an efficient brute-force attack  the important notes  is the login form or the login submit link which is blocked ? blocking the login form is useless  resistance to brute-force is first a problem of password complexity so you need a strict password policy especially in the case of distributed brute force  i don t mention the fact to hash your passwords with salt you re already doing this right ? because if it is easier to access to the password database than brute-forcing the attacker will choose this solution  a chain is only as strong as its weakest link",2009-11-13 05:37:34.53 UTC,939,1727329,i am trying to write a script to prevent brute-force login attempts in a website i m building the logic goes something like this   user sends login information  check if username and password is correct  if yes let them in  if no record a failed attempt in the database check if there s too many fails within a given timeframe eg 5 in 5 minutes:  if yes then pause execution for 10 seconds   then report a login failure to the user  report a login failure to the user immediately       explaining this to a co-worker i was asked how this would help if a hacker sent say 1000 requests in one second would the first 5 would return immediately and then the remaining 995 all take only 10 seconds?   i have a sneaking suspicion that i don t fully understand how http works - is that situation above even possible or is there a limit to the number of concurrent requests that a server will handle from one client?  would a better solution be to have an increasing sleep time?   so the first 5 would be fast and then every subsequent one would increase the sleep,0.01703940362087327,16,so,brute-force|ddos|php|security,brute-force/dos prevention in php,5,ddos|attacks|weakness|protection|denial of service,0.22423696517944336
53391,there s a section of the webplatform.org docs that explains this     the values are quantized as to not expose private information to attackers if chrome is run with the flag   the values are not quantized    http://docs.webplatform.org/wiki/apis/timing/properties/memory   so by default the number is not precise and  it only updates every 20 minutes! this should explain why your number doesn t change if you use the flag the number will be precise and current  the  webkit commit message  explains     this patch adds an option to expose quantized and rate-limited memory   information to web pages  web pages can only learn new data every 20   minutes which helps mitigate attacks where the attacker compares two   readings to extract side-channel information  the patch also only   reports 100 distinct memory values which combined with the rate   limits makes it difficult for attackers to learn about small changes in    memory use,2014-08-06 17:00:31.147 UTC,335,25166051,first of all i ve looked around the internet and found it quite badly documented.somewhere in my code i have a big memory leak that i m trying to track and after using   it looks like the value remains at the same level of 10mb which is not true because when we compare to the values either visible here   or if we look at the timeline in devtools we can see a big difference does anyone encountered a similar issue? do i need to manually update these values to run a command update measure etc?   following this topic: information heap size it looks like this value is increased by a certain step can we somehow see what is it or modify it? in my case from what i can see now the page has about 10mb 30 minutes later there will be about 400mb and half an hour after the page will crash. any ideas guys?  why the code is leaking it s a different issue please treat this one as i was trying to use this variable to create some kind of test,0.020895522388059702,7,so,google-chrome|javascript|memory-leaks|performance,chrome usedjsheapsize property,2,attacks|memory leaks,0.22224901616573334
58250,not necessarily  btw .. you only have about 1.6gb of available memory in a win32 process  32-bit architecture can address 4gb of ram  top 2gb is used reserved .. leaving your application with about 1.6gb max  try switching to 64-bit code,2015-05-13 16:12:11.487 UTC,101,30220186,my 32-bit process is throwing outofmemoryexception   when this happens the perfmon counters for the process say    process - privatebytes ~2000mb     .net - bytesinallheaps ~500mb    my process does interop with unmanaged code but my question is does the size ratio definitely indicate an unmanaged memory leak,0.0297029702970297,3,so,.net|c#|memory-leaks,does large difference between privatebytes vs bytesinallheaps definitily indicate unmanaged memory leak,1,memory leaks,0.21738813817501068
30313,well it all depends what is definition of virus signature ? i suggest you to parse executable and use only  code-section  but polymorphic virus keeps there malicious code in data-section in encrypted form so i am not very much sure are you using some kind of n-gram technique? or just mining frequent hex-codes? scan time is very important issue! once i have written a command line saner that was able to find a file in less than a second -infect tons of files in a seconds the technique was        perhaps your pattern scan is inefficient  i can scan for a pattern in a 7 mb file in about 1/20th of a second using code like this  note if you really want to use code like this you have to make a correction  you can t always set matchedlength back to 0 when you realize that you aren t looking at a match but it does work for this particular pattern  you have to pre-process the pattern so you know what to reset to when you don t find a match but that will not add significant time to the algorithm  i could make the effort to correctly complete the algorithm but i won t do that now if your question is just about performance  i m just demonstrating that it is possible to scan large files quickly if you do it correctly   edit  here are some thoughts about how you could match multiple patterns at once this is just off the top of my head and i have not tried to compile the code  create a class to contain information about the status of a pattern   declare a variable to track all the patterns that you need to check and index them by the first byte of the pattern for quick lookup   check all the patterns that are currently a potential match to see if the next byte also matches on these patterns if not stop looking at that pattern at that position   see if the current byte looks like the beginning of a new pattern that you would be searching for if so add it to the list of active patterns,2011-03-14 21:16:43.9 UTC,489,5304657,i have been developing an antivirus using vb.net the virus scanner works fine but i was thinking of ways to optimize the scanning speed because large files take forever   the algorithm i m using to detect the viruses is via binary converted to hex signatures i think i don t have to look around the whole file just to find if it s a virus or not i think there s a specific place and a specific number of bytes that i should scan instead of scanning the whole file anyway if anyone can provide any help in this subject please do so  thanks in advance  btw the virus signatures come from the hex collection from the clamav antivirus.,0.022494887525562373,11,so,performance|signatures|vb.net|virus,antivirus scan speed optimization,2,virus|malicious code,0.21624577045440674
10300,modsecurity can limit the total length of but not the number of parameters there is a module that has recently been updated with this feature it s in beta modifier  http://yoyo.org/~steve/mod_ifier.html   i took some time to understand modsecurity then i found owasp modsecurity core rule set in file modsecurity_crs_23_request_limits.conf there was a rule that limited number of arguments in a request  on my company webserver using modsecurity core rule too but there s not have this file i don t know why :  you can see it here from line 30: http://mod-security.svn.sourceforge.net/viewvc/mod-security/crs/tags/2.2.3/base_rules/modsecurity_crs_23_request_limits.conf?revision=1882&amp;view=markup   you can find a great article that describes mod_security options on defending this here   http://blog.spiderlabs.com/2012/01/modsecurity-mitigations-for-aspnet-hashtable-dos-vulnerability-cve-2011-3414.html,2012-01-04 07:56:35.053 UTC,242,8723671,"i have just read about new technique dos that called hashdos details about it  https://cryptanalysis.eu/blog/2011/12/28/effective-dos-attacks-against-web-application-plattforms-hashdos/   this dos technique post a large number of parameter and trigger worse case of hashtable algorithm web server will take more time to do the job   they said     so you can keep about 10.000 core i7 cpu cores busy processing php  requests using a gigabit internet connection alternatively for  asp.net 30,000 core2 cpu cores or for java tomcat 100,000 core i7  cpu cores or for cruby 1.8 1,000,000 core i7 cpu cores can be kept  busy with a single gigabit connection   so i want to limit number of parameter in post content for my company website.i know modsecurity can do that but i m not familiar with modsecurity   thanks in advance",0.0371900826446281,9,so,apache|mod-security,configure modsecurity to limit number of parameter,5,cve|owasp|attacks|vulnerability|denial of service,0.21525733172893524
12671,basically at each step   will evaluate the result of the function   at every step bringing it to whnf weak head normal form concretely this means that the result will be evaluated until the first constructor  however the result of   is   which is already in whnf since it starts with a pair constructor what we want is to force the evaluation of the pair components here one basic solution is to return   so that the components will be evaluated before returning the pair  a more modern approach could be exploiting   instead,2018-01-27 07:35:18.163 UTC,156,48473669,if i call   with any value for   and 50000000 fifty million for spins it ramps up and keeps growing in memory size until has choked every last bit of memory and then crashes    i don t see how memory is leaked doesn t each call to   replace the accumulator value? doesn t it get released,0.019230769230769232,3,so,haskell|performance,why does this code grow so large in memory,3,leak|exploit|weakness,0.2146887630224228
19241,this is a really bad idea you should use    which does everything for you and then you can use   to store information about that user  if you store a password hash in the database and use it as a cookie then you totally undermine the purpose of hashing passwords   an attacker can use sql injection to obtain the hash and then just login without having to crack the hash   wordpress was vulnerable to this a few years ago   that code base has had some very serious security problems,2011-03-05 18:54:49.047 UTC,131,5206039,i m trying to store a password in a cookie stackoverflow seems to recommend hash_hmac but wordpress uses phpass?  what s the difference from a security perspective and which should be used,0.022900763358778626,3,so,cookies|php|security,what s the difference between phpass and hash_hmac,3,attacks|sql injection|vulnerability,0.2116483598947525
53699,,2015-05-28 11:19:55.417 UTC,95,30505112,hi i am trying to crawl a website specifically www.cvedetails.com and all the cves and after about 600 cves i got this error   i have changed the   value in the   to 900 seconds but apparently it doesn t work fine  i am using also a sleep after x requests but no success have tried to change the user-agent and the same  i have no idea what can i do not for being blocked from the crawling  thank you in advance,0.031578947368421054,3,so,ddos|flooding|goutte|php|web-crawler,maximum execution time for of 30 seconds exceeded in goutte,1,cve,0.2095729112625122
14685,jmx can be used to support any of the mxbeans metrics.refer java documentation -  http://docs.oracle.com/javase/7/docs/api/java/lang/management/managementfactory.html section method summary   memory usage diagram you can see gc runs and detect memory leaks  stack trace of specified  thread  jvm uptime os information  all jmx exposed data with your application   i would add   class loaders behaviour  threads    garbage collector activity duration and frequency  deadlock detection  connector traffic in / out  request processing time  number of sessions in relation to max configured  average session duration   number of sessions rejected  is webmodule running ?  uptime if less than 5 minutes then someone restarted the jvm  connector threads relative to the max available connector threads  datasource pools usage relative lease time  jms queue size dlq size   see also  jmx4perl s predefined nagios check  for further metrics ..,2010-04-02 17:50:10.237 UTC,161,2568232,may i know what are the typical metrics that application developers usually find interesting with the use of jmx other than   cpu utilization   memory consumption   nicholas,0.018633540372670808,3,so,jmx,interesting metrics from jmx,3,deadlock|memory leak|exposed data,0.2005009800195694
11253,frank answered in the comments so i m just adding two additional information that might be useful   firebase prevents using passwords that have less than 6 characters see  firebaseauthweakpasswordexception   a specific error code error_too_many_requests is thrown when unusual activity is detected on a specific device so you could easily make some tests to see how fast the error is triggered i just did and 3-4 incorrect password attempts triggered the error and blocked the requests from my device   from the doc: auth/too-many-requests  thrown if requests are blocked from a device due to unusual activity trying again after some delay would unblock,2017-01-11 23:12:34.49 UTC,230,41602468,i am looking for more details regarding firebase protection against brute force password guessing  in this  thread  kato says we throttle requests by origin to mitigate any brute force approaches can any more info be given ? especially    when does throttling kicks in and if at any point the client is completely blocked for a certain time etc ?    can we have a way in the security rules to specify a number of failed attempts after which the client will be locked out for a certain amount of time ?    i want to switch to a digit only  numpad  password on my app and am worried an enumeration attack i need to determine the minimum number of digits to make passwords safe  thanks,0.017391304347826087,4,so,firebase|firebase-authentication,firebase user enumeration attack,3,attacks|weakness|protection,0.20006275177001953
21695,you can use request filter and check the content-length in it if content-length is more than allowed value you can simply return from there only,2014-05-08 13:54:55.943 UTC,271,23543851,i have developed a node application that allows users to upload profile and other images i have validation on both client and server to check the file type and size however i recently became aware of the fact that a malicious attack could involve bypassing the form validation on the client side and uploading huge images just to use up server disk space  how is this preventable in node/express? i should point out that i am also using socketio so would like to know how to limit the data size sent via socketio too  in principle the logic in my head would be straightforward...i know that a file upload should never exceed 1mb in my application as soon as the data received exceeds that i want to just cancel the request and send a 400 response or something and perhaps ban the ip  i did read about a solution involving clearing the temp folder periodically but wouldn t it be better to stop the request mid process as soon as the file exceeds a file size limit?  what is a simple and effective solution to this? is it a common attack and is it something worth worrying about for a small website?  what about limiting socketio data to say under 10 kb and disconnecting the socket if the data transfered exceeds that? is this necessary or possible,0.02214022140221402,6,so,express|file-upload|node.js|socket.io|xss,how to limit file upload size to prevent xss attacks using up server space,3,bypass|attacks|cross site scripting,0.19420017302036285
